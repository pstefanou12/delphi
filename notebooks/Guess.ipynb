{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook is to Show that Guessing the Noise Variances is not an Adequate Approach to Using [Daskalakis et al. 2019]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('..')\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import numpy as np\n",
    "import torch as ch\n",
    "from torch import Tensor\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.utils.data import DataLoader\n",
    "from delphi.stats.truncated_regression import truncated_regression\n",
    "from delphi.utils.datasets import TruncatedRegressionDataset\n",
    "from delphi.oracle import Left, Interval\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:1 | Loss 0.0530 | Train1 0.000 | Train5 -1.000 | Reg term: 0.0 ||: 100%|██████████| 223/223 [00:06<00:00, 32.89it/s]\n",
      "Epoch:2 | Loss 0.0602 | Train1 0.000 | Train5 -1.000 | Reg term: 0.0 ||: 100%|██████████| 223/223 [00:06<00:00, 32.19it/s]\n",
      "Epoch:3 | Loss 0.0635 | Train1 0.000 | Train5 -1.000 | Reg term: 0.0 ||: 100%|██████████| 223/223 [00:06<00:00, 31.97it/s]\n",
      "Epoch:4 | Loss 0.0624 | Train1 0.000 | Train5 -1.000 | Reg term: 0.0 ||: 100%|██████████| 223/223 [00:07<00:00, 31.54it/s]\n",
      "Epoch:5 | Loss 0.0625 | Train1 0.000 | Train5 -1.000 | Reg term: 0.0 ||: 100%|██████████| 223/223 [00:07<00:00, 31.67it/s]\n",
      "Epoch:6 | Loss 0.0626 | Train1 0.000 | Train5 -1.000 | Reg term: 0.0 ||: 100%|██████████| 223/223 [00:06<00:00, 31.94it/s]\n",
      "Epoch:7 | Loss 0.0625 | Train1 0.000 | Train5 -1.000 | Reg term: 0.0 ||: 100%|██████████| 223/223 [00:06<00:00, 32.20it/s]\n",
      "Epoch:8 | Loss 0.0625 | Train1 0.000 | Train5 -1.000 | Reg term: 0.0 ||: 100%|██████████| 223/223 [00:07<00:00, 31.68it/s]\n",
      "Epoch:9 | Loss 0.0623 | Train1 0.000 | Train5 -1.000 | Reg term: 0.0 ||: 100%|██████████| 223/223 [00:07<00:00, 31.68it/s]\n",
      "Epoch:10 | Loss 0.0626 | Train1 0.000 | Train5 -1.000 | Reg term: 0.0 ||: 100%|██████████| 223/223 [00:06<00:00, 32.06it/s]\n",
      "Epoch:1 | Loss 0.0530 | Train1 0.000 | Train5 -1.000 | Reg term: 0.0 ||: 100%|██████████| 223/223 [00:07<00:00, 31.49it/s]\n",
      "Epoch:2 | Loss 1.1037 | Train1 0.000 | Train5 -1.000 | Reg term: 0.0 ||: 100%|██████████| 223/223 [00:07<00:00, 31.45it/s]\n",
      "Epoch:3 | Loss 2.0287 | Train1 0.000 | Train5 -1.000 | Reg term: 0.0 ||: 100%|██████████| 223/223 [00:07<00:00, 31.39it/s]\n",
      "Epoch:4 | Loss 2.0387 | Train1 0.000 | Train5 -1.000 | Reg term: 0.0 ||: 100%|██████████| 223/223 [00:07<00:00, 31.63it/s]\n",
      "Epoch:5 | Loss 2.0366 | Train1 0.000 | Train5 -1.000 | Reg term: 0.0 ||: 100%|██████████| 223/223 [00:07<00:00, 31.47it/s]\n",
      "Epoch:6 | Loss 2.0327 | Train1 0.000 | Train5 -1.000 | Reg term: 0.0 ||: 100%|██████████| 223/223 [00:07<00:00, 31.59it/s]\n",
      "Epoch:7 | Loss 2.0393 | Train1 0.000 | Train5 -1.000 | Reg term: 0.0 ||: 100%|██████████| 223/223 [00:06<00:00, 31.96it/s]\n",
      "Epoch:8 | Loss 2.0461 | Train1 0.000 | Train5 -1.000 | Reg term: 0.0 ||: 100%|██████████| 223/223 [00:06<00:00, 32.09it/s]\n",
      "Epoch:9 | Loss 2.0454 | Train1 0.000 | Train5 -1.000 | Reg term: 0.0 ||: 100%|██████████| 223/223 [00:07<00:00, 31.60it/s]\n",
      "Epoch:10 | Loss 2.0518 | Train1 0.000 | Train5 -1.000 | Reg term: 0.0 ||: 100%|██████████| 223/223 [00:07<00:00, 31.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# regression parameters\n",
    "num_samples, dims = 100000, 1\n",
    "phi = Left(Tensor([2.0]))\n",
    "var = 1.0\n",
    "\n",
    "# generate random uniform weights\n",
    "W = ch.zeros(1, 1)\n",
    "W0  = ch.zeros(1, 1)\n",
    "\n",
    "# generate data\n",
    "X = MultivariateNormal(ch.zeros(dims), ch.eye(dims)/dims).sample(ch.Size([num_samples]))\n",
    "y = X.mm(W) + W0 + Normal(ch.zeros(1), ch.sqrt(Tensor([var]))).sample(ch.Size([num_samples]))\n",
    "# truncate\n",
    "indices = phi(y).nonzero(as_tuple=False).flatten()\n",
    "y_trunc, x_trunc = y[indices], X[indices]\n",
    "\n",
    "# empirical linear regression\n",
    "reg = LinearRegression() \n",
    "reg.fit(x_trunc, y_trunc)\n",
    "emp_var = ch.var(ch.from_numpy(reg.predict(x_trunc)) - y_trunc, dim=0).unsqueeze(0)\n",
    "\n",
    "# experiment parameters\n",
    "args = Parameters({ \n",
    "    'alpha': Tensor([y_trunc.size(0)/num_samples]), \n",
    "    'phi': phi, \n",
    "    'epochs': 10,\n",
    "    'num_workers': 20, \n",
    "    'batch_size': 10,\n",
    "    'bias': True,\n",
    "    'num_samples': 100,\n",
    "    'clamp': True, \n",
    "    'radius': 5.0, \n",
    "    'var_lr': 1e-2,\n",
    "    'var': emp_var,\n",
    "    'lr': 1e-1,\n",
    "    'shuffle': True\n",
    "})\n",
    "\n",
    "# truncated linear regression with known noise variance - empirical estimate \n",
    "# dataset \n",
    "data = TruncatedRegressionDataset(x_trunc, y_trunc, bias=args.bias, unknown=False)\n",
    "S = DataLoader(data, batch_size=args.batch_size, num_workers=args.num_workers, shuffle=args.shuffle)\n",
    "trunc_reg = truncated_regression(phi=phi, alpha=args.alpha, epochs=args.epochs, lr=args.lr, num_samples=args.num_samples, var=args.var, device='cuda')\n",
    "results = trunc_reg.fit(S)\n",
    "w_, w0_ = results.weight.detach().cpu(), results.bias.detach().cpu()\n",
    "\n",
    "# truncated linear regression with known noise variance - actual estimate\n",
    "# dataset \n",
    "data = TruncatedRegressionDataset(x_trunc, y_trunc, bias=args.bias, unknown=False)\n",
    "S = DataLoader(data, batch_size=args.batch_size, num_workers=args.num_workers, shuffle=args.shuffle)\n",
    "trunc_reg = truncated_regression(phi=phi, alpha=args.alpha, epochs=args.epochs, lr=args.lr, num_samples=args.num_samples, var=var, device='cuda')\n",
    "results = trunc_reg.fit(S)\n",
    "w, w0 = results.weight.detach().cpu(), results.bias.detach().cpu()\n",
    "\n",
    "# calculate metrics \n",
    "real_params = ch.cat([W, W0])\n",
    "real_var_params = ch.cat([w, w0.unsqueeze(0)])\n",
    "emp_var_params = ch.cat([w_.t(), w0_.unsqueeze(0)])\n",
    "\n",
    "real_error = ch.abs((y_trunc - w.mm(x_trunc.T).T - w0).pow(2).mean(0) - var)\n",
    "emp_var_error = ch.abs((y_trunc - w_.mm(x_trunc.T).T - w0_).pow(2).mean(0) - emp_var)\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare Consistencies Between Real Noise Variance and Empirical Noise Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real error: tensor([3.1010])\n",
      "emp var error: tensor([[0.0166]])\n"
     ]
    }
   ],
   "source": [
    "print(\"real error: {}\".format(real_error))\n",
    "print(\"emp var error: {}\".format(emp_var_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
