{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../..')\n",
    "from cox.utils import Parameters\n",
    "from cox.store import Store\n",
    "from cox.readers import CollectionReader\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import itertools\n",
    "import numpy as np\n",
    "import torch as ch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Gumbel, Uniform\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import datetime\n",
    "from delphi.oracle import oracle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# procedure hyperparameters\n",
    "args = Parameters({ \n",
    "    'epochs': 25,\n",
    "    'num_workers': 0, \n",
    "    'batch_size': 10,\n",
    "    'bias': True,\n",
    "    'num_samples': 1000,\n",
    "    'clamp': True, \n",
    "    'radius': 5.0, \n",
    "    'var_lr': 1e-2,\n",
    "    'lr': 1e-1,\n",
    "    'shuffle': False, \n",
    "    'tol': 1e-2,\n",
    "    'samples': 10000,  # number of samples to generate for ground truth\n",
    "    'in_features': 3, # number of in-features to multi-log-reg\n",
    "    'k': 2, # number of classes\n",
    "    'lower': -1, # lower bound for generating ground truth weights\n",
    "    'upper': 1,  # upper bound for generating ground truth weights\n",
    "    'custom_criterion': F.gumbel_softmax,\n",
    "    'trials': 10,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch CE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CE LOSS TABLE FOR METRICS\n",
    "CE_LOSS_TABLE_NAME = 'ce_loss'\n",
    "\n",
    "STORE_PATH = '/home/pstefanou/MultinomialLogisticRegression'\n",
    "store = Store(STORE_PATH)\n",
    "\n",
    "store.add_table(CE_LOSS_TABLE_NAME, { \n",
    "    'train_acc': float, \"\"\n",
    "    'val_acc': float, \n",
    "    'train_loss': float, \n",
    "    'val_loss': float,\n",
    "    'epoch': int,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CE Latent Variable Model Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "gumbel = Gumbel(0, 1)\n",
    "\n",
    "class GumbelCE(ch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, pred, targ):\n",
    "        ctx.save_for_backward(pred, targ)\n",
    "        loss = ch.nn.CrossEntropyLoss()\n",
    "        return loss(pred, targ)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        pred, targ = ctx.saved_tensors\n",
    "        # make num_samples copies of pred logits\n",
    "        stacked = pred[None, ...].repeat(1000, 1, 1)        \n",
    "        # add gumbel noise to logits\n",
    "        rand_noise = gumbel.sample(stacked.size())\n",
    "        noised = stacked + rand_noise \n",
    "        noised_labs = noised.argmax(-1)\n",
    "        # remove the logits from the trials, where the kth logit is not the largest value\n",
    "        good_mask = noised_labs.eq(targ)[..., None]\n",
    "        inner_exp = 1 - ch.exp(-rand_noise)\n",
    "        avg = (inner_exp * good_mask).sum(0) / (good_mask.sum(0) + 1e-5) / pred.size(0)\n",
    "        return -avg , None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging in: /home/pstefanou/MultinomialLogisticRegression/8f0f5d89-0b21-4fec-b950-fc836c4b2abb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<cox.store.Table at 0x7f942ef214e0>"
      ]
     },
     "execution_count": 814,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CE LOSS TABLE FOR METRICS\n",
    "LATENT_CE_TABLE_NAME = 'latent_ce_grad'\n",
    "\n",
    "STORE_PATH = '/home/pstefanou/MultinomialLogisticRegression'\n",
    "store = Store(STORE_PATH)\n",
    "\n",
    "store.add_table(LATENT_CE_TABLE_NAME, { \n",
    "    'train_acc': float, \n",
    "    'val_acc': float, \n",
    "    'train_loss': float, \n",
    "    'val_loss': float,\n",
    "    'epoch': int,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-815-85ef1b54db2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGumbelCE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;31m#             print(\"latent variable grad: {}\".format(multi_log_reg.weight.grad))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#             print((\"---\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.6/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# _forward_cls is defined by derived class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-666-731cee247cf3>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, grad_output)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mrand_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgumbel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstacked\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mnoised\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstacked\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrand_noise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mnoised_labs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnoised\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;31m# remove the logits from the trials, where the kth logit is not the largest value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mgood_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnoised_labs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "multi_log_reg = nn.Linear(in_features=args.IN_FEATURES, out_features=args.K, bias=args.bias)\n",
    "# optimizer and scheduler\n",
    "optimizer = ch.optim.SGD(multi_log_reg.parameters(), lr=1e-1)\n",
    "scheduler = ch.optim.lr_scheduler.CosineAnnealingLR(optimizer, args.epochs)\n",
    "\n",
    "for i in range(args.trials):\n",
    "    for epoch in range(args.epochs): \n",
    "        # train loop\n",
    "        train_loss, train_acc = Tensor([]), Tensor([])\n",
    "        for batch_X, batch_y in train_loader: \n",
    "            optimizer.zero_grad()\n",
    "            pred = multi_log_reg(batch_X)\n",
    "            loss = ce_loss(pred, batch_y)\n",
    "            loss.backward(retain_graph=True) \n",
    "            optimizer.zero_grad() \n",
    "            loss = GumbelCE.apply(pred,  batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            # keep track of algorithm training loss and accuracy\n",
    "            acc = (ch.argmax(ch.nn.Softmax(dim=1)(pred), dim=1) == batch_y).sum() / batch_y.size(0)\n",
    "            train_loss = ch.cat([train_loss, Tensor([loss])]) if train_loss.size() != ch.Size([0]) else Tensor([loss])\n",
    "            train_acc = ch.cat([train_acc, Tensor([acc])]) if train_acc.size() != ch.Size([0]) else Tensor([acc])\n",
    "        # validation loop\n",
    "        val_loss, val_acc = Tensor([]), Tensor([])\n",
    "        with ch.no_grad(): \n",
    "            for batch_X, batch_y in val_loader: \n",
    "                pred = multi_log_reg(batch_X)\n",
    "                loss = gumbel_ce(pred, batch_y)\n",
    "                # keep track of algorithm validation loss and accuracy\n",
    "                acc = (ch.argmax(ch.nn.Softmax(dim=1)(pred), dim=1) == batch_y).sum() / batch_y.size(0)            \n",
    "                val_loss = ch.cat([val_loss, Tensor([loss])]) if val_loss.size() != ch.Size([0]) else Tensor([loss])\n",
    "                val_acc = ch.cat([val_acc, Tensor([acc])]) if val_acc.size() != ch.Size([0]) else Tensor([acc])\n",
    "\n",
    "        store[LATENT_CE_TABLE_NAME].append_row({ \n",
    "            'train_acc': float(train_acc.mean()), \n",
    "            'val_acc': float(val_acc.mean()), \n",
    "            'train_loss': float(train_loss.mean()), \n",
    "            'val_loss': float(val_loss.mean()),\n",
    "            'epoch': int(epoch + 1),\n",
    "        })\n",
    "store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = CollectionReader(STORE_PATH)\n",
    "results = reader.df(LATENT_CE_TABLE_NAME)\n",
    "reader.close() # close reader\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=results, x='epoch', y='train_loss', label='train loss')\n",
    "ax = sns.lineplot(data=results, x='epoch', y='val_loss', color='red', label='val loss')\n",
    "ax.set(xlabel='epoch', ylabel='Gumbel CE Loss')\n",
    "plt.show()\n",
    "\n",
    "sns.lineplot(data=results, x='epoch', y='train_acc', label='train acc')\n",
    "ax = sns.lineplot(data=results, x='epoch', y='val_acc', color='red', label='val acc')\n",
    "ax.set(xlabel='epoch', ylabel='Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truncated Multinomial Logistic Regression Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "gumbel = Gumbel(0, 1)\n",
    "\n",
    "class TruncatedGumbelCE(ch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, pred, targ, phi):\n",
    "        ctx.save_for_backward(pred, targ)\n",
    "        ctx.phi = phi\n",
    "        ce_loss = ch.nn.CrossEntropyLoss()\n",
    "        return ce_loss(pred, targ)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        pred, targ = ctx.saved_tensors\n",
    "        # make num_samples copies of pred logits\n",
    "        stacked = pred[None, ...].repeat(args.num_samples, 1, 1)   \n",
    "        # add gumbel noise to logits\n",
    "        rand_noise = gumbel.sample(stacked.size())\n",
    "        noised = stacked + rand_noise \n",
    "        # truncate - if one of the noisy logits does not fall within the truncation set, remove it\n",
    "        filtered = ch.all(ctx.phi(noised).bool(), dim=2).float().unsqueeze(2)\n",
    "        noised_labs = noised.argmax(-1)\n",
    "        # mask takes care of invalid logits and truncation set\n",
    "        mask = noised_labs.eq(targ)[..., None] * filtered\n",
    "        inner_exp = 1 - ch.exp(-rand_noise)\n",
    "        \n",
    "        avg = ((inner_exp * mask).sum(0) / (mask.sum(0) + 1e-5) - (inner_exp * filtered).sum(0) / (filtered.sum(0) + 1e-5)) \n",
    "        return -avg / pred.size(0), None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Membership oracles for Multinomial Logistic Regression Logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_Lower(oracle): \n",
    "    \"\"\"\n",
    "    Lower bound truncation on the DNN logits.\n",
    "    \"\"\"\n",
    "    def __init__(self, lower): \n",
    "        self.lower = lower\n",
    "        \n",
    "    def __call__(self, x): \n",
    "        return (x > self.lower).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(oracle): \n",
    "    def __call__(self, x): \n",
    "        return ch.ones(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truncate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = DNN_Lower(Tensor([-2, -2]))\n",
    "# phi = Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground truth weights: Parameter containing:\n",
      "tensor([[ 0.1578, -0.2211, -0.8262],\n",
      "        [-0.7063, -0.6298, -0.9827]], requires_grad=True)\n",
      "ground truth bias: Parameter containing:\n",
      "tensor([ 0.9465, -0.5519], requires_grad=True)\n",
      "alpha: 0.6078\n"
     ]
    }
   ],
   "source": [
    "# generate ground-truth from uniform distribution\n",
    "U = Uniform(args.lower, args.upper)\n",
    "ground_truth = nn.Linear(in_features=args.IN_FEATURES, out_features=args.K, bias=args.bias)\n",
    "ground_truth.weight = nn.Parameter(U.sample(ch.Size([args.K, args.IN_FEATURES])))\n",
    "if ground_truth.bias is not None: \n",
    "    ground_truth.bias = nn.Parameter(U.sample(ch.Size([args.K,])))\n",
    "print(\"ground truth weights: {}\".format(ground_truth.weight))\n",
    "print(\"ground truth bias: {}\".format(ground_truth.bias))\n",
    "# independent variable \n",
    "U_ = Uniform(-5, 5)\n",
    "X = U_.sample(ch.Size([args.samples, args.IN_FEATURES]))\n",
    "# determine base model logits \n",
    "z = ground_truth(X)\n",
    "# apply softmax to unnormalized likelihoods\n",
    "y = ch.argmax(ch.nn.Softmax(dim=1)(z), dim=1)\n",
    "\n",
    "# TRUNCATE\n",
    "trunc = phi(z)\n",
    "indices = ch.all(trunc.bool(), dim=1).float().nonzero(as_tuple=False).flatten()\n",
    "y_trunc = y[indices]\n",
    "x_trunc = X[indices]\n",
    "alpha = x_trunc.size(0) / X.size(0)\n",
    "print(\"alpha: {}\".format(alpha))\n",
    "\n",
    "\n",
    "# all synthetic data \n",
    "ds = TensorDataset(x_trunc, y_trunc)\n",
    "# split ds into training and validation data sets\n",
    "train_length = int(len(ds)*.8)\n",
    "val_length = len(ds) - train_length\n",
    "train_ds, val_ds = ch.utils.data.random_split(ds, [train_length, val_length])\n",
    "# train and validation loaders\n",
    "train_loader = DataLoader(train_ds, num_workers=args.num_workers, batch_size=args.batch_size)\n",
    "val_loader = DataLoader(val_ds, num_workers=args.num_workers, batch_size=args.batch_size)\n",
    "\n",
    "# test dataset\n",
    "y_test = y[~indices]\n",
    "x_test = X[~indices]\n",
    "test_ds = TensorDataset(x_test, y_test)\n",
    "test_loader = DataLoader(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging in: /home/pstefanou/MultinomialLogisticRegression/3caf7896-e1bb-4339-91da-06f735e5d653\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<cox.store.Table at 0x7f1c8403a0f0>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CE LOSS TABLE FOR METRICS\n",
    "LATENT_CE_TABLE_NAME = 'trunc_test'\n",
    "\n",
    "STORE_PATH = '/home/pstefanou/MultinomialLogisticRegression'\n",
    "store = Store(STORE_PATH)\n",
    "\n",
    "store.add_table(LATENT_CE_TABLE_NAME, { \n",
    "    'trunc_train_acc': float, \n",
    "    'trunc_val_acc': float, \n",
    "    'trunc_train_loss': float, \n",
    "    'trunc_val_loss': float,\n",
    "    'naive_train_acc': float, \n",
    "    'naive_val_acc': float, \n",
    "    'naive_train_loss': float, \n",
    "    'naive_val_loss': float,\n",
    "    'epoch': int,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "trunc_multi_log_reg = nn.Linear(in_features=args.IN_FEATURES, out_features=args.K, bias=args.bias)\n",
    "naive_multi_log_reg = nn.Linear(in_features=args.IN_FEATURES, out_features=args.K, bias=args.bias)\n",
    "# optimizer and scheduler\n",
    "trunc_opt = ch.optim.SGD(trunc_multi_log_reg.parameters(), lr=1e-1)\n",
    "naive_opt = ch.optim.SGD(naive_multi_log_reg.parameters(), lr=1e-1)\n",
    "trunc_scheduler = ch.optim.lr_scheduler.CosineAnnealingLR(trunc_opt, args.epochs)\n",
    "naive_scheduler = ch.optim.lr_scheduler.CosineAnnealingLR(naive_opt, args.epochs)\n",
    "trunc_ce_loss = TruncatedGumbelCE.apply\n",
    "ce_loss = ch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(args.trials):\n",
    "    for epoch in range(args.epochs): \n",
    "        # train loop\n",
    "        trunc_train_loss, trunc_train_acc = Tensor([]), Tensor([])\n",
    "        naive_train_loss, naive_train_acc = Tensor([]), Tensor([])\n",
    "        for batch_X, batch_y in train_loader: \n",
    "            # truncated multinomial regression\n",
    "            trunc_opt.zero_grad()\n",
    "            pred = trunc_multi_log_reg(batch_X)\n",
    "            loss = trunc_ce_loss(pred, batch_y, phi)\n",
    "            loss.backward() \n",
    "            trunc_opt.step()\n",
    "            trunc_scheduler.step()\n",
    "            # keep track of truncated algorithm training loss and accuracy\n",
    "            acc = (ch.argmax(ch.nn.Softmax(dim=1)(pred), dim=1) == batch_y).sum() / batch_y.size(0)\n",
    "            trunc_train_loss = ch.cat([trunc_train_loss, Tensor([loss])]) if trunc_train_loss.size() != ch.Size([0]) else Tensor([loss])\n",
    "            trunc_train_acc = ch.cat([trunc_train_acc, Tensor([acc])]) if trunc_train_acc.size() != ch.Size([0]) else Tensor([acc])\n",
    "            \n",
    "            # naive multinomial regression\n",
    "            naive_opt.zero_grad()\n",
    "            pred = naive_multi_log_reg(batch_X)\n",
    "            loss = ce_loss(pred, batch_y)\n",
    "            loss.backward() \n",
    "            naive_opt.step()\n",
    "            naive_scheduler.step()\n",
    "            # keep track of naive algorithm training loss and accuracy\n",
    "            acc = (ch.argmax(ch.nn.Softmax(dim=1)(pred), dim=1) == batch_y).sum() / batch_y.size(0)\n",
    "            naive_train_loss = ch.cat([naive_train_loss, Tensor([loss])]) if naive_train_loss.size() != ch.Size([0]) else Tensor([loss])\n",
    "            naive_train_acc = ch.cat([naive_train_acc, Tensor([acc])]) if naive_train_acc.size() != ch.Size([0]) else Tensor([acc])\n",
    "        # validation loop\n",
    "        trunc_val_loss, trunc_val_acc = Tensor([]), Tensor([])\n",
    "        naive_val_loss, naive_val_acc = Tensor([]), Tensor([])\n",
    "        with ch.no_grad(): \n",
    "            for batch_X, batch_y in val_loader: \n",
    "                # truncated validation loop\n",
    "                pred = trunc_multi_log_reg(batch_X)\n",
    "                loss = trunc_ce_loss(pred, batch_y, phi)\n",
    "                # keep track of algorithm validation loss and accuracy\n",
    "                acc = (ch.argmax(ch.nn.Softmax(dim=1)(pred), dim=1) == batch_y).sum() / batch_y.size(0)            \n",
    "                trunc_val_loss = ch.cat([trunc_val_loss, Tensor([loss])]) if trunc_val_loss.size() != ch.Size([0]) else Tensor([loss])\n",
    "                trunc_val_acc = ch.cat([trunc_val_acc, Tensor([acc])]) if trunc_val_acc.size() != ch.Size([0]) else Tensor([acc])\n",
    "                \n",
    "                # naive validation loop\n",
    "                pred = naive_multi_log_reg(batch_X)\n",
    "                loss = ce_loss(pred, batch_y)\n",
    "                # keep track of algorithm validation loss and accuracy\n",
    "                acc = (ch.argmax(ch.nn.Softmax(dim=1)(pred), dim=1) == batch_y).sum() / batch_y.size(0)            \n",
    "                naive_val_loss = ch.cat([naive_val_loss, Tensor([loss])]) if naive_val_loss.size() != ch.Size([0]) else Tensor([loss])\n",
    "                naive_val_acc = ch.cat([naive_val_acc, Tensor([acc])]) if naive_val_acc.size() != ch.Size([0]) else Tensor([acc])\n",
    "\n",
    "\n",
    "        store[LATENT_CE_TABLE_NAME].append_row({ \n",
    "            'trunc_train_acc': float(trunc_train_acc.mean()), \n",
    "            'trunc_val_acc': float(trunc_val_acc.mean()), \n",
    "            'trunc_train_loss': float(trunc_train_loss.mean()), \n",
    "            'trunc_val_loss': float(trunc_val_loss.mean()),\n",
    "            'naive_train_acc': float(naive_train_acc.mean()), \n",
    "            'naive_val_acc': float(naive_val_acc.mean()), \n",
    "            'naive_train_loss': float(naive_train_loss.mean()), \n",
    "            'naive_val_loss': float(naive_val_loss.mean()),\n",
    "            'epoch': int(epoch + 1),\n",
    "        })\n",
    "store.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Experiment Data from Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = CollectionReader(STORE_PATH)\n",
    "results = reader.df(LATENT_CE_TABLE_NAME)\n",
    "reader.close() # close reader\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and Accuracy Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=results, x='epoch', y='trunc_train_loss', label='trunc train loss')\n",
    "sns.lineplot(data=results, x='epoch', y='naive_train_loss', label='naive train loss')\n",
    "sns.lineplot(data=results, x='epoch', y='trunc_val_loss', color='red', label='trunc val loss')\n",
    "ax = sns.lineplot(data=results, x='epoch', y='naive_val_loss', color='red', label='naive val loss')\n",
    "ax.set(xlabel='epoch', ylabel='CE Loss')\n",
    "plt.show()\n",
    "\n",
    "sns.lineplot(data=results, x='epoch', y='trunc_train_acc', label='trunc train acc')\n",
    "sns.lineplot(data=results, x='epoch', y='naive_train_acc', label='naive train acc')\n",
    "sns.lineplot(data=results, x='epoch', y='trunc_val_acc', label='trunc val acc')\n",
    "ax = sns.lineplot(data=results, x='epoch', y='naive_val_acc', label='naive val acc')\n",
    "ax.set(xlabel='epoch', ylabel='Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
