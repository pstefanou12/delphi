{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../..')\n",
    "from cox.utils import Parameters\n",
    "from cox.store import Store\n",
    "from cox.readers import CollectionReader\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import itertools\n",
    "import numpy as np\n",
    "import torch as ch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Gumbel, Uniform\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import datetime\n",
    "from delphi.oracle import oracle\n",
    "\n",
    "# set default tensor type \n",
    "ch.set_default_tensor_type(ch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# procedure hyperparameters\n",
    "args = Parameters({ \n",
    "    'epochs': 25,\n",
    "    'num_workers': 0, \n",
    "    'batch_size': 100,\n",
    "    'bias': True,\n",
    "    'num_samples': 10000,\n",
    "    'clamp': True, \n",
    "    'radius': 5.0, \n",
    "    'var_lr': 1e-2,\n",
    "    'lr': 1e-1,\n",
    "    'shuffle': False, \n",
    "    'tol': 1e-2,\n",
    "    'samples': 10000,  # number of samples to generate for ground truth\n",
    "    'in_features': 3, # number of in-features to multi-log-reg\n",
    "    'k': 3, # number of classes\n",
    "    'lower': -1, # lower bound for generating ground truth weights\n",
    "    'upper': 1,  # upper bound for generating ground truth weights\n",
    "    'trials': 10,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CE Latent Variable Model Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "gumbel = Gumbel(0, 1)\n",
    "\n",
    "class GumbelCE(ch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, pred, targ):\n",
    "        ctx.save_for_backward(pred, targ)\n",
    "        loss = ch.nn.CrossEntropyLoss()\n",
    "        return loss(pred, targ)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        pred, targ = ctx.saved_tensors\n",
    "        # make num_samples copies of pred logits\n",
    "        stacked = pred[None, ...].repeat(1000, 1, 1)        \n",
    "        # add gumbel noise to logits\n",
    "        rand_noise = gumbel.sample(stacked.size())\n",
    "        noised = stacked + rand_noise \n",
    "        noised_labs = noised.argmax(-1)\n",
    "        # remove the logits from the trials, where the kth logit is not the largest value\n",
    "        good_mask = noised_labs.eq(targ)[..., None]\n",
    "        inner_exp = 1 - ch.exp(-rand_noise)\n",
    "        avg = (inner_exp * good_mask).sum(0) / (good_mask.sum(0) + 1e-5) / pred.size(0)\n",
    "        return -avg , None\n",
    "    \n",
    "class TruncatedGumbelCE(ch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, pred, targ, phi):\n",
    "        ctx.save_for_backward(pred, targ)\n",
    "        ctx.phi = phi\n",
    "        ce_loss = ch.nn.CrossEntropyLoss()\n",
    "        return ce_loss(pred, targ)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        pred, targ = ctx.saved_tensors\n",
    "        # make num_samples copies of pred logits\n",
    "        stacked = pred[None, ...].repeat(args.num_samples, 1, 1)   \n",
    "        # add gumbel noise to logits\n",
    "        rand_noise = gumbel.sample(stacked.size())\n",
    "        noised = stacked + rand_noise \n",
    "        # truncate - if one of the noisy logits does not fall within the truncation set, remove it\n",
    "        filtered = ch.all(ctx.phi(noised).bool(), dim=2).float().unsqueeze(2)\n",
    "        noised_labs = noised.argmax(-1)\n",
    "        # mask takes care of invalid logits and truncation set\n",
    "        mask = noised_labs.eq(targ)[..., None] * filtered\n",
    "        inner_exp = 1 - ch.exp(-rand_noise)\n",
    "                \n",
    "        avg = ((inner_exp * mask).sum(0) / (mask.sum(0) + 1e-5) - (inner_exp * filtered).sum(0) / (filtered.sum(0) + 1e-5)) \n",
    "        return -avg / pred.size(0), None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truncated Multinomial Logistic Regression Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Membership oracles for Multinomial Logistic Regression Logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_Lower(oracle): \n",
    "    \"\"\"\n",
    "    Lower bound truncation on the DNN logits.\n",
    "    \"\"\"\n",
    "    def __init__(self, lower): \n",
    "        self.lower = lower\n",
    "        \n",
    "    def __call__(self, x): \n",
    "        return (x > self.lower).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(oracle): \n",
    "    def __call__(self, x): \n",
    "        return ch.ones(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truncate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = DNN_Lower(Tensor([0, 0, 0]))\n",
    "# phi = Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground truth weights: Parameter containing:\n",
      "tensor([[-0.1333,  0.3580, -0.9374],\n",
      "        [ 0.8632, -0.6256,  0.4731],\n",
      "        [ 0.5971,  0.1293, -0.4455]], requires_grad=True)\n",
      "ground truth bias: Parameter containing:\n",
      "tensor([0.0050, 0.2373, 0.2112], requires_grad=True)\n",
      "alpha: 0.1624\n"
     ]
    }
   ],
   "source": [
    "# generate ground-truth from uniform distribution\n",
    "U = Uniform(args.lower, args.upper)\n",
    "ground_truth = nn.Linear(in_features=args.IN_FEATURES, out_features=args.K, bias=args.bias)\n",
    "ground_truth.weight = nn.Parameter(U.sample(ch.Size([args.K, args.IN_FEATURES])))\n",
    "if ground_truth.bias is not None: \n",
    "    ground_truth.bias = nn.Parameter(U.sample(ch.Size([args.K,])))\n",
    "print(\"ground truth weights: {}\".format(ground_truth.weight))\n",
    "print(\"ground truth bias: {}\".format(ground_truth.bias))\n",
    "# independent variable \n",
    "U_ = Uniform(-5, 5)\n",
    "X = U_.sample(ch.Size([args.samples, args.IN_FEATURES]))\n",
    "# determine base model logits \n",
    "z = ground_truth(X)\n",
    "# apply softmax to unnormalized likelihoods\n",
    "y = ch.argmax(ch.nn.Softmax(dim=1)(z), dim=1)\n",
    "\n",
    "# TRUNCATE\n",
    "trunc = phi(z)\n",
    "indices = ch.all(trunc.bool(), dim=1).float().nonzero(as_tuple=False).flatten()\n",
    "y_trunc = y[indices]\n",
    "x_trunc = X[indices]\n",
    "alpha = x_trunc.size(0) / X.size(0)\n",
    "print(\"alpha: {}\".format(alpha))\n",
    "\n",
    "# all synthetic data \n",
    "ds = TensorDataset(x_trunc, y_trunc)\n",
    "# split ds into training and validation data sets\n",
    "train_length = int(len(ds)*.8)\n",
    "val_length = len(ds) - train_length\n",
    "train_ds, val_ds = ch.utils.data.random_split(ds, [train_length, val_length])\n",
    "# train and validation loaders\n",
    "train_loader = DataLoader(train_ds, num_workers=args.num_workers, batch_size=args.batch_size)\n",
    "val_loader = DataLoader(val_ds, num_workers=args.num_workers, batch_size=args.batch_size)\n",
    "\n",
    "# test dataset\n",
    "y_test = y[~indices]\n",
    "x_test = X[~indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging in: /home/pstefanou/MultinomialLogisticRegression/96b5298c-b14c-4491-973c-60d48a442084\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<cox.store.Table at 0x7faf2f577f28>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRUNC CE LOSS TABLE FOR METRICS\n",
    "LATENT_CE_TABLE_NAME = 'trunc_multi_log_reg'\n",
    "\n",
    "STORE_PATH = '/home/pstefanou/MultinomialLogisticRegression'\n",
    "store = Store(STORE_PATH)\n",
    "\n",
    "store.add_table(LATENT_CE_TABLE_NAME, { \n",
    "    'trunc_train_acc': float, \n",
    "    'trunc_val_acc': float, \n",
    "    'trunc_train_loss': float, \n",
    "    'trunc_val_loss': float,\n",
    "    'naive_train_acc': float, \n",
    "    'naive_val_acc': float, \n",
    "    'naive_train_loss': float, \n",
    "    'naive_val_loss': float,\n",
    "    'trunc_test_acc': float, \n",
    "    'naive_test_acc': float,\n",
    "    'epoch': int,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "trunc_multi_log_reg = nn.Linear(in_features=args.IN_FEATURES, out_features=args.K, bias=args.bias)\n",
    "naive_multi_log_reg = nn.Linear(in_features=args.IN_FEATURES, out_features=args.K, bias=args.bias)\n",
    "# optimizer and scheduler\n",
    "trunc_opt = ch.optim.SGD(trunc_multi_log_reg.parameters(), lr=1e-1)\n",
    "naive_opt = ch.optim.SGD(naive_multi_log_reg.parameters(), lr=1e-1)\n",
    "trunc_scheduler = ch.optim.lr_scheduler.CosineAnnealingLR(trunc_opt, args.epochs)\n",
    "naive_scheduler = ch.optim.lr_scheduler.CosineAnnealingLR(naive_opt, args.epochs)\n",
    "trunc_ce_loss = TruncatedGumbelCE.apply\n",
    "ce_loss = ch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(args.trials):\n",
    "    for epoch in range(args.epochs): \n",
    "        # train loop\n",
    "        trunc_train_loss, trunc_train_acc = Tensor([]), Tensor([])\n",
    "        naive_train_loss, naive_train_acc = Tensor([]), Tensor([])\n",
    "        for batch_X, batch_y in train_loader: \n",
    "            # truncated multinomial regression\n",
    "            trunc_opt.zero_grad()\n",
    "            pred = trunc_multi_log_reg(batch_X)\n",
    "            loss = trunc_ce_loss(pred, batch_y, phi)\n",
    "            loss.backward() \n",
    "            trunc_opt.step()\n",
    "            trunc_scheduler.step()\n",
    "            # keep track of truncated algorithm training loss and accuracy\n",
    "            acc = (ch.argmax(ch.nn.Softmax(dim=1)(pred), dim=1) == batch_y).sum() / batch_y.size(0)\n",
    "            trunc_train_loss = ch.cat([trunc_train_loss, Tensor([loss])]) if trunc_train_loss.size() != ch.Size([0]) else Tensor([loss])\n",
    "            trunc_train_acc = ch.cat([trunc_train_acc, Tensor([acc])]) if trunc_train_acc.size() != ch.Size([0]) else Tensor([acc])\n",
    "            \n",
    "            # naive multinomial regression\n",
    "            naive_opt.zero_grad()\n",
    "            pred = naive_multi_log_reg(batch_X)\n",
    "            loss = ce_loss(pred, batch_y)\n",
    "            loss.backward() \n",
    "            naive_opt.step()\n",
    "            naive_scheduler.step()\n",
    "            # keep track of naive algorithm training loss and accuracy\n",
    "            acc = (ch.argmax(ch.nn.Softmax(dim=1)(pred), dim=1) == batch_y).sum() / batch_y.size(0)\n",
    "            naive_train_loss = ch.cat([naive_train_loss, Tensor([loss])]) if naive_train_loss.size() != ch.Size([0]) else Tensor([loss])\n",
    "            naive_train_acc = ch.cat([naive_train_acc, Tensor([acc])]) if naive_train_acc.size() != ch.Size([0]) else Tensor([acc])\n",
    "        # validation loop\n",
    "        trunc_val_loss, trunc_val_acc = Tensor([]), Tensor([])\n",
    "        naive_val_loss, naive_val_acc = Tensor([]), Tensor([])\n",
    "        with ch.no_grad(): \n",
    "            for batch_X, batch_y in val_loader: \n",
    "                # truncated validation loop\n",
    "                pred = trunc_multi_log_reg(batch_X)\n",
    "                loss = trunc_ce_loss(pred, batch_y, phi)\n",
    "                # keep track of algorithm validation loss and accuracy\n",
    "                acc = (ch.argmax(ch.nn.Softmax(dim=1)(pred), dim=1) == batch_y).sum() / batch_y.size(0)            \n",
    "                trunc_val_loss = ch.cat([trunc_val_loss, Tensor([loss])]) if trunc_val_loss.size() != ch.Size([0]) else Tensor([loss])\n",
    "                trunc_val_acc = ch.cat([trunc_val_acc, Tensor([acc])]) if trunc_val_acc.size() != ch.Size([0]) else Tensor([acc])\n",
    "                \n",
    "                # naive validation loop\n",
    "                pred = naive_multi_log_reg(batch_X)\n",
    "                loss = ce_loss(pred, batch_y)\n",
    "                # keep track of algorithm validation loss and accuracy\n",
    "                acc = (ch.argmax(ch.nn.Softmax(dim=1)(pred), dim=1) == batch_y).sum() / batch_y.size(0)            \n",
    "                naive_val_loss = ch.cat([naive_val_loss, Tensor([loss])]) if naive_val_loss.size() != ch.Size([0]) else Tensor([loss])\n",
    "                naive_val_acc = ch.cat([naive_val_acc, Tensor([acc])]) if naive_val_acc.size() != ch.Size([0]) else Tensor([acc])\n",
    "\n",
    "            # test set accuracy\n",
    "            trunc_test_pred = trunc_multi_log_reg(x_test)\n",
    "            naive_test_pred = naive_multi_log_reg(x_test)\n",
    "\n",
    "            trunc_test_acc = (ch.argmax(ch.nn.Softmax(dim=1)(trunc_test_pred), dim=1) == y_test).sum() / y.size(0)\n",
    "            naive_test_acc = (ch.argmax(ch.nn.Softmax(dim=1)(naive_test_pred), dim=1) == y_test).sum() / y.size(0)\n",
    "                \n",
    "        store[LATENT_CE_TABLE_NAME].append_row({ \n",
    "            'trunc_train_acc': float(trunc_train_acc.mean()), \n",
    "            'trunc_val_acc': float(trunc_val_acc.mean()), \n",
    "            'trunc_train_loss': float(trunc_train_loss.mean()), \n",
    "            'trunc_val_loss': float(trunc_val_loss.mean()),\n",
    "            'naive_train_acc': float(naive_train_acc.mean()), \n",
    "            'naive_val_acc': float(naive_val_acc.mean()), \n",
    "            'naive_train_loss': float(naive_train_loss.mean()), \n",
    "            'naive_val_loss': float(naive_val_loss.mean()),\n",
    "            'trunc_test_acc': float(trunc_test_acc), \n",
    "            'naive_test_acc': float(naive_test_acc),\n",
    "            'epoch': int(epoch + 1),\n",
    "        })\n",
    "    \n",
    "store.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Experiment Data from Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = CollectionReader(STORE_PATH)\n",
    "results = reader.df(LATENT_CE_TABLE_NAME)\n",
    "reader.close() # close reader\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and Accuracy Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=results, x='epoch', y='trunc_train_loss', label='trunc train loss')\n",
    "sns.lineplot(data=results, x='epoch', y='naive_train_loss', label='naive train loss')\n",
    "sns.lineplot(data=results, x='epoch', y='trunc_val_loss', color='red', label='trunc val loss')\n",
    "ax = sns.lineplot(data=results, x='epoch', y='naive_val_loss', color='red', label='naive val loss')\n",
    "ax.set(xlabel='epoch', ylabel='CE Loss')\n",
    "plt.show()\n",
    "\n",
    "sns.lineplot(data=results, x='epoch', y='trunc_train_acc', label='trunc train acc')\n",
    "sns.lineplot(data=results, x='epoch', y='naive_train_acc', label='naive train acc')\n",
    "sns.lineplot(data=results, x='epoch', y='trunc_val_acc', label='trunc val acc')\n",
    "ax = sns.lineplot(data=results, x='epoch', y='naive_val_acc', label='naive val acc')\n",
    "ax.set(xlabel='epoch', ylabel='Accuracy')\n",
    "plt.show()\n",
    "\n",
    "sns.lineplot(data=results, x='epoch', y='trunc_test_acc', label='trunc test acc')\n",
    "ax = sns.lineplot(data=results, x='epoch', y='naive_test_acc', label='naive test acc')\n",
    "ax.set(xlabel='epoch', ylabel='Test Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-2.6573,  3.4919,  1.0961],\n",
       "        [-0.8624, -3.2136, -1.1666],\n",
       "        [ 1.6849,  0.0851, -0.1340]], requires_grad=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trunc_multi_log_reg.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.9136,  3.4561,  1.1448],\n",
       "        [-0.1245, -3.2757, -1.1225],\n",
       "        [ 2.1877,  0.1857, -0.0396]], requires_grad=True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_multi_log_reg.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.3503, -1.2745,  1.3401], requires_grad=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_multi_log_reg.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.2135, -0.6709,  1.9963], requires_grad=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trunc_multi_log_reg.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.7573,  0.9134,  0.2062],\n",
       "        [-0.2224, -0.9683, -0.4470],\n",
       "        [ 0.3331, -0.1171, -0.1744]], requires_grad=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.3866, -0.6208,  0.1278], requires_grad=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
