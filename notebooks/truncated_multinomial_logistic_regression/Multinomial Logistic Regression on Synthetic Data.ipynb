{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../..')\n",
    "from cox.utils import Parameters\n",
    "from cox.store import Store\n",
    "from cox.readers import CollectionReader\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import itertools\n",
    "import numpy as np\n",
    "import torch as ch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Gumbel, Uniform\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import datetime\n",
    "from delphi.oracle import oracle\n",
    "\n",
    "# set default tensor type \n",
    "ch.set_default_tensor_type(ch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# procedure hyperparameters\n",
    "args = Parameters({ \n",
    "    'epochs': 25,\n",
    "    'num_workers': 0, \n",
    "    'batch_size': 100,\n",
    "    'bias': True,\n",
    "    'num_samples': 1000,\n",
    "    'clamp': True, \n",
    "    'radius': 5.0, \n",
    "    'var_lr': 1e-2,\n",
    "    'lr': 1e-1,\n",
    "    'shuffle': False, \n",
    "    'samples': 10000,  # number of samples to generate for ground truth\n",
    "    'in_features': 10, # number of in-features to multi-log-reg\n",
    "    'k': 10, # number of classes\n",
    "    'lower': -1, # lower bound for generating ground truth weights\n",
    "    'upper': 1,  # upper bound for generating ground truth weights\n",
    "    'trials': 10,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CE Latent Variable Model Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "gumbel = Gumbel(0, 1)\n",
    "\n",
    "class GumbelCE(ch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, pred, targ):\n",
    "        ctx.save_for_backward(pred, targ)\n",
    "        loss = ch.nn.CrossEntropyLoss()\n",
    "        return loss(pred, targ)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        pred, targ = ctx.saved_tensors\n",
    "        # make num_samples copies of pred logits\n",
    "        stacked = pred[None, ...].repeat(1000, 1, 1)        \n",
    "        # add gumbel noise to logits\n",
    "        rand_noise = gumbel.sample(stacked.size())\n",
    "        noised = stacked + rand_noise \n",
    "        noised_labs = noised.argmax(-1)\n",
    "        # remove the logits from the trials, where the kth logit is not the largest value\n",
    "        good_mask = noised_labs.eq(targ)[..., None]\n",
    "        inner_exp = 1 - ch.exp(-rand_noise)\n",
    "        avg = (inner_exp * good_mask).sum(0) / (good_mask.sum(0) + 1e-5) / pred.size(0)\n",
    "        return -avg , None\n",
    "    \n",
    "class TruncatedGumbelCE(ch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, pred, targ, phi):\n",
    "        ctx.save_for_backward(pred, targ)\n",
    "        ctx.phi = phi\n",
    "        ce_loss = ch.nn.CrossEntropyLoss()\n",
    "        return ce_loss(pred, targ)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        pred, targ = ctx.saved_tensors\n",
    "        # make num_samples copies of pred logits\n",
    "        stacked = pred[None, ...].repeat(args.num_samples, 1, 1)   \n",
    "        # add gumbel noise to logits\n",
    "        rand_noise = gumbel.sample(stacked.size())\n",
    "        noised = stacked + rand_noise \n",
    "        # truncate - if one of the noisy logits does not fall within the truncation set, remove it\n",
    "        filtered = ch.all(ctx.phi(noised).bool(), dim=2).float().unsqueeze(2)\n",
    "        noised_labs = noised.argmax(-1)\n",
    "        # mask takes care of invalid logits and truncation set\n",
    "        mask = noised_labs.eq(targ)[..., None] * filtered\n",
    "        inner_exp = 1 - ch.exp(-rand_noise)\n",
    "                \n",
    "        avg = ((inner_exp * mask).sum(0) / (mask.sum(0) + 1e-5) - (inner_exp * filtered).sum(0) / (filtered.sum(0) + 1e-5)) \n",
    "        return -avg / pred.size(0), None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truncated Multinomial Logistic Regression Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Membership oracles for Multinomial Logistic Regression Logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_Lower(oracle): \n",
    "    \"\"\"\n",
    "    Lower bound truncation on the DNN logits.\n",
    "    \"\"\"\n",
    "    def __init__(self, lower): \n",
    "        self.lower = lower\n",
    "        \n",
    "    def __call__(self, x): \n",
    "        return (x > self.lower).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(oracle): \n",
    "    def __call__(self, x): \n",
    "        return ch.ones(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truncate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = DNN_Lower(ch.full(ch.Size([args.K,]), -5))\n",
    "# phi = Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging in: /home/pstefanou/MultinomialLogisticRegression/6223ad17-746c-428e-b3c2-27349706ff4f\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<cox.store.Table at 0x7faf47940048>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRUNC CE LOSS TABLE FOR METRICS\n",
    "LATENT_CE_TABLE_NAME = 'trunc_test_9'\n",
    "\n",
    "STORE_PATH = '/home/pstefanou/MultinomialLogisticRegression'\n",
    "store = Store(STORE_PATH)\n",
    "\n",
    "store.add_table(LATENT_CE_TABLE_NAME, { \n",
    "    'trunc_train_acc': float, \n",
    "    'trunc_val_acc': float, \n",
    "    'trunc_train_loss': float, \n",
    "    'trunc_val_loss': float,\n",
    "    'naive_train_acc': float, \n",
    "    'naive_val_acc': float, \n",
    "    'naive_train_loss': float, \n",
    "    'naive_val_loss': float,\n",
    "    'trunc_test_acc': float, \n",
    "    'naive_test_acc': float,\n",
    "    'epoch': int,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.1152\n",
      "alpha: 0.2724\n",
      "alpha: 0.1377\n",
      "alpha: 0.1331\n",
      "alpha: 0.1183\n",
      "alpha: 0.1196\n",
      "alpha: 0.1312\n",
      "alpha: 0.1542\n",
      "alpha: 0.1612\n",
      "alpha: 0.174\n",
      "alpha: 0.1066\n",
      "alpha: 0.1641\n",
      "alpha: 0.1478\n",
      "alpha: 0.1903\n",
      "alpha: 0.1547\n",
      "alpha: 0.166\n",
      "alpha: 0.223\n",
      "alpha: 0.1476\n",
      "alpha: 0.13\n",
      "alpha: 0.149\n",
      "alpha: 0.1802\n",
      "alpha: 0.1276\n",
      "alpha: 0.1405\n",
      "alpha: 0.1186\n",
      "alpha: 0.1302\n",
      "alpha: 0.1231\n",
      "alpha: 0.106\n",
      "alpha: 0.1405\n",
      "alpha: 0.1402\n",
      "alpha: 0.1364\n",
      "alpha: 0.1805\n",
      "alpha: 0.139\n",
      "alpha: 0.1607\n",
      "alpha: 0.1404\n",
      "alpha: 0.1591\n",
      "alpha: 0.1618\n",
      "alpha: 0.1549\n",
      "alpha: 0.1814\n",
      "alpha: 0.1738\n",
      "alpha: 0.1498\n",
      "alpha: 0.1589\n",
      "alpha: 0.1504\n",
      "alpha: 0.1764\n",
      "alpha: 0.1211\n",
      "alpha: 0.0652\n",
      "alpha: 0.0725\n",
      "alpha: 0.1755\n",
      "alpha: 0.1021\n",
      "alpha: 0.2049\n",
      "alpha: 0.2368\n",
      "alpha: 0.2531\n",
      "alpha: 0.1472\n",
      "alpha: 0.1614\n",
      "alpha: 0.0866\n",
      "alpha: 0.1682\n",
      "alpha: 0.0657\n",
      "alpha: 0.1978\n",
      "alpha: 0.2124\n",
      "alpha: 0.1827\n",
      "alpha: 0.214\n",
      "alpha: 0.2314\n",
      "alpha: 0.1567\n",
      "alpha: 0.1317\n",
      "alpha: 0.1443\n",
      "alpha: 0.2134\n",
      "alpha: 0.1369\n",
      "alpha: 0.1004\n",
      "alpha: 0.1216\n",
      "alpha: 0.1173\n",
      "alpha: 0.1389\n",
      "alpha: 0.134\n",
      "alpha: 0.194\n",
      "alpha: 0.1359\n",
      "alpha: 0.1531\n",
      "alpha: 0.1188\n",
      "alpha: 0.1869\n",
      "alpha: 0.1974\n",
      "alpha: 0.1941\n",
      "alpha: 0.1778\n",
      "alpha: 0.2317\n",
      "alpha: 0.1802\n",
      "alpha: 0.167\n",
      "alpha: 0.1508\n",
      "alpha: 0.1355\n",
      "alpha: 0.1825\n",
      "alpha: 0.2601\n",
      "alpha: 0.1411\n",
      "alpha: 0.1189\n",
      "alpha: 0.1783\n",
      "alpha: 0.22\n",
      "alpha: 0.1793\n",
      "alpha: 0.1937\n",
      "alpha: 0.2544\n",
      "alpha: 0.1505\n",
      "alpha: 0.2139\n",
      "alpha: 0.1231\n",
      "alpha: 0.2412\n",
      "alpha: 0.1115\n",
      "alpha: 0.143\n",
      "alpha: 0.1508\n",
      "alpha: 0.171\n",
      "alpha: 0.1945\n",
      "alpha: 0.207\n",
      "alpha: 0.1309\n",
      "alpha: 0.1919\n",
      "alpha: 0.1195\n",
      "alpha: 0.1507\n",
      "alpha: 0.1971\n",
      "alpha: 0.1868\n",
      "alpha: 0.1605\n",
      "alpha: 0.1411\n",
      "alpha: 0.1516\n",
      "alpha: 0.111\n",
      "alpha: 0.1658\n",
      "alpha: 0.2159\n",
      "alpha: 0.1726\n",
      "alpha: 0.1582\n",
      "alpha: 0.0849\n",
      "alpha: 0.1568\n",
      "alpha: 0.0975\n",
      "alpha: 0.1818\n",
      "alpha: 0.1671\n",
      "alpha: 0.1451\n",
      "alpha: 0.1769\n",
      "alpha: 0.1481\n",
      "alpha: 0.1405\n",
      "alpha: 0.1447\n",
      "alpha: 0.1712\n",
      "alpha: 0.15\n",
      "alpha: 0.137\n",
      "alpha: 0.2079\n",
      "alpha: 0.1705\n",
      "alpha: 0.1638\n",
      "alpha: 0.2358\n",
      "alpha: 0.0981\n",
      "alpha: 0.16\n",
      "alpha: 0.1548\n",
      "alpha: 0.1837\n",
      "alpha: 0.1533\n",
      "alpha: 0.1651\n",
      "alpha: 0.178\n",
      "alpha: 0.1334\n",
      "alpha: 0.1884\n",
      "alpha: 0.1382\n",
      "alpha: 0.1887\n",
      "alpha: 0.1662\n",
      "alpha: 0.2496\n",
      "alpha: 0.1382\n",
      "alpha: 0.2191\n",
      "alpha: 0.1857\n",
      "alpha: 0.1972\n",
      "alpha: 0.1663\n",
      "alpha: 0.1285\n",
      "alpha: 0.1512\n",
      "alpha: 0.2115\n",
      "alpha: 0.1706\n",
      "alpha: 0.1377\n",
      "alpha: 0.1118\n",
      "alpha: 0.1914\n",
      "alpha: 0.1175\n",
      "alpha: 0.1365\n",
      "alpha: 0.1728\n",
      "alpha: 0.1651\n",
      "alpha: 0.1812\n",
      "alpha: 0.1303\n",
      "alpha: 0.1915\n",
      "alpha: 0.2152\n",
      "alpha: 0.1049\n",
      "alpha: 0.1426\n",
      "alpha: 0.1487\n",
      "alpha: 0.1264\n",
      "alpha: 0.1862\n",
      "alpha: 0.1212\n",
      "alpha: 0.1948\n",
      "alpha: 0.0801\n",
      "alpha: 0.1787\n",
      "alpha: 0.16\n",
      "alpha: 0.1601\n",
      "alpha: 0.1053\n",
      "alpha: 0.1446\n",
      "alpha: 0.2467\n",
      "alpha: 0.1327\n",
      "alpha: 0.2199\n",
      "alpha: 0.135\n",
      "alpha: 0.1326\n",
      "alpha: 0.1382\n",
      "alpha: 0.1773\n",
      "alpha: 0.2714\n",
      "alpha: 0.1514\n",
      "alpha: 0.1681\n",
      "alpha: 0.1456\n",
      "alpha: 0.1197\n",
      "alpha: 0.1129\n",
      "alpha: 0.1448\n",
      "alpha: 0.1367\n",
      "alpha: 0.1623\n",
      "alpha: 0.1529\n",
      "alpha: 0.2038\n",
      "alpha: 0.1523\n",
      "alpha: 0.0922\n",
      "alpha: 0.1669\n",
      "alpha: 0.1699\n",
      "alpha: 0.101\n",
      "alpha: 0.1449\n",
      "alpha: 0.1734\n",
      "alpha: 0.1177\n",
      "alpha: 0.201\n",
      "alpha: 0.1671\n",
      "alpha: 0.13\n",
      "alpha: 0.2496\n",
      "alpha: 0.1694\n",
      "alpha: 0.1499\n",
      "alpha: 0.1883\n",
      "alpha: 0.1271\n",
      "alpha: 0.1157\n",
      "alpha: 0.1355\n",
      "alpha: 0.1055\n",
      "alpha: 0.1524\n",
      "alpha: 0.1202\n",
      "alpha: 0.1691\n",
      "alpha: 0.1914\n",
      "alpha: 0.1653\n",
      "alpha: 0.0613\n",
      "alpha: 0.2622\n",
      "alpha: 0.1845\n",
      "alpha: 0.1796\n",
      "alpha: 0.1597\n",
      "alpha: 0.1693\n",
      "alpha: 0.1114\n",
      "alpha: 0.1498\n",
      "alpha: 0.201\n",
      "alpha: 0.0915\n",
      "alpha: 0.216\n",
      "alpha: 0.2054\n",
      "alpha: 0.1104\n",
      "alpha: 0.1437\n",
      "alpha: 0.1709\n",
      "alpha: 0.1879\n",
      "alpha: 0.2148\n",
      "alpha: 0.1571\n",
      "alpha: 0.1629\n",
      "alpha: 0.1696\n",
      "alpha: 0.1059\n",
      "alpha: 0.108\n",
      "alpha: 0.1152\n",
      "alpha: 0.1581\n",
      "alpha: 0.1581\n",
      "alpha: 0.1325\n",
      "alpha: 0.1707\n",
      "alpha: 0.1833\n",
      "alpha: 0.2288\n",
      "alpha: 0.1271\n",
      "alpha: 0.1648\n",
      "alpha: 0.1465\n",
      "alpha: 0.1207\n",
      "alpha: 0.1314\n",
      "alpha: 0.1235\n",
      "alpha: 0.1413\n",
      "alpha: 0.2265\n",
      "alpha: 0.1724\n",
      "alpha: 0.216\n",
      "alpha: 0.1648\n",
      "alpha: 0.17\n",
      "alpha: 0.09\n",
      "alpha: 0.1561\n",
      "alpha: 0.1458\n",
      "alpha: 0.2438\n",
      "alpha: 0.1926\n",
      "alpha: 0.1559\n",
      "alpha: 0.2461\n",
      "alpha: 0.1416\n",
      "alpha: 0.2035\n",
      "alpha: 0.198\n",
      "alpha: 0.1195\n",
      "alpha: 0.1629\n",
      "alpha: 0.2843\n",
      "alpha: 0.1771\n",
      "alpha: 0.2633\n",
      "alpha: 0.1079\n",
      "alpha: 0.1205\n",
      "alpha: 0.1298\n",
      "alpha: 0.1636\n",
      "alpha: 0.2302\n",
      "alpha: 0.1617\n",
      "alpha: 0.1778\n",
      "alpha: 0.1579\n",
      "alpha: 0.1843\n",
      "alpha: 0.1212\n",
      "alpha: 0.0826\n",
      "alpha: 0.1199\n",
      "alpha: 0.1632\n",
      "alpha: 0.1895\n",
      "alpha: 0.2127\n",
      "alpha: 0.2055\n",
      "alpha: 0.1559\n",
      "alpha: 0.0982\n",
      "alpha: 0.172\n",
      "alpha: 0.0917\n",
      "alpha: 0.261\n",
      "alpha: 0.1705\n",
      "alpha: 0.1233\n",
      "alpha: 0.153\n",
      "alpha: 0.207\n",
      "alpha: 0.1116\n",
      "alpha: 0.1512\n",
      "alpha: 0.1965\n",
      "alpha: 0.1826\n",
      "alpha: 0.1613\n",
      "alpha: 0.2284\n",
      "alpha: 0.1121\n",
      "alpha: 0.1691\n",
      "alpha: 0.1868\n",
      "alpha: 0.2048\n",
      "alpha: 0.1748\n",
      "alpha: 0.1613\n",
      "alpha: 0.139\n",
      "alpha: 0.2252\n",
      "alpha: 0.1703\n",
      "alpha: 0.2306\n",
      "alpha: 0.1944\n",
      "alpha: 0.152\n",
      "alpha: 0.1078\n",
      "alpha: 0.1878\n",
      "alpha: 0.1872\n",
      "alpha: 0.2806\n",
      "alpha: 0.1579\n",
      "alpha: 0.1328\n",
      "alpha: 0.0964\n",
      "alpha: 0.1933\n",
      "alpha: 0.1305\n",
      "alpha: 0.119\n",
      "alpha: 0.0788\n",
      "alpha: 0.1686\n",
      "alpha: 0.1295\n",
      "alpha: 0.1723\n",
      "alpha: 0.1881\n",
      "alpha: 0.1871\n",
      "alpha: 0.1464\n",
      "alpha: 0.1885\n",
      "alpha: 0.1808\n",
      "alpha: 0.2349\n",
      "alpha: 0.134\n",
      "alpha: 0.1259\n",
      "alpha: 0.1676\n",
      "alpha: 0.1716\n",
      "alpha: 0.1299\n",
      "alpha: 0.1407\n",
      "alpha: 0.1752\n",
      "alpha: 0.2188\n",
      "alpha: 0.1268\n",
      "alpha: 0.244\n",
      "alpha: 0.1863\n",
      "alpha: 0.1885\n",
      "alpha: 0.1813\n",
      "alpha: 0.2673\n",
      "alpha: 0.1616\n",
      "alpha: 0.1975\n",
      "alpha: 0.1679\n",
      "alpha: 0.2035\n",
      "alpha: 0.2275\n",
      "alpha: 0.1975\n",
      "alpha: 0.1467\n",
      "alpha: 0.2067\n",
      "alpha: 0.1742\n",
      "alpha: 0.1551\n",
      "alpha: 0.2425\n",
      "alpha: 0.1436\n",
      "alpha: 0.2417\n",
      "alpha: 0.1834\n",
      "alpha: 0.1626\n",
      "alpha: 0.1751\n",
      "alpha: 0.1746\n",
      "alpha: 0.2906\n",
      "alpha: 0.1471\n",
      "alpha: 0.1403\n",
      "alpha: 0.1587\n",
      "alpha: 0.1268\n",
      "alpha: 0.2152\n",
      "alpha: 0.1254\n",
      "alpha: 0.175\n",
      "alpha: 0.1396\n",
      "alpha: 0.1162\n",
      "alpha: 0.0948\n",
      "alpha: 0.0906\n",
      "alpha: 0.1325\n",
      "alpha: 0.2342\n",
      "alpha: 0.2222\n",
      "alpha: 0.1741\n",
      "alpha: 0.1933\n",
      "alpha: 0.1021\n",
      "alpha: 0.2058\n",
      "alpha: 0.1519\n",
      "alpha: 0.1957\n",
      "alpha: 0.1791\n",
      "alpha: 0.2284\n",
      "alpha: 0.215\n",
      "alpha: 0.1368\n",
      "alpha: 0.1591\n",
      "alpha: 0.2143\n",
      "alpha: 0.1143\n",
      "alpha: 0.2422\n",
      "alpha: 0.1788\n",
      "alpha: 0.1119\n",
      "alpha: 0.1814\n",
      "alpha: 0.1245\n",
      "alpha: 0.0932\n",
      "alpha: 0.129\n",
      "alpha: 0.1425\n",
      "alpha: 0.2203\n",
      "alpha: 0.1503\n",
      "alpha: 0.211\n",
      "alpha: 0.1406\n",
      "alpha: 0.1763\n",
      "alpha: 0.1908\n",
      "alpha: 0.1821\n",
      "alpha: 0.1753\n",
      "alpha: 0.149\n",
      "alpha: 0.1298\n",
      "alpha: 0.1771\n",
      "alpha: 0.1923\n",
      "alpha: 0.1787\n",
      "alpha: 0.2316\n",
      "alpha: 0.2249\n",
      "alpha: 0.1852\n",
      "alpha: 0.1414\n",
      "alpha: 0.1696\n",
      "alpha: 0.2608\n",
      "alpha: 0.2349\n",
      "alpha: 0.1632\n",
      "alpha: 0.1922\n",
      "alpha: 0.1682\n",
      "alpha: 0.0918\n",
      "alpha: 0.1504\n",
      "alpha: 0.172\n",
      "alpha: 0.169\n",
      "alpha: 0.1413\n",
      "alpha: 0.1086\n",
      "alpha: 0.2008\n",
      "alpha: 0.1723\n",
      "alpha: 0.1819\n",
      "alpha: 0.099\n",
      "alpha: 0.136\n",
      "alpha: 0.1237\n",
      "alpha: 0.2116\n",
      "alpha: 0.1259\n",
      "alpha: 0.158\n",
      "alpha: 0.1535\n",
      "alpha: 0.141\n",
      "alpha: 0.1918\n",
      "alpha: 0.1549\n",
      "alpha: 0.1698\n",
      "alpha: 0.2321\n",
      "alpha: 0.1635\n",
      "alpha: 0.1977\n",
      "alpha: 0.1734\n",
      "alpha: 0.0946\n",
      "alpha: 0.1587\n",
      "alpha: 0.1459\n",
      "alpha: 0.1906\n",
      "alpha: 0.1867\n",
      "alpha: 0.1177\n",
      "alpha: 0.1657\n",
      "alpha: 0.2582\n",
      "alpha: 0.1482\n",
      "alpha: 0.2518\n",
      "alpha: 0.1309\n",
      "alpha: 0.1476\n",
      "alpha: 0.1273\n",
      "alpha: 0.1413\n",
      "alpha: 0.1864\n",
      "alpha: 0.1521\n",
      "alpha: 0.2008\n",
      "alpha: 0.2022\n",
      "alpha: 0.1138\n",
      "alpha: 0.1193\n",
      "alpha: 0.2042\n",
      "alpha: 0.2072\n",
      "alpha: 0.1076\n",
      "alpha: 0.235\n",
      "alpha: 0.169\n",
      "alpha: 0.2575\n",
      "alpha: 0.1724\n",
      "alpha: 0.1134\n",
      "alpha: 0.1497\n",
      "alpha: 0.1748\n",
      "alpha: 0.1821\n",
      "alpha: 0.2024\n",
      "alpha: 0.1053\n",
      "alpha: 0.1589\n",
      "alpha: 0.1586\n",
      "alpha: 0.2145\n",
      "alpha: 0.2636\n",
      "alpha: 0.136\n",
      "alpha: 0.2035\n",
      "alpha: 0.18\n",
      "alpha: 0.1102\n",
      "alpha: 0.0947\n",
      "alpha: 0.2051\n",
      "alpha: 0.1921\n",
      "alpha: 0.1502\n",
      "alpha: 0.0964\n",
      "alpha: 0.1349\n",
      "alpha: 0.1403\n",
      "alpha: 0.2207\n",
      "alpha: 0.202\n",
      "alpha: 0.1527\n",
      "alpha: 0.1343\n",
      "alpha: 0.1624\n",
      "alpha: 0.1429\n",
      "alpha: 0.1289\n",
      "alpha: 0.1944\n",
      "alpha: 0.149\n",
      "alpha: 0.087\n",
      "alpha: 0.1827\n",
      "alpha: 0.1644\n",
      "alpha: 0.1346\n",
      "alpha: 0.1411\n",
      "alpha: 0.1316\n",
      "alpha: 0.1313\n",
      "alpha: 0.1467\n",
      "alpha: 0.1985\n",
      "alpha: 0.1126\n",
      "alpha: 0.1867\n",
      "alpha: 0.1697\n",
      "alpha: 0.1887\n",
      "alpha: 0.1449\n",
      "alpha: 0.1082\n",
      "alpha: 0.1563\n",
      "alpha: 0.1639\n",
      "alpha: 0.1535\n",
      "alpha: 0.135\n",
      "alpha: 0.135\n",
      "alpha: 0.2046\n",
      "alpha: 0.2404\n",
      "alpha: 0.1261\n",
      "alpha: 0.1789\n",
      "alpha: 0.1535\n",
      "alpha: 0.1776\n",
      "alpha: 0.1802\n",
      "alpha: 0.2689\n",
      "alpha: 0.2034\n",
      "alpha: 0.1824\n",
      "alpha: 0.1454\n",
      "alpha: 0.1465\n",
      "alpha: 0.1277\n",
      "alpha: 0.1149\n",
      "alpha: 0.2045\n",
      "alpha: 0.181\n",
      "alpha: 0.2014\n",
      "alpha: 0.1389\n",
      "alpha: 0.1038\n",
      "alpha: 0.1429\n",
      "alpha: 0.136\n",
      "alpha: 0.2336\n",
      "alpha: 0.1396\n",
      "alpha: 0.2335\n",
      "alpha: 0.1369\n",
      "alpha: 0.1408\n",
      "alpha: 0.1393\n",
      "alpha: 0.1101\n",
      "alpha: 0.2613\n",
      "alpha: 0.2032\n",
      "alpha: 0.2052\n",
      "alpha: 0.1651\n",
      "alpha: 0.1041\n",
      "alpha: 0.1974\n",
      "alpha: 0.1982\n",
      "alpha: 0.1514\n",
      "alpha: 0.1736\n",
      "alpha: 0.1792\n",
      "alpha: 0.15\n",
      "alpha: 0.1699\n",
      "alpha: 0.1577\n",
      "alpha: 0.1255\n",
      "alpha: 0.0958\n",
      "alpha: 0.1867\n",
      "alpha: 0.1237\n",
      "alpha: 0.1668\n",
      "alpha: 0.1719\n",
      "alpha: 0.1023\n",
      "alpha: 0.1445\n",
      "alpha: 0.1598\n",
      "alpha: 0.1004\n",
      "alpha: 0.1121\n",
      "alpha: 0.1902\n",
      "alpha: 0.1198\n",
      "alpha: 0.1102\n",
      "alpha: 0.0782\n",
      "alpha: 0.2016\n",
      "alpha: 0.1393\n",
      "alpha: 0.2008\n",
      "alpha: 0.1377\n",
      "alpha: 0.2326\n",
      "alpha: 0.2234\n",
      "alpha: 0.103\n",
      "alpha: 0.1634\n",
      "alpha: 0.1977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.1726\n",
      "alpha: 0.2101\n",
      "alpha: 0.2019\n",
      "alpha: 0.1365\n",
      "alpha: 0.185\n",
      "alpha: 0.2223\n",
      "alpha: 0.1928\n",
      "alpha: 0.1648\n",
      "alpha: 0.1325\n",
      "alpha: 0.1499\n",
      "alpha: 0.1395\n",
      "alpha: 0.1256\n",
      "alpha: 0.1264\n",
      "alpha: 0.1512\n",
      "alpha: 0.2451\n",
      "alpha: 0.1334\n",
      "alpha: 0.1392\n",
      "alpha: 0.1756\n",
      "alpha: 0.1445\n",
      "alpha: 0.268\n",
      "alpha: 0.1614\n",
      "alpha: 0.1558\n",
      "alpha: 0.1353\n",
      "alpha: 0.1722\n",
      "alpha: 0.1375\n",
      "alpha: 0.1088\n",
      "alpha: 0.206\n",
      "alpha: 0.1179\n",
      "alpha: 0.1973\n",
      "alpha: 0.2084\n",
      "alpha: 0.1982\n",
      "alpha: 0.1382\n",
      "alpha: 0.1448\n",
      "alpha: 0.1096\n",
      "alpha: 0.2414\n",
      "alpha: 0.1034\n",
      "alpha: 0.1452\n",
      "alpha: 0.2191\n",
      "alpha: 0.1964\n",
      "alpha: 0.1608\n",
      "alpha: 0.0529\n",
      "alpha: 0.1404\n",
      "alpha: 0.1686\n",
      "alpha: 0.2244\n",
      "alpha: 0.1921\n",
      "alpha: 0.2333\n",
      "alpha: 0.2062\n",
      "alpha: 0.2419\n",
      "alpha: 0.1663\n",
      "alpha: 0.1837\n",
      "alpha: 0.062\n",
      "alpha: 0.2396\n",
      "alpha: 0.1174\n",
      "alpha: 0.2329\n",
      "alpha: 0.1651\n",
      "alpha: 0.1255\n",
      "alpha: 0.1925\n",
      "alpha: 0.134\n",
      "alpha: 0.1576\n",
      "alpha: 0.1644\n",
      "alpha: 0.1959\n",
      "alpha: 0.2137\n",
      "alpha: 0.1305\n",
      "alpha: 0.1214\n",
      "alpha: 0.1492\n",
      "alpha: 0.1259\n",
      "alpha: 0.1639\n",
      "alpha: 0.2264\n",
      "alpha: 0.1435\n",
      "alpha: 0.1597\n",
      "alpha: 0.1587\n",
      "alpha: 0.1198\n",
      "alpha: 0.1188\n",
      "alpha: 0.195\n",
      "alpha: 0.1332\n",
      "alpha: 0.1711\n",
      "alpha: 0.1699\n",
      "alpha: 0.1979\n",
      "alpha: 0.1578\n",
      "alpha: 0.1735\n",
      "alpha: 0.1785\n",
      "alpha: 0.2169\n",
      "alpha: 0.1963\n",
      "alpha: 0.0881\n",
      "alpha: 0.127\n",
      "alpha: 0.1221\n",
      "alpha: 0.1378\n",
      "alpha: 0.1023\n",
      "alpha: 0.1348\n",
      "alpha: 0.2686\n",
      "alpha: 0.1927\n",
      "alpha: 0.1946\n",
      "alpha: 0.1408\n",
      "alpha: 0.1432\n",
      "alpha: 0.172\n",
      "alpha: 0.1632\n",
      "alpha: 0.1152\n",
      "alpha: 0.1416\n",
      "alpha: 0.1338\n",
      "alpha: 0.2099\n",
      "alpha: 0.1503\n",
      "alpha: 0.1743\n",
      "alpha: 0.1614\n",
      "alpha: 0.1822\n",
      "alpha: 0.2241\n",
      "alpha: 0.1455\n",
      "alpha: 0.2364\n",
      "alpha: 0.1629\n",
      "alpha: 0.1811\n",
      "alpha: 0.1755\n",
      "alpha: 0.1682\n",
      "alpha: 0.1323\n",
      "alpha: 0.1646\n",
      "alpha: 0.1978\n",
      "alpha: 0.2098\n",
      "alpha: 0.2007\n",
      "alpha: 0.2126\n",
      "alpha: 0.2066\n",
      "alpha: 0.0747\n",
      "alpha: 0.2443\n",
      "alpha: 0.1918\n",
      "alpha: 0.1833\n",
      "alpha: 0.149\n",
      "alpha: 0.1825\n",
      "alpha: 0.1252\n",
      "alpha: 0.1095\n",
      "alpha: 0.2218\n",
      "alpha: 0.0985\n",
      "alpha: 0.2109\n",
      "alpha: 0.1051\n",
      "alpha: 0.1752\n",
      "alpha: 0.1009\n",
      "alpha: 0.1736\n",
      "alpha: 0.156\n",
      "alpha: 0.1851\n",
      "alpha: 0.1795\n",
      "alpha: 0.166\n",
      "alpha: 0.1379\n",
      "alpha: 0.2062\n",
      "alpha: 0.1307\n",
      "alpha: 0.1671\n",
      "alpha: 0.1772\n",
      "alpha: 0.164\n",
      "alpha: 0.1536\n",
      "alpha: 0.1233\n",
      "alpha: 0.1819\n",
      "alpha: 0.1298\n",
      "alpha: 0.1893\n",
      "alpha: 0.1843\n",
      "alpha: 0.1395\n",
      "alpha: 0.0884\n",
      "alpha: 0.1951\n",
      "alpha: 0.1003\n",
      "alpha: 0.139\n",
      "alpha: 0.1145\n",
      "alpha: 0.1309\n",
      "alpha: 0.1285\n",
      "alpha: 0.1594\n",
      "alpha: 0.2322\n",
      "alpha: 0.1216\n",
      "alpha: 0.1709\n",
      "alpha: 0.1669\n",
      "alpha: 0.1602\n",
      "alpha: 0.1585\n",
      "alpha: 0.1504\n",
      "alpha: 0.1717\n",
      "alpha: 0.1741\n",
      "alpha: 0.1973\n",
      "alpha: 0.122\n",
      "alpha: 0.1502\n",
      "alpha: 0.1725\n",
      "alpha: 0.1365\n",
      "alpha: 0.2081\n",
      "alpha: 0.1515\n",
      "alpha: 0.1454\n",
      "alpha: 0.1667\n",
      "alpha: 0.1735\n",
      "alpha: 0.2318\n",
      "alpha: 0.1511\n",
      "alpha: 0.1189\n",
      "alpha: 0.2321\n",
      "alpha: 0.1882\n",
      "alpha: 0.1891\n",
      "alpha: 0.1693\n",
      "alpha: 0.1553\n",
      "alpha: 0.2213\n",
      "alpha: 0.1065\n",
      "alpha: 0.1489\n",
      "alpha: 0.227\n",
      "alpha: 0.1332\n",
      "alpha: 0.2171\n",
      "alpha: 0.1625\n",
      "alpha: 0.0748\n",
      "alpha: 0.1586\n",
      "alpha: 0.1672\n",
      "alpha: 0.1681\n",
      "alpha: 0.1768\n",
      "alpha: 0.1754\n",
      "alpha: 0.1248\n",
      "alpha: 0.1589\n",
      "alpha: 0.2487\n",
      "alpha: 0.1134\n",
      "alpha: 0.1776\n",
      "alpha: 0.1892\n",
      "alpha: 0.1681\n",
      "alpha: 0.1738\n",
      "alpha: 0.1392\n",
      "alpha: 0.1564\n",
      "alpha: 0.155\n",
      "alpha: 0.2145\n",
      "alpha: 0.1541\n",
      "alpha: 0.221\n",
      "alpha: 0.1031\n",
      "alpha: 0.15\n",
      "alpha: 0.1881\n",
      "alpha: 0.161\n",
      "alpha: 0.1811\n",
      "alpha: 0.1719\n",
      "alpha: 0.1847\n",
      "alpha: 0.1739\n",
      "alpha: 0.1512\n",
      "alpha: 0.1526\n",
      "alpha: 0.1208\n",
      "alpha: 0.2487\n",
      "alpha: 0.1083\n",
      "alpha: 0.15\n",
      "alpha: 0.2742\n",
      "alpha: 0.2076\n",
      "alpha: 0.1072\n",
      "alpha: 0.1838\n",
      "alpha: 0.1878\n",
      "alpha: 0.1719\n",
      "alpha: 0.1891\n",
      "alpha: 0.1018\n",
      "alpha: 0.1969\n",
      "alpha: 0.19\n",
      "alpha: 0.1685\n",
      "alpha: 0.0881\n",
      "alpha: 0.1451\n",
      "alpha: 0.2765\n",
      "alpha: 0.2507\n",
      "alpha: 0.2045\n",
      "alpha: 0.1624\n",
      "alpha: 0.1624\n",
      "alpha: 0.1427\n",
      "alpha: 0.1222\n",
      "alpha: 0.1807\n",
      "alpha: 0.2712\n",
      "alpha: 0.1354\n",
      "alpha: 0.2194\n",
      "alpha: 0.1879\n",
      "alpha: 0.1648\n",
      "alpha: 0.1687\n",
      "alpha: 0.1508\n",
      "alpha: 0.1063\n",
      "alpha: 0.1553\n",
      "alpha: 0.1211\n",
      "alpha: 0.1126\n",
      "alpha: 0.1536\n",
      "alpha: 0.1542\n",
      "alpha: 0.1569\n",
      "alpha: 0.1551\n",
      "alpha: 0.1145\n",
      "alpha: 0.1367\n",
      "alpha: 0.2368\n",
      "alpha: 0.1626\n",
      "alpha: 0.1365\n",
      "alpha: 0.1379\n",
      "alpha: 0.1985\n",
      "alpha: 0.1644\n",
      "alpha: 0.1304\n",
      "alpha: 0.1736\n",
      "alpha: 0.155\n",
      "alpha: 0.2235\n",
      "alpha: 0.2658\n",
      "alpha: 0.2064\n",
      "alpha: 0.2177\n",
      "alpha: 0.2279\n",
      "alpha: 0.2047\n",
      "alpha: 0.1465\n",
      "alpha: 0.261\n",
      "alpha: 0.0926\n",
      "alpha: 0.2012\n",
      "alpha: 0.2541\n",
      "alpha: 0.2052\n",
      "alpha: 0.1778\n",
      "alpha: 0.2185\n",
      "alpha: 0.093\n",
      "alpha: 0.1584\n",
      "alpha: 0.1101\n",
      "alpha: 0.1992\n",
      "alpha: 0.0975\n",
      "alpha: 0.1439\n",
      "alpha: 0.1159\n",
      "alpha: 0.1338\n",
      "alpha: 0.106\n",
      "alpha: 0.1248\n",
      "alpha: 0.2053\n",
      "alpha: 0.2573\n",
      "alpha: 0.1935\n",
      "alpha: 0.1503\n",
      "alpha: 0.2151\n",
      "alpha: 0.1552\n",
      "alpha: 0.1262\n",
      "alpha: 0.0741\n",
      "alpha: 0.1873\n",
      "alpha: 0.2002\n",
      "alpha: 0.1917\n",
      "alpha: 0.1731\n",
      "alpha: 0.2468\n",
      "alpha: 0.1244\n",
      "alpha: 0.1393\n",
      "alpha: 0.082\n",
      "alpha: 0.1511\n",
      "alpha: 0.1627\n",
      "alpha: 0.1707\n",
      "alpha: 0.1725\n",
      "alpha: 0.1595\n",
      "alpha: 0.1371\n",
      "alpha: 0.1071\n",
      "alpha: 0.1657\n",
      "alpha: 0.0908\n",
      "alpha: 0.1621\n",
      "alpha: 0.0657\n",
      "alpha: 0.118\n",
      "alpha: 0.1915\n",
      "alpha: 0.1807\n",
      "alpha: 0.0729\n",
      "alpha: 0.2107\n",
      "alpha: 0.153\n",
      "alpha: 0.1629\n",
      "alpha: 0.1409\n",
      "alpha: 0.1353\n",
      "alpha: 0.158\n",
      "alpha: 0.281\n",
      "alpha: 0.1364\n",
      "alpha: 0.1697\n",
      "alpha: 0.2103\n",
      "alpha: 0.1867\n",
      "alpha: 0.1898\n",
      "alpha: 0.1205\n",
      "alpha: 0.1635\n",
      "alpha: 0.1862\n",
      "alpha: 0.2159\n",
      "alpha: 0.1549\n",
      "alpha: 0.1505\n",
      "alpha: 0.147\n",
      "alpha: 0.142\n",
      "alpha: 0.1575\n",
      "alpha: 0.0802\n",
      "alpha: 0.1943\n",
      "alpha: 0.2435\n",
      "alpha: 0.1115\n",
      "alpha: 0.1861\n",
      "alpha: 0.3153\n",
      "alpha: 0.1932\n",
      "alpha: 0.1096\n",
      "alpha: 0.2026\n",
      "alpha: 0.1688\n",
      "alpha: 0.1191\n",
      "alpha: 0.1453\n",
      "alpha: 0.0857\n",
      "alpha: 0.208\n",
      "alpha: 0.157\n",
      "alpha: 0.1504\n",
      "alpha: 0.1353\n",
      "alpha: 0.1838\n",
      "alpha: 0.1234\n",
      "alpha: 0.1857\n",
      "alpha: 0.1531\n",
      "alpha: 0.204\n",
      "alpha: 0.2408\n",
      "alpha: 0.0996\n",
      "alpha: 0.1734\n",
      "alpha: 0.1288\n",
      "alpha: 0.1514\n",
      "alpha: 0.1166\n",
      "alpha: 0.177\n",
      "alpha: 0.2328\n",
      "alpha: 0.1161\n",
      "alpha: 0.1241\n",
      "alpha: 0.1492\n",
      "alpha: 0.2064\n",
      "alpha: 0.1675\n",
      "alpha: 0.1491\n",
      "alpha: 0.1749\n",
      "alpha: 0.2213\n",
      "alpha: 0.1815\n",
      "alpha: 0.1687\n",
      "alpha: 0.1135\n",
      "alpha: 0.1601\n",
      "alpha: 0.1292\n",
      "alpha: 0.0905\n",
      "alpha: 0.1457\n",
      "alpha: 0.2544\n",
      "alpha: 0.1977\n",
      "alpha: 0.074\n",
      "alpha: 0.1732\n",
      "alpha: 0.2155\n",
      "alpha: 0.202\n",
      "alpha: 0.1635\n",
      "alpha: 0.1639\n",
      "alpha: 0.2179\n",
      "alpha: 0.1562\n",
      "alpha: 0.1552\n",
      "alpha: 0.1363\n",
      "alpha: 0.2235\n",
      "alpha: 0.1816\n",
      "alpha: 0.1941\n",
      "alpha: 0.1332\n",
      "alpha: 0.2219\n",
      "alpha: 0.1926\n",
      "alpha: 0.1057\n",
      "alpha: 0.2718\n",
      "alpha: 0.1706\n",
      "alpha: 0.1593\n",
      "alpha: 0.1551\n",
      "alpha: 0.1565\n",
      "alpha: 0.1537\n",
      "alpha: 0.1787\n",
      "alpha: 0.2348\n",
      "alpha: 0.1471\n",
      "alpha: 0.1896\n",
      "alpha: 0.1168\n",
      "alpha: 0.1345\n",
      "alpha: 0.1306\n",
      "alpha: 0.1296\n",
      "alpha: 0.152\n",
      "alpha: 0.2824\n",
      "alpha: 0.2182\n",
      "alpha: 0.2074\n",
      "alpha: 0.1388\n",
      "alpha: 0.2146\n",
      "alpha: 0.2187\n",
      "alpha: 0.2457\n",
      "alpha: 0.2077\n",
      "alpha: 0.2271\n",
      "alpha: 0.1443\n",
      "alpha: 0.1904\n",
      "alpha: 0.1278\n",
      "alpha: 0.1608\n",
      "alpha: 0.1934\n",
      "alpha: 0.1216\n",
      "alpha: 0.2301\n",
      "alpha: 0.142\n",
      "alpha: 0.1519\n",
      "alpha: 0.1509\n",
      "alpha: 0.1463\n",
      "alpha: 0.1825\n",
      "alpha: 0.1318\n",
      "alpha: 0.1719\n",
      "alpha: 0.1794\n",
      "alpha: 0.2411\n",
      "alpha: 0.1521\n",
      "alpha: 0.1477\n",
      "alpha: 0.1391\n",
      "alpha: 0.0677\n",
      "alpha: 0.2025\n",
      "alpha: 0.2049\n",
      "alpha: 0.0912\n",
      "alpha: 0.1964\n",
      "alpha: 0.1497\n",
      "alpha: 0.1205\n",
      "alpha: 0.165\n",
      "alpha: 0.1416\n",
      "alpha: 0.2188\n",
      "alpha: 0.2319\n",
      "alpha: 0.1134\n",
      "alpha: 0.1989\n",
      "alpha: 0.1818\n",
      "alpha: 0.1192\n",
      "alpha: 0.185\n",
      "alpha: 0.1012\n",
      "alpha: 0.2238\n",
      "alpha: 0.2049\n",
      "alpha: 0.1369\n",
      "alpha: 0.1913\n",
      "alpha: 0.211\n",
      "alpha: 0.1585\n",
      "alpha: 0.1695\n",
      "alpha: 0.1096\n",
      "alpha: 0.1474\n",
      "alpha: 0.1719\n",
      "alpha: 0.245\n",
      "alpha: 0.1649\n",
      "alpha: 0.2235\n",
      "alpha: 0.1066\n",
      "alpha: 0.1493\n",
      "alpha: 0.1048\n",
      "alpha: 0.1863\n",
      "alpha: 0.2062\n",
      "alpha: 0.1363\n",
      "alpha: 0.1741\n",
      "alpha: 0.091\n",
      "alpha: 0.1601\n",
      "alpha: 0.1806\n",
      "alpha: 0.1988\n",
      "alpha: 0.2084\n",
      "alpha: 0.1971\n",
      "alpha: 0.2568\n",
      "alpha: 0.1954\n",
      "alpha: 0.1488\n",
      "alpha: 0.2167\n",
      "alpha: 0.1196\n",
      "alpha: 0.1596\n",
      "alpha: 0.2136\n",
      "alpha: 0.1746\n",
      "alpha: 0.0874\n",
      "alpha: 0.124\n",
      "alpha: 0.1495\n",
      "alpha: 0.2237\n",
      "alpha: 0.1596\n",
      "alpha: 0.2417\n",
      "alpha: 0.129\n",
      "alpha: 0.106\n",
      "alpha: 0.1932\n",
      "alpha: 0.1738\n",
      "alpha: 0.2075\n",
      "alpha: 0.212\n",
      "alpha: 0.1642\n",
      "alpha: 0.1198\n",
      "alpha: 0.1244\n",
      "alpha: 0.1763\n",
      "alpha: 0.1606\n",
      "alpha: 0.2019\n",
      "alpha: 0.2002\n",
      "alpha: 0.192\n",
      "alpha: 0.1247\n",
      "alpha: 0.1406\n",
      "alpha: 0.1447\n",
      "alpha: 0.1558\n",
      "alpha: 0.172\n",
      "alpha: 0.1673\n",
      "alpha: 0.1442\n",
      "alpha: 0.131\n",
      "alpha: 0.0687\n",
      "alpha: 0.0895\n",
      "alpha: 0.1537\n",
      "alpha: 0.1432\n",
      "alpha: 0.1792\n",
      "alpha: 0.1622\n",
      "alpha: 0.1755\n",
      "alpha: 0.1783\n",
      "alpha: 0.1251\n",
      "alpha: 0.1512\n",
      "alpha: 0.1153\n",
      "alpha: 0.1341\n",
      "alpha: 0.1559\n",
      "alpha: 0.285\n",
      "alpha: 0.1473\n",
      "alpha: 0.1588\n",
      "alpha: 0.214\n",
      "alpha: 0.1122\n",
      "alpha: 0.1351\n",
      "alpha: 0.0922\n",
      "alpha: 0.1291\n",
      "alpha: 0.0982\n",
      "alpha: 0.1091\n",
      "alpha: 0.1488\n",
      "alpha: 0.1777\n",
      "alpha: 0.118\n",
      "alpha: 0.1595\n",
      "alpha: 0.1773\n",
      "alpha: 0.1744\n",
      "alpha: 0.1424\n",
      "alpha: 0.1451\n",
      "alpha: 0.2242\n",
      "alpha: 0.2369\n",
      "alpha: 0.1296\n",
      "alpha: 0.1987\n",
      "alpha: 0.1785\n",
      "alpha: 0.1221\n",
      "alpha: 0.2256\n",
      "alpha: 0.1273\n",
      "alpha: 0.1352\n",
      "alpha: 0.1242\n",
      "alpha: 0.1746\n",
      "alpha: 0.1876\n",
      "alpha: 0.2216\n",
      "alpha: 0.147\n",
      "alpha: 0.2102\n",
      "alpha: 0.1559\n",
      "alpha: 0.1804\n",
      "alpha: 0.1318\n",
      "alpha: 0.0929\n",
      "alpha: 0.1541\n",
      "alpha: 0.1517\n",
      "alpha: 0.2215\n",
      "alpha: 0.1281\n",
      "alpha: 0.1704\n",
      "alpha: 0.1542\n",
      "alpha: 0.2061\n",
      "alpha: 0.1095\n",
      "alpha: 0.2033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.1045\n",
      "alpha: 0.1507\n",
      "alpha: 0.1873\n",
      "alpha: 0.1603\n",
      "alpha: 0.1049\n",
      "alpha: 0.1518\n",
      "alpha: 0.2252\n",
      "alpha: 0.1644\n",
      "alpha: 0.1926\n",
      "alpha: 0.1514\n",
      "alpha: 0.1467\n",
      "alpha: 0.1851\n",
      "alpha: 0.1641\n",
      "alpha: 0.0986\n",
      "alpha: 0.2384\n",
      "alpha: 0.1861\n",
      "alpha: 0.0746\n",
      "alpha: 0.1423\n",
      "alpha: 0.1589\n",
      "alpha: 0.1774\n",
      "alpha: 0.1467\n",
      "alpha: 0.1544\n",
      "alpha: 0.1854\n",
      "alpha: 0.166\n",
      "alpha: 0.1034\n",
      "alpha: 0.0945\n",
      "alpha: 0.1159\n",
      "alpha: 0.1577\n",
      "alpha: 0.1797\n",
      "alpha: 0.1732\n",
      "alpha: 0.1732\n",
      "alpha: 0.1254\n",
      "alpha: 0.1124\n",
      "alpha: 0.2204\n",
      "alpha: 0.1962\n",
      "alpha: 0.1044\n",
      "alpha: 0.2084\n",
      "alpha: 0.1666\n",
      "alpha: 0.0952\n",
      "alpha: 0.1669\n",
      "alpha: 0.1727\n",
      "alpha: 0.2741\n",
      "alpha: 0.1089\n",
      "alpha: 0.186\n",
      "alpha: 0.1326\n",
      "alpha: 0.2695\n",
      "alpha: 0.0839\n",
      "alpha: 0.1949\n",
      "alpha: 0.0929\n",
      "alpha: 0.182\n",
      "alpha: 0.1822\n",
      "alpha: 0.1305\n",
      "alpha: 0.1549\n",
      "alpha: 0.1787\n",
      "alpha: 0.1197\n",
      "alpha: 0.1598\n",
      "alpha: 0.1358\n",
      "alpha: 0.2436\n",
      "alpha: 0.0737\n",
      "alpha: 0.1683\n",
      "alpha: 0.2239\n",
      "alpha: 0.2053\n",
      "alpha: 0.0974\n",
      "alpha: 0.1401\n",
      "alpha: 0.1667\n",
      "alpha: 0.1932\n",
      "alpha: 0.1312\n",
      "alpha: 0.1239\n",
      "alpha: 0.171\n",
      "alpha: 0.1836\n",
      "alpha: 0.1649\n",
      "alpha: 0.2115\n",
      "alpha: 0.1162\n",
      "alpha: 0.1603\n",
      "alpha: 0.2346\n",
      "alpha: 0.1826\n",
      "alpha: 0.2108\n",
      "alpha: 0.2321\n",
      "alpha: 0.112\n",
      "alpha: 0.1805\n",
      "alpha: 0.1605\n",
      "alpha: 0.1691\n",
      "alpha: 0.146\n",
      "alpha: 0.1348\n",
      "alpha: 0.1228\n",
      "alpha: 0.1261\n",
      "alpha: 0.2058\n",
      "alpha: 0.1869\n",
      "alpha: 0.2046\n",
      "alpha: 0.1202\n",
      "alpha: 0.1836\n",
      "alpha: 0.0858\n",
      "alpha: 0.1211\n",
      "alpha: 0.1291\n",
      "alpha: 0.1241\n",
      "alpha: 0.211\n",
      "alpha: 0.186\n",
      "alpha: 0.1257\n",
      "alpha: 0.1738\n",
      "alpha: 0.1844\n",
      "alpha: 0.1529\n",
      "alpha: 0.2672\n",
      "alpha: 0.1597\n",
      "alpha: 0.2177\n",
      "alpha: 0.1844\n",
      "alpha: 0.1495\n",
      "alpha: 0.1837\n",
      "alpha: 0.1857\n",
      "alpha: 0.1724\n",
      "alpha: 0.1732\n",
      "alpha: 0.1548\n",
      "alpha: 0.2054\n",
      "alpha: 0.1389\n",
      "alpha: 0.1121\n",
      "alpha: 0.1695\n",
      "alpha: 0.1581\n",
      "alpha: 0.1326\n",
      "alpha: 0.1404\n",
      "alpha: 0.1404\n",
      "alpha: 0.1694\n",
      "alpha: 0.1707\n",
      "alpha: 0.1237\n",
      "alpha: 0.1871\n",
      "alpha: 0.1248\n",
      "alpha: 0.193\n",
      "alpha: 0.1618\n",
      "alpha: 0.1605\n",
      "alpha: 0.1114\n",
      "alpha: 0.1521\n",
      "alpha: 0.1364\n",
      "alpha: 0.1823\n",
      "alpha: 0.1748\n",
      "alpha: 0.1486\n",
      "alpha: 0.1931\n",
      "alpha: 0.2335\n",
      "alpha: 0.1008\n",
      "alpha: 0.1903\n",
      "alpha: 0.1475\n",
      "alpha: 0.1769\n",
      "alpha: 0.1556\n",
      "alpha: 0.18\n",
      "alpha: 0.1433\n",
      "alpha: 0.1357\n",
      "alpha: 0.1318\n",
      "alpha: 0.1526\n",
      "alpha: 0.1487\n",
      "alpha: 0.2296\n",
      "alpha: 0.1301\n",
      "alpha: 0.1305\n",
      "alpha: 0.1712\n",
      "alpha: 0.124\n",
      "alpha: 0.1546\n",
      "alpha: 0.0779\n",
      "alpha: 0.1327\n",
      "alpha: 0.1161\n",
      "alpha: 0.1221\n",
      "alpha: 0.1905\n",
      "alpha: 0.1342\n",
      "alpha: 0.136\n",
      "alpha: 0.1995\n",
      "alpha: 0.169\n",
      "alpha: 0.23\n",
      "alpha: 0.1318\n",
      "alpha: 0.1555\n",
      "alpha: 0.2081\n",
      "alpha: 0.1727\n",
      "alpha: 0.175\n",
      "alpha: 0.1798\n",
      "alpha: 0.2975\n",
      "alpha: 0.1888\n",
      "alpha: 0.1778\n",
      "alpha: 0.1719\n",
      "alpha: 0.1611\n",
      "alpha: 0.1483\n",
      "alpha: 0.1765\n",
      "alpha: 0.1616\n",
      "alpha: 0.123\n",
      "alpha: 0.1751\n",
      "alpha: 0.1676\n",
      "alpha: 0.1175\n",
      "alpha: 0.1909\n",
      "alpha: 0.1952\n",
      "alpha: 0.1628\n",
      "alpha: 0.2046\n",
      "alpha: 0.1822\n",
      "alpha: 0.1753\n",
      "alpha: 0.1876\n",
      "alpha: 0.1121\n",
      "alpha: 0.1725\n",
      "alpha: 0.1618\n",
      "alpha: 0.0957\n",
      "alpha: 0.1357\n",
      "alpha: 0.152\n",
      "alpha: 0.1377\n",
      "alpha: 0.2511\n",
      "alpha: 0.2471\n",
      "alpha: 0.1906\n",
      "alpha: 0.1432\n",
      "alpha: 0.1791\n",
      "alpha: 0.1766\n",
      "alpha: 0.1753\n",
      "alpha: 0.2488\n",
      "alpha: 0.134\n",
      "alpha: 0.1367\n",
      "alpha: 0.1632\n",
      "alpha: 0.1595\n",
      "alpha: 0.1286\n",
      "alpha: 0.2282\n",
      "alpha: 0.1194\n",
      "alpha: 0.113\n",
      "alpha: 0.1037\n",
      "alpha: 0.1407\n",
      "alpha: 0.2401\n",
      "alpha: 0.1798\n",
      "alpha: 0.145\n",
      "alpha: 0.1663\n",
      "alpha: 0.1636\n",
      "alpha: 0.1346\n",
      "alpha: 0.1476\n",
      "alpha: 0.2225\n",
      "alpha: 0.2055\n",
      "alpha: 0.1594\n",
      "alpha: 0.1993\n",
      "alpha: 0.1225\n",
      "alpha: 0.138\n",
      "alpha: 0.1582\n",
      "alpha: 0.172\n",
      "alpha: 0.1464\n",
      "alpha: 0.1559\n",
      "alpha: 0.1461\n",
      "alpha: 0.2157\n",
      "alpha: 0.188\n",
      "alpha: 0.1416\n",
      "alpha: 0.1523\n",
      "alpha: 0.2302\n",
      "alpha: 0.124\n",
      "alpha: 0.1769\n",
      "alpha: 0.2187\n",
      "alpha: 0.2036\n",
      "alpha: 0.1039\n",
      "alpha: 0.1744\n",
      "alpha: 0.1291\n",
      "alpha: 0.2642\n",
      "alpha: 0.0835\n",
      "alpha: 0.1911\n",
      "alpha: 0.1203\n",
      "alpha: 0.1439\n",
      "alpha: 0.1625\n",
      "alpha: 0.1872\n",
      "alpha: 0.1582\n",
      "alpha: 0.1646\n",
      "alpha: 0.1744\n",
      "alpha: 0.1561\n",
      "alpha: 0.147\n",
      "alpha: 0.2587\n",
      "alpha: 0.2072\n",
      "alpha: 0.1428\n",
      "alpha: 0.1108\n",
      "alpha: 0.17\n",
      "alpha: 0.1755\n",
      "alpha: 0.215\n",
      "alpha: 0.25\n",
      "alpha: 0.186\n",
      "alpha: 0.1834\n",
      "alpha: 0.133\n",
      "alpha: 0.1364\n",
      "alpha: 0.2152\n",
      "alpha: 0.2111\n",
      "alpha: 0.232\n",
      "alpha: 0.1472\n",
      "alpha: 0.1365\n",
      "alpha: 0.2109\n",
      "alpha: 0.188\n",
      "alpha: 0.1964\n",
      "alpha: 0.2012\n",
      "alpha: 0.2075\n",
      "alpha: 0.2056\n",
      "alpha: 0.1602\n",
      "alpha: 0.1328\n",
      "alpha: 0.1901\n",
      "alpha: 0.2007\n",
      "alpha: 0.1369\n",
      "alpha: 0.078\n",
      "alpha: 0.1275\n",
      "alpha: 0.1463\n",
      "alpha: 0.1674\n",
      "alpha: 0.2806\n",
      "alpha: 0.1768\n",
      "alpha: 0.1586\n",
      "alpha: 0.1465\n",
      "alpha: 0.1246\n",
      "alpha: 0.2365\n",
      "alpha: 0.1427\n",
      "alpha: 0.1426\n",
      "alpha: 0.1193\n",
      "alpha: 0.283\n",
      "alpha: 0.1606\n",
      "alpha: 0.2017\n",
      "alpha: 0.1767\n",
      "alpha: 0.1571\n",
      "alpha: 0.1304\n",
      "alpha: 0.1564\n",
      "alpha: 0.1527\n",
      "alpha: 0.1802\n",
      "alpha: 0.1899\n",
      "alpha: 0.1545\n",
      "alpha: 0.2058\n",
      "alpha: 0.1014\n",
      "alpha: 0.1262\n",
      "alpha: 0.1398\n",
      "alpha: 0.1703\n",
      "alpha: 0.223\n",
      "alpha: 0.1604\n",
      "alpha: 0.1126\n",
      "alpha: 0.1159\n",
      "alpha: 0.1319\n",
      "alpha: 0.1268\n",
      "alpha: 0.1866\n",
      "alpha: 0.1954\n",
      "alpha: 0.1461\n",
      "alpha: 0.2271\n",
      "alpha: 0.2378\n",
      "alpha: 0.1456\n",
      "alpha: 0.1107\n",
      "alpha: 0.1275\n",
      "alpha: 0.1815\n",
      "alpha: 0.14\n",
      "alpha: 0.2368\n",
      "alpha: 0.2194\n",
      "alpha: 0.148\n",
      "alpha: 0.1181\n",
      "alpha: 0.2083\n",
      "alpha: 0.1318\n",
      "alpha: 0.1198\n",
      "alpha: 0.241\n",
      "alpha: 0.0913\n",
      "alpha: 0.1516\n",
      "alpha: 0.1745\n",
      "alpha: 0.1292\n",
      "alpha: 0.1153\n",
      "alpha: 0.2471\n",
      "alpha: 0.1499\n",
      "alpha: 0.12\n",
      "alpha: 0.1131\n",
      "alpha: 0.1688\n",
      "alpha: 0.1167\n",
      "alpha: 0.2126\n",
      "alpha: 0.161\n",
      "alpha: 0.0839\n",
      "alpha: 0.1755\n",
      "alpha: 0.1652\n",
      "alpha: 0.232\n",
      "alpha: 0.0973\n",
      "alpha: 0.1833\n",
      "alpha: 0.1487\n",
      "alpha: 0.1687\n",
      "alpha: 0.1598\n",
      "alpha: 0.1314\n",
      "alpha: 0.1674\n",
      "alpha: 0.1645\n",
      "alpha: 0.1226\n",
      "alpha: 0.1461\n",
      "alpha: 0.2193\n",
      "alpha: 0.1532\n",
      "alpha: 0.0991\n",
      "alpha: 0.2037\n",
      "alpha: 0.1318\n",
      "alpha: 0.1137\n",
      "alpha: 0.1948\n",
      "alpha: 0.1554\n",
      "alpha: 0.1343\n",
      "alpha: 0.1898\n",
      "alpha: 0.1975\n",
      "alpha: 0.1605\n",
      "alpha: 0.1611\n",
      "alpha: 0.2121\n",
      "alpha: 0.177\n",
      "alpha: 0.1495\n",
      "alpha: 0.1552\n",
      "alpha: 0.1048\n",
      "alpha: 0.1622\n",
      "alpha: 0.1329\n",
      "alpha: 0.1423\n",
      "alpha: 0.2226\n",
      "alpha: 0.125\n",
      "alpha: 0.1827\n",
      "alpha: 0.181\n",
      "alpha: 0.2038\n",
      "alpha: 0.1113\n",
      "alpha: 0.1146\n",
      "alpha: 0.1683\n",
      "alpha: 0.2522\n",
      "alpha: 0.192\n",
      "alpha: 0.2199\n",
      "alpha: 0.1241\n",
      "alpha: 0.1801\n",
      "alpha: 0.1533\n",
      "alpha: 0.0961\n",
      "alpha: 0.191\n",
      "alpha: 0.2164\n",
      "alpha: 0.2003\n",
      "alpha: 0.1094\n",
      "alpha: 0.1592\n",
      "alpha: 0.1597\n",
      "alpha: 0.1525\n",
      "alpha: 0.213\n",
      "alpha: 0.198\n",
      "alpha: 0.1738\n",
      "alpha: 0.2513\n",
      "alpha: 0.0994\n",
      "alpha: 0.1946\n",
      "alpha: 0.1735\n",
      "alpha: 0.2429\n",
      "alpha: 0.1392\n",
      "alpha: 0.1509\n",
      "alpha: 0.1808\n",
      "alpha: 0.1157\n",
      "alpha: 0.2742\n",
      "alpha: 0.0975\n",
      "alpha: 0.2312\n",
      "alpha: 0.218\n",
      "alpha: 0.1522\n",
      "alpha: 0.1593\n",
      "alpha: 0.1384\n",
      "alpha: 0.1597\n",
      "alpha: 0.1843\n",
      "alpha: 0.1373\n",
      "alpha: 0.0935\n",
      "alpha: 0.2622\n",
      "alpha: 0.2402\n",
      "alpha: 0.1783\n",
      "alpha: 0.1545\n",
      "alpha: 0.1794\n",
      "alpha: 0.1835\n",
      "alpha: 0.2063\n",
      "alpha: 0.1154\n",
      "alpha: 0.1535\n",
      "alpha: 0.2778\n",
      "alpha: 0.1095\n",
      "alpha: 0.1016\n",
      "alpha: 0.1586\n",
      "alpha: 0.1372\n",
      "alpha: 0.1829\n",
      "alpha: 0.1436\n",
      "alpha: 0.2473\n",
      "alpha: 0.2021\n",
      "alpha: 0.1869\n",
      "alpha: 0.2125\n",
      "alpha: 0.2148\n",
      "alpha: 0.1234\n",
      "alpha: 0.1365\n",
      "alpha: 0.1455\n",
      "alpha: 0.1516\n",
      "alpha: 0.2026\n",
      "alpha: 0.1349\n",
      "alpha: 0.1458\n",
      "alpha: 0.1253\n",
      "alpha: 0.2518\n",
      "alpha: 0.1358\n",
      "alpha: 0.1452\n",
      "alpha: 0.2288\n",
      "alpha: 0.2001\n",
      "alpha: 0.1443\n",
      "alpha: 0.1813\n",
      "alpha: 0.1249\n",
      "alpha: 0.1481\n",
      "alpha: 0.132\n",
      "alpha: 0.1618\n",
      "alpha: 0.0945\n",
      "alpha: 0.111\n",
      "alpha: 0.1844\n",
      "alpha: 0.2464\n",
      "alpha: 0.2313\n",
      "alpha: 0.1528\n",
      "alpha: 0.2743\n",
      "alpha: 0.125\n",
      "alpha: 0.2912\n",
      "alpha: 0.1515\n",
      "alpha: 0.188\n",
      "alpha: 0.1333\n",
      "alpha: 0.1984\n",
      "alpha: 0.1215\n",
      "alpha: 0.1795\n",
      "alpha: 0.1529\n",
      "alpha: 0.174\n",
      "alpha: 0.1478\n",
      "alpha: 0.1394\n",
      "alpha: 0.1049\n",
      "alpha: 0.128\n",
      "alpha: 0.1195\n",
      "alpha: 0.1861\n",
      "alpha: 0.1312\n",
      "alpha: 0.174\n",
      "alpha: 0.1777\n",
      "alpha: 0.1642\n",
      "alpha: 0.1531\n",
      "alpha: 0.0868\n",
      "alpha: 0.1651\n",
      "alpha: 0.1816\n",
      "alpha: 0.1632\n",
      "alpha: 0.1793\n",
      "alpha: 0.171\n",
      "alpha: 0.251\n",
      "alpha: 0.1509\n",
      "alpha: 0.1584\n",
      "alpha: 0.1563\n",
      "alpha: 0.1631\n",
      "alpha: 0.1243\n",
      "alpha: 0.0604\n",
      "alpha: 0.191\n",
      "alpha: 0.1817\n",
      "alpha: 0.2347\n",
      "alpha: 0.1743\n",
      "alpha: 0.2086\n",
      "alpha: 0.0968\n",
      "alpha: 0.1653\n",
      "alpha: 0.2336\n",
      "alpha: 0.0956\n",
      "alpha: 0.2039\n",
      "alpha: 0.1696\n",
      "alpha: 0.1854\n",
      "alpha: 0.1452\n",
      "alpha: 0.1216\n",
      "alpha: 0.177\n",
      "alpha: 0.1619\n",
      "alpha: 0.116\n",
      "alpha: 0.1483\n",
      "alpha: 0.183\n",
      "alpha: 0.159\n",
      "alpha: 0.1365\n",
      "alpha: 0.1786\n",
      "alpha: 0.2258\n",
      "alpha: 0.1564\n",
      "alpha: 0.1138\n",
      "alpha: 0.1445\n",
      "alpha: 0.1959\n",
      "alpha: 0.108\n",
      "alpha: 0.1547\n",
      "alpha: 0.1689\n",
      "alpha: 0.2796\n",
      "alpha: 0.1519\n",
      "alpha: 0.1425\n",
      "alpha: 0.1673\n",
      "alpha: 0.1899\n",
      "alpha: 0.2093\n",
      "alpha: 0.1225\n",
      "alpha: 0.1365\n",
      "alpha: 0.1053\n",
      "alpha: 0.1496\n",
      "alpha: 0.146\n",
      "alpha: 0.0873\n",
      "alpha: 0.2074\n",
      "alpha: 0.1827\n",
      "alpha: 0.1381\n",
      "alpha: 0.1524\n",
      "alpha: 0.1603\n",
      "alpha: 0.1159\n",
      "alpha: 0.1522\n",
      "alpha: 0.1209\n",
      "alpha: 0.1303\n",
      "alpha: 0.2012\n",
      "alpha: 0.1044\n",
      "alpha: 0.1555\n",
      "alpha: 0.1386\n",
      "alpha: 0.2093\n",
      "alpha: 0.1051\n",
      "alpha: 0.132\n",
      "alpha: 0.1392\n",
      "alpha: 0.2125\n",
      "alpha: 0.1449\n",
      "alpha: 0.1908\n",
      "alpha: 0.1916\n",
      "alpha: 0.266\n",
      "alpha: 0.1572\n",
      "alpha: 0.2155\n",
      "alpha: 0.1194\n",
      "alpha: 0.1342\n",
      "alpha: 0.1779\n",
      "alpha: 0.1743\n",
      "alpha: 0.2072\n",
      "alpha: 0.1653\n",
      "alpha: 0.2098\n",
      "alpha: 0.1092\n",
      "alpha: 0.1864\n",
      "alpha: 0.1521\n",
      "alpha: 0.1306\n",
      "alpha: 0.1466\n",
      "alpha: 0.1271\n",
      "alpha: 0.1464\n",
      "alpha: 0.1224\n",
      "alpha: 0.1214\n",
      "alpha: 0.1361\n",
      "alpha: 0.1934\n",
      "alpha: 0.2025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.1004\n",
      "alpha: 0.1042\n",
      "alpha: 0.1771\n",
      "alpha: 0.1647\n",
      "alpha: 0.2231\n",
      "alpha: 0.2255\n",
      "alpha: 0.1831\n",
      "alpha: 0.1519\n",
      "alpha: 0.1751\n",
      "alpha: 0.1659\n",
      "alpha: 0.2071\n",
      "alpha: 0.1775\n",
      "alpha: 0.1125\n",
      "alpha: 0.2219\n",
      "alpha: 0.182\n",
      "alpha: 0.166\n",
      "alpha: 0.2149\n",
      "alpha: 0.1612\n",
      "alpha: 0.1716\n",
      "alpha: 0.0978\n",
      "alpha: 0.1751\n",
      "alpha: 0.1413\n",
      "alpha: 0.2771\n",
      "alpha: 0.2328\n",
      "alpha: 0.1995\n",
      "alpha: 0.2614\n",
      "alpha: 0.1476\n",
      "alpha: 0.0947\n",
      "alpha: 0.1223\n",
      "alpha: 0.2225\n",
      "alpha: 0.1185\n",
      "alpha: 0.1454\n",
      "alpha: 0.1851\n",
      "alpha: 0.1769\n",
      "alpha: 0.1813\n",
      "alpha: 0.2281\n",
      "alpha: 0.1849\n",
      "alpha: 0.1497\n",
      "alpha: 0.2026\n",
      "alpha: 0.1107\n",
      "alpha: 0.1442\n",
      "alpha: 0.1038\n",
      "alpha: 0.191\n",
      "alpha: 0.1626\n",
      "alpha: 0.0949\n",
      "alpha: 0.1185\n",
      "alpha: 0.127\n",
      "alpha: 0.2043\n",
      "alpha: 0.1304\n",
      "alpha: 0.1584\n",
      "alpha: 0.0999\n",
      "alpha: 0.164\n",
      "alpha: 0.1525\n",
      "alpha: 0.2113\n",
      "alpha: 0.1288\n",
      "alpha: 0.1224\n",
      "alpha: 0.1564\n",
      "alpha: 0.1343\n",
      "alpha: 0.1738\n",
      "alpha: 0.1353\n",
      "alpha: 0.1453\n",
      "alpha: 0.1778\n",
      "alpha: 0.1698\n",
      "alpha: 0.1869\n",
      "alpha: 0.1702\n",
      "alpha: 0.1285\n",
      "alpha: 0.1428\n",
      "alpha: 0.1235\n",
      "alpha: 0.1431\n",
      "alpha: 0.0773\n",
      "alpha: 0.1111\n",
      "alpha: 0.2101\n",
      "alpha: 0.1088\n",
      "alpha: 0.2206\n",
      "alpha: 0.1926\n",
      "alpha: 0.1884\n",
      "alpha: 0.1474\n",
      "alpha: 0.1208\n",
      "alpha: 0.176\n",
      "alpha: 0.1751\n",
      "alpha: 0.1996\n",
      "alpha: 0.1154\n",
      "alpha: 0.1556\n",
      "alpha: 0.0821\n",
      "alpha: 0.099\n",
      "alpha: 0.1333\n",
      "alpha: 0.1294\n",
      "alpha: 0.1193\n",
      "alpha: 0.1522\n",
      "alpha: 0.1188\n",
      "alpha: 0.2014\n",
      "alpha: 0.1414\n",
      "alpha: 0.2008\n",
      "alpha: 0.1023\n",
      "alpha: 0.1216\n",
      "alpha: 0.1391\n",
      "alpha: 0.1184\n",
      "alpha: 0.1054\n",
      "alpha: 0.1679\n",
      "alpha: 0.0895\n",
      "alpha: 0.2614\n",
      "alpha: 0.1517\n",
      "alpha: 0.1302\n",
      "alpha: 0.1859\n",
      "alpha: 0.1975\n",
      "alpha: 0.1184\n",
      "alpha: 0.1968\n",
      "alpha: 0.1938\n",
      "alpha: 0.1249\n",
      "alpha: 0.2904\n",
      "alpha: 0.1001\n",
      "alpha: 0.1512\n",
      "alpha: 0.1223\n",
      "alpha: 0.1842\n",
      "alpha: 0.1561\n",
      "alpha: 0.2316\n",
      "alpha: 0.1735\n",
      "alpha: 0.1312\n",
      "alpha: 0.2619\n",
      "alpha: 0.1828\n",
      "alpha: 0.156\n",
      "alpha: 0.2023\n",
      "alpha: 0.1793\n",
      "alpha: 0.1831\n",
      "alpha: 0.1697\n",
      "alpha: 0.1608\n",
      "alpha: 0.1231\n",
      "alpha: 0.2529\n",
      "alpha: 0.1437\n",
      "alpha: 0.145\n",
      "alpha: 0.1507\n",
      "alpha: 0.2256\n",
      "alpha: 0.1787\n",
      "alpha: 0.1105\n",
      "alpha: 0.114\n",
      "alpha: 0.1742\n",
      "alpha: 0.1391\n",
      "alpha: 0.108\n",
      "alpha: 0.1608\n",
      "alpha: 0.1213\n",
      "alpha: 0.1756\n",
      "alpha: 0.1488\n",
      "alpha: 0.1528\n",
      "alpha: 0.1359\n",
      "alpha: 0.2121\n",
      "alpha: 0.2081\n",
      "alpha: 0.1711\n",
      "alpha: 0.1235\n",
      "alpha: 0.1262\n",
      "alpha: 0.1768\n",
      "alpha: 0.1855\n",
      "alpha: 0.0848\n",
      "alpha: 0.1359\n",
      "alpha: 0.1615\n",
      "alpha: 0.1514\n",
      "alpha: 0.1262\n",
      "alpha: 0.0679\n",
      "alpha: 0.1312\n",
      "alpha: 0.1801\n",
      "alpha: 0.2165\n",
      "alpha: 0.1814\n",
      "alpha: 0.1574\n",
      "alpha: 0.2546\n",
      "alpha: 0.1641\n",
      "alpha: 0.1679\n",
      "alpha: 0.1604\n",
      "alpha: 0.1893\n",
      "alpha: 0.177\n",
      "alpha: 0.2137\n",
      "alpha: 0.1721\n",
      "alpha: 0.2693\n",
      "alpha: 0.1479\n",
      "alpha: 0.1192\n",
      "alpha: 0.1796\n",
      "alpha: 0.1675\n",
      "alpha: 0.1466\n",
      "alpha: 0.194\n",
      "alpha: 0.139\n",
      "alpha: 0.1059\n",
      "alpha: 0.116\n",
      "alpha: 0.2204\n",
      "alpha: 0.1783\n",
      "alpha: 0.1331\n",
      "alpha: 0.166\n",
      "alpha: 0.1518\n",
      "alpha: 0.2346\n",
      "alpha: 0.1839\n",
      "alpha: 0.1297\n",
      "alpha: 0.1723\n",
      "alpha: 0.1805\n",
      "alpha: 0.2025\n",
      "alpha: 0.1098\n",
      "alpha: 0.2618\n",
      "alpha: 0.1775\n",
      "alpha: 0.167\n",
      "alpha: 0.129\n",
      "alpha: 0.2011\n",
      "alpha: 0.1919\n",
      "alpha: 0.162\n",
      "alpha: 0.1905\n",
      "alpha: 0.1317\n",
      "alpha: 0.1861\n",
      "alpha: 0.2881\n",
      "alpha: 0.1585\n",
      "alpha: 0.1863\n",
      "alpha: 0.2053\n",
      "alpha: 0.1421\n",
      "alpha: 0.1376\n",
      "alpha: 0.1028\n",
      "alpha: 0.2098\n",
      "alpha: 0.192\n",
      "alpha: 0.0828\n",
      "alpha: 0.2555\n",
      "alpha: 0.1814\n",
      "alpha: 0.1696\n",
      "alpha: 0.1913\n",
      "alpha: 0.1102\n",
      "alpha: 0.1828\n",
      "alpha: 0.1785\n",
      "alpha: 0.1837\n",
      "alpha: 0.1266\n",
      "alpha: 0.1014\n",
      "alpha: 0.2103\n",
      "alpha: 0.196\n",
      "alpha: 0.1825\n",
      "alpha: 0.1975\n",
      "alpha: 0.2394\n",
      "alpha: 0.1109\n",
      "alpha: 0.1386\n",
      "alpha: 0.1403\n",
      "alpha: 0.1746\n",
      "alpha: 0.1565\n",
      "alpha: 0.1095\n",
      "alpha: 0.1311\n",
      "alpha: 0.1631\n",
      "alpha: 0.1941\n",
      "alpha: 0.1493\n",
      "alpha: 0.1804\n",
      "alpha: 0.1229\n",
      "alpha: 0.1583\n",
      "alpha: 0.2009\n",
      "alpha: 0.1966\n",
      "alpha: 0.2189\n",
      "alpha: 0.1904\n",
      "alpha: 0.2002\n",
      "alpha: 0.2101\n",
      "alpha: 0.2284\n",
      "alpha: 0.1385\n",
      "alpha: 0.1326\n",
      "alpha: 0.192\n",
      "alpha: 0.1104\n",
      "alpha: 0.2189\n",
      "alpha: 0.1909\n",
      "alpha: 0.1635\n",
      "alpha: 0.117\n",
      "alpha: 0.1939\n",
      "alpha: 0.2514\n",
      "alpha: 0.1682\n",
      "alpha: 0.2207\n",
      "alpha: 0.1299\n",
      "alpha: 0.1141\n",
      "alpha: 0.1689\n",
      "alpha: 0.1185\n",
      "alpha: 0.2183\n",
      "alpha: 0.14\n",
      "alpha: 0.2678\n",
      "alpha: 0.2452\n",
      "alpha: 0.2018\n",
      "alpha: 0.0931\n",
      "alpha: 0.1829\n",
      "alpha: 0.179\n",
      "alpha: 0.1498\n",
      "alpha: 0.1919\n",
      "alpha: 0.1823\n",
      "alpha: 0.1533\n",
      "alpha: 0.264\n",
      "alpha: 0.1192\n",
      "alpha: 0.1712\n",
      "alpha: 0.1943\n",
      "alpha: 0.1186\n",
      "alpha: 0.1566\n",
      "alpha: 0.0744\n",
      "alpha: 0.2005\n",
      "alpha: 0.2069\n",
      "alpha: 0.1851\n",
      "alpha: 0.1968\n",
      "alpha: 0.2547\n",
      "alpha: 0.107\n",
      "alpha: 0.2347\n",
      "alpha: 0.1632\n",
      "alpha: 0.1783\n",
      "alpha: 0.1801\n",
      "alpha: 0.1743\n",
      "alpha: 0.1392\n",
      "alpha: 0.2089\n",
      "alpha: 0.2042\n",
      "alpha: 0.2863\n",
      "alpha: 0.1436\n",
      "alpha: 0.1233\n",
      "alpha: 0.2195\n",
      "alpha: 0.1054\n",
      "alpha: 0.1761\n",
      "alpha: 0.2194\n",
      "alpha: 0.1766\n",
      "alpha: 0.1546\n",
      "alpha: 0.2036\n",
      "alpha: 0.2745\n",
      "alpha: 0.1544\n",
      "alpha: 0.1668\n",
      "alpha: 0.1466\n",
      "alpha: 0.1862\n",
      "alpha: 0.192\n",
      "alpha: 0.2595\n",
      "alpha: 0.1674\n",
      "alpha: 0.1392\n",
      "alpha: 0.1793\n",
      "alpha: 0.1891\n",
      "alpha: 0.1653\n",
      "alpha: 0.2352\n",
      "alpha: 0.1858\n",
      "alpha: 0.1288\n",
      "alpha: 0.1477\n",
      "alpha: 0.2017\n",
      "alpha: 0.1821\n",
      "alpha: 0.1727\n",
      "alpha: 0.2251\n",
      "alpha: 0.1184\n",
      "alpha: 0.1572\n",
      "alpha: 0.207\n",
      "alpha: 0.2117\n",
      "alpha: 0.1576\n",
      "alpha: 0.1861\n",
      "alpha: 0.1298\n",
      "alpha: 0.2364\n",
      "alpha: 0.1167\n",
      "alpha: 0.1685\n",
      "alpha: 0.0982\n",
      "alpha: 0.2648\n",
      "alpha: 0.1842\n",
      "alpha: 0.1011\n",
      "alpha: 0.092\n",
      "alpha: 0.1624\n",
      "alpha: 0.1593\n",
      "alpha: 0.1484\n",
      "alpha: 0.1264\n",
      "alpha: 0.2592\n",
      "alpha: 0.1011\n",
      "alpha: 0.1162\n",
      "alpha: 0.1303\n",
      "alpha: 0.1656\n",
      "alpha: 0.1983\n",
      "alpha: 0.1888\n",
      "alpha: 0.1073\n",
      "alpha: 0.186\n",
      "alpha: 0.1817\n",
      "alpha: 0.1736\n",
      "alpha: 0.1706\n",
      "alpha: 0.209\n",
      "alpha: 0.1476\n",
      "alpha: 0.2361\n",
      "alpha: 0.1339\n",
      "alpha: 0.2049\n",
      "alpha: 0.1577\n",
      "alpha: 0.1309\n",
      "alpha: 0.1355\n",
      "alpha: 0.3258\n",
      "alpha: 0.1493\n",
      "alpha: 0.159\n",
      "alpha: 0.2853\n",
      "alpha: 0.1367\n",
      "alpha: 0.1739\n",
      "alpha: 0.0974\n",
      "alpha: 0.2184\n",
      "alpha: 0.1452\n",
      "alpha: 0.0558\n",
      "alpha: 0.1017\n",
      "alpha: 0.1892\n",
      "alpha: 0.2047\n",
      "alpha: 0.1596\n",
      "alpha: 0.3029\n",
      "alpha: 0.177\n",
      "alpha: 0.1177\n",
      "alpha: 0.156\n",
      "alpha: 0.1073\n",
      "alpha: 0.2077\n",
      "alpha: 0.2142\n",
      "alpha: 0.1487\n",
      "alpha: 0.0904\n",
      "alpha: 0.1688\n",
      "alpha: 0.1839\n",
      "alpha: 0.2644\n",
      "alpha: 0.187\n",
      "alpha: 0.1288\n",
      "alpha: 0.1514\n",
      "alpha: 0.1587\n",
      "alpha: 0.2014\n",
      "alpha: 0.1487\n",
      "alpha: 0.0952\n",
      "alpha: 0.1392\n",
      "alpha: 0.1877\n",
      "alpha: 0.2339\n",
      "alpha: 0.1694\n",
      "alpha: 0.0918\n",
      "alpha: 0.1143\n",
      "alpha: 0.1536\n",
      "alpha: 0.151\n",
      "alpha: 0.1477\n",
      "alpha: 0.1685\n",
      "alpha: 0.1493\n",
      "alpha: 0.222\n",
      "alpha: 0.2265\n",
      "alpha: 0.2713\n",
      "alpha: 0.158\n",
      "alpha: 0.0769\n",
      "alpha: 0.2791\n",
      "alpha: 0.1981\n",
      "alpha: 0.1622\n",
      "alpha: 0.1678\n",
      "alpha: 0.1176\n",
      "alpha: 0.1852\n",
      "alpha: 0.1569\n",
      "alpha: 0.1564\n",
      "alpha: 0.1515\n",
      "alpha: 0.1773\n",
      "alpha: 0.1524\n",
      "alpha: 0.13\n",
      "alpha: 0.1148\n",
      "alpha: 0.1489\n",
      "alpha: 0.1687\n",
      "alpha: 0.1581\n",
      "alpha: 0.2088\n",
      "alpha: 0.2083\n",
      "alpha: 0.1739\n",
      "alpha: 0.1236\n",
      "alpha: 0.1518\n",
      "alpha: 0.1487\n",
      "alpha: 0.187\n",
      "alpha: 0.1619\n",
      "alpha: 0.2373\n",
      "alpha: 0.1782\n",
      "alpha: 0.2279\n",
      "alpha: 0.1584\n",
      "alpha: 0.2008\n",
      "alpha: 0.211\n",
      "alpha: 0.1201\n",
      "alpha: 0.1379\n",
      "alpha: 0.2185\n",
      "alpha: 0.1106\n",
      "alpha: 0.1868\n",
      "alpha: 0.2075\n",
      "alpha: 0.2782\n",
      "alpha: 0.2093\n",
      "alpha: 0.2434\n",
      "alpha: 0.1634\n",
      "alpha: 0.3126\n",
      "alpha: 0.2289\n",
      "alpha: 0.139\n",
      "alpha: 0.1516\n",
      "alpha: 0.1393\n",
      "alpha: 0.1545\n",
      "alpha: 0.2182\n",
      "alpha: 0.2599\n",
      "alpha: 0.2062\n",
      "alpha: 0.1132\n",
      "alpha: 0.1402\n",
      "alpha: 0.1227\n",
      "alpha: 0.1526\n",
      "alpha: 0.1434\n",
      "alpha: 0.1437\n",
      "alpha: 0.1215\n",
      "alpha: 0.1147\n",
      "alpha: 0.1233\n",
      "alpha: 0.1693\n",
      "alpha: 0.1251\n",
      "alpha: 0.1292\n",
      "alpha: 0.1744\n",
      "alpha: 0.1249\n",
      "alpha: 0.1426\n",
      "alpha: 0.0942\n",
      "alpha: 0.1663\n",
      "alpha: 0.1592\n",
      "alpha: 0.2191\n",
      "alpha: 0.1816\n",
      "alpha: 0.1055\n",
      "alpha: 0.2138\n",
      "alpha: 0.1763\n",
      "alpha: 0.1564\n",
      "alpha: 0.1876\n",
      "alpha: 0.1717\n",
      "alpha: 0.1716\n",
      "alpha: 0.1716\n",
      "alpha: 0.1391\n",
      "alpha: 0.1111\n",
      "alpha: 0.2123\n",
      "alpha: 0.1596\n",
      "alpha: 0.1429\n",
      "alpha: 0.1323\n",
      "alpha: 0.1138\n",
      "alpha: 0.1574\n",
      "alpha: 0.1866\n",
      "alpha: 0.2222\n",
      "alpha: 0.1156\n",
      "alpha: 0.1573\n",
      "alpha: 0.1634\n",
      "alpha: 0.1981\n",
      "alpha: 0.1291\n",
      "alpha: 0.1194\n",
      "alpha: 0.1573\n",
      "alpha: 0.1404\n",
      "alpha: 0.1819\n",
      "alpha: 0.1343\n",
      "alpha: 0.2197\n",
      "alpha: 0.229\n",
      "alpha: 0.2058\n",
      "alpha: 0.1416\n",
      "alpha: 0.1541\n",
      "alpha: 0.1668\n",
      "alpha: 0.1829\n",
      "alpha: 0.1961\n",
      "alpha: 0.2386\n",
      "alpha: 0.1374\n",
      "alpha: 0.1621\n",
      "alpha: 0.1547\n",
      "alpha: 0.1784\n",
      "alpha: 0.1496\n",
      "alpha: 0.1696\n",
      "alpha: 0.2557\n",
      "alpha: 0.1473\n",
      "alpha: 0.1902\n",
      "alpha: 0.1096\n",
      "alpha: 0.1037\n",
      "alpha: 0.1546\n",
      "alpha: 0.1545\n",
      "alpha: 0.1608\n",
      "alpha: 0.1266\n",
      "alpha: 0.1541\n",
      "alpha: 0.1281\n",
      "alpha: 0.1083\n",
      "alpha: 0.1438\n",
      "alpha: 0.2018\n",
      "alpha: 0.1655\n",
      "alpha: 0.167\n",
      "alpha: 0.2635\n",
      "alpha: 0.1723\n",
      "alpha: 0.1703\n",
      "alpha: 0.1428\n",
      "alpha: 0.0828\n",
      "alpha: 0.1201\n",
      "alpha: 0.1717\n",
      "alpha: 0.1727\n",
      "alpha: 0.1635\n",
      "alpha: 0.1557\n",
      "alpha: 0.1649\n",
      "alpha: 0.1864\n",
      "alpha: 0.1506\n",
      "alpha: 0.1787\n",
      "alpha: 0.1906\n",
      "alpha: 0.1785\n",
      "alpha: 0.1508\n",
      "alpha: 0.1598\n",
      "alpha: 0.1691\n",
      "alpha: 0.192\n",
      "alpha: 0.2483\n",
      "alpha: 0.1346\n",
      "alpha: 0.2072\n",
      "alpha: 0.119\n",
      "alpha: 0.2043\n",
      "alpha: 0.2661\n",
      "alpha: 0.1571\n",
      "alpha: 0.135\n",
      "alpha: 0.1627\n",
      "alpha: 0.2232\n",
      "alpha: 0.1864\n",
      "alpha: 0.2155\n",
      "alpha: 0.1497\n",
      "alpha: 0.2035\n",
      "alpha: 0.0875\n",
      "alpha: 0.1314\n",
      "alpha: 0.0591\n",
      "alpha: 0.237\n",
      "alpha: 0.1147\n",
      "alpha: 0.1685\n",
      "alpha: 0.1655\n",
      "alpha: 0.1926\n",
      "alpha: 0.1812\n",
      "alpha: 0.1225\n",
      "alpha: 0.2556\n",
      "alpha: 0.1297\n",
      "alpha: 0.1865\n",
      "alpha: 0.1558\n",
      "alpha: 0.1709\n",
      "alpha: 0.2077\n",
      "alpha: 0.2044\n",
      "alpha: 0.1395\n"
     ]
    }
   ],
   "source": [
    "U = Uniform(args.lower, args.upper) # distribution to generate ground-truth parameters\n",
    "U_ = Uniform(-5, 5) # distribution to generate samples\n",
    "\n",
    "# perform trials number of experiments\n",
    "for i in range(args.trials):\n",
    "    \n",
    "    # continue to generate synthetic data until survival probability of more than 40%\n",
    "    alpha = None\n",
    "    while alpha is None or alpha < .5:\n",
    "        # generate ground-truth from uniform distribution\n",
    "        ground_truth = nn.Linear(in_features=args.IN_FEATURES, out_features=args.K, bias=args.bias)\n",
    "        ground_truth.weight = nn.Parameter(U.sample(ch.Size([args.K, args.IN_FEATURES])))\n",
    "        if ground_truth.bias is not None: \n",
    "            ground_truth.bias = nn.Parameter(U.sample(ch.Size([args.K,])))\n",
    "        # independent variable \n",
    "        X = U_.sample(ch.Size([args.samples, args.IN_FEATURES]))\n",
    "        # determine base model logits \n",
    "        z = ground_truth(X)\n",
    "        # apply softmax to unnormalized likelihoods\n",
    "        y = ch.argmax(ch.nn.Softmax(dim=1)(z), dim=1)\n",
    "\n",
    "        # TRUNCATE\n",
    "        trunc = phi(z)\n",
    "        indices = ch.all(trunc.bool(), dim=1).float().nonzero(as_tuple=False).flatten()\n",
    "        y_trunc = y[indices]\n",
    "        x_trunc = X[indices]\n",
    "        alpha = x_trunc.size(0) / X.size(0)\n",
    "\n",
    "        # all synthetic data \n",
    "        ds = TensorDataset(x_trunc, y_trunc)\n",
    "        # split ds into training and validation data sets - 80% training, 20% validation\n",
    "        train_length = int(len(ds)*.8)\n",
    "        val_length = len(ds) - train_length\n",
    "        train_ds, val_ds = ch.utils.data.random_split(ds, [train_length, val_length])\n",
    "        # train and validation loaders\n",
    "        train_loader = DataLoader(train_ds, num_workers=args.num_workers, batch_size=args.batch_size)\n",
    "        val_loader = DataLoader(val_ds, num_workers=args.num_workers, batch_size=args.batch_size)\n",
    "\n",
    "        # test dataset\n",
    "        y_test = y[~indices]\n",
    "        x_test = X[~indices]\n",
    "        \n",
    "        print('alpha: {}'.format(alpha))\n",
    "        \n",
    "    print(\"alpha: {}\".format(alpha))\n",
    "\n",
    "    \n",
    "    # reset classifier models at the beginning of each trial\n",
    "    trunc_multi_log_reg = nn.Linear(in_features=args.IN_FEATURES, out_features=args.K, bias=args.bias)\n",
    "    naive_multi_log_reg = nn.Linear(in_features=args.IN_FEATURES, out_features=args.K, bias=args.bias)\n",
    "    # optimizer and scheduler\n",
    "    trunc_opt = ch.optim.SGD(trunc_multi_log_reg.parameters(), lr=1e-1)\n",
    "    naive_opt = ch.optim.SGD(naive_multi_log_reg.parameters(), lr=1e-1)\n",
    "    # use cosine annealing for learning rate scheduler\n",
    "    trunc_scheduler = ch.optim.lr_scheduler.CosineAnnealingLR(trunc_opt, args.epochs)\n",
    "    naive_scheduler = ch.optim.lr_scheduler.CosineAnnealingLR(naive_opt, args.epochs)\n",
    "    # gradients\n",
    "    trunc_ce_loss = TruncatedGumbelCE.apply\n",
    "    ce_loss = ch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # train classifier\n",
    "    for epoch in range(args.epochs): \n",
    "        # train loop\n",
    "        trunc_train_loss, trunc_train_acc = Tensor([]), Tensor([])\n",
    "        naive_train_loss, naive_train_acc = Tensor([]), Tensor([])\n",
    "        for batch_X, batch_y in train_loader: \n",
    "            # truncated multinomial regression\n",
    "            trunc_opt.zero_grad()\n",
    "            pred = trunc_multi_log_reg(batch_X)\n",
    "            loss = trunc_ce_loss(pred, batch_y, phi)\n",
    "            loss.backward() \n",
    "            trunc_opt.step()\n",
    "            trunc_scheduler.step()\n",
    "            # keep track of truncated algorithm training loss and accuracy\n",
    "            acc = (ch.argmax(ch.nn.Softmax(dim=1)(pred), dim=1) == batch_y).sum() / batch_y.size(0)\n",
    "            trunc_train_loss = ch.cat([trunc_train_loss, Tensor([loss])]) if trunc_train_loss.size() != ch.Size([0]) else Tensor([loss])\n",
    "            trunc_train_acc = ch.cat([trunc_train_acc, Tensor([acc])]) if trunc_train_acc.size() != ch.Size([0]) else Tensor([acc])\n",
    "            \n",
    "            # naive multinomial regression\n",
    "            naive_opt.zero_grad()\n",
    "            pred = naive_multi_log_reg(batch_X)\n",
    "            loss = ce_loss(pred, batch_y)\n",
    "            loss.backward() \n",
    "            naive_opt.step()\n",
    "            naive_scheduler.step()\n",
    "            # keep track of naive algorithm training loss and accuracy\n",
    "            acc = (ch.argmax(ch.nn.Softmax(dim=1)(pred), dim=1) == batch_y).sum() / batch_y.size(0)\n",
    "            naive_train_loss = ch.cat([naive_train_loss, Tensor([loss])]) if naive_train_loss.size() != ch.Size([0]) else Tensor([loss])\n",
    "            naive_train_acc = ch.cat([naive_train_acc, Tensor([acc])]) if naive_train_acc.size() != ch.Size([0]) else Tensor([acc])\n",
    "        # validation loop\n",
    "        trunc_val_loss, trunc_val_acc = Tensor([]), Tensor([])\n",
    "        naive_val_loss, naive_val_acc = Tensor([]), Tensor([])\n",
    "        with ch.no_grad(): \n",
    "            for batch_X, batch_y in val_loader: \n",
    "                # truncated validation loop\n",
    "                pred = trunc_multi_log_reg(batch_X)\n",
    "                loss = trunc_ce_loss(pred, batch_y, phi)\n",
    "                # keep track of algorithm validation loss and accuracy\n",
    "                acc = (ch.argmax(ch.nn.Softmax(dim=1)(pred), dim=1) == batch_y).sum() / batch_y.size(0)            \n",
    "                trunc_val_loss = ch.cat([trunc_val_loss, Tensor([loss])]) if trunc_val_loss.size() != ch.Size([0]) else Tensor([loss])\n",
    "                trunc_val_acc = ch.cat([trunc_val_acc, Tensor([acc])]) if trunc_val_acc.size() != ch.Size([0]) else Tensor([acc])\n",
    "                \n",
    "                # naive validation loop\n",
    "                pred = naive_multi_log_reg(batch_X)\n",
    "                loss = ce_loss(pred, batch_y)\n",
    "                # keep track of algorithm validation loss and accuracy\n",
    "                acc = (ch.argmax(ch.nn.Softmax(dim=1)(pred), dim=1) == batch_y).sum() / batch_y.size(0)            \n",
    "                naive_val_loss = ch.cat([naive_val_loss, Tensor([loss])]) if naive_val_loss.size() != ch.Size([0]) else Tensor([loss])\n",
    "                naive_val_acc = ch.cat([naive_val_acc, Tensor([acc])]) if naive_val_acc.size() != ch.Size([0]) else Tensor([acc])\n",
    "\n",
    "            # test set accuracy\n",
    "            trunc_test_pred = trunc_multi_log_reg(x_test)\n",
    "            naive_test_pred = naive_multi_log_reg(x_test)\n",
    "            trunc_test_acc = (ch.argmax(ch.nn.Softmax(dim=1)(trunc_test_pred), dim=1) == y_test).sum() / y.size(0)\n",
    "            naive_test_acc = (ch.argmax(ch.nn.Softmax(dim=1)(naive_test_pred), dim=1) == y_test).sum() / y.size(0)\n",
    "                \n",
    "        store[LATENT_CE_TABLE_NAME].append_row({ \n",
    "            'trunc_train_acc': float(trunc_train_acc.mean()), \n",
    "            'trunc_val_acc': float(trunc_val_acc.mean()), \n",
    "            'trunc_train_loss': float(trunc_train_loss.mean()), \n",
    "            'trunc_val_loss': float(trunc_val_loss.mean()),\n",
    "            'naive_train_acc': float(naive_train_acc.mean()), \n",
    "            'naive_val_acc': float(naive_val_acc.mean()), \n",
    "            'naive_train_loss': float(naive_train_loss.mean()), \n",
    "            'naive_val_loss': float(naive_val_loss.mean()),\n",
    "            'trunc_test_acc': float(trunc_test_acc), \n",
    "            'naive_test_acc': float(naive_test_acc),\n",
    "            'epoch': int(epoch + 1),\n",
    "        })\n",
    "    \n",
    "store.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Experiment Data from Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = CollectionReader(STORE_PATH)\n",
    "results = reader.df(LATENT_CE_TABLE_NAME)\n",
    "reader.close() # close reader\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and Accuracy Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=results, x='epoch', y='trunc_train_loss', label='trunc train loss')\n",
    "sns.lineplot(data=results, x='epoch', y='naive_train_loss', label='naive train loss')\n",
    "sns.lineplot(data=results, x='epoch', y='trunc_val_loss', color='red', label='trunc val loss')\n",
    "ax = sns.lineplot(data=results, x='epoch', y='naive_val_loss', color='red', label='naive val loss')\n",
    "ax.set(xlabel='epoch', ylabel='CE Loss')\n",
    "plt.show()\n",
    "\n",
    "sns.lineplot(data=results, x='epoch', y='trunc_train_acc', label='trunc train acc')\n",
    "sns.lineplot(data=results, x='epoch', y='naive_train_acc', label='naive train acc')\n",
    "sns.lineplot(data=results, x='epoch', y='trunc_val_acc', label='trunc val acc')\n",
    "ax = sns.lineplot(data=results, x='epoch', y='naive_val_acc', label='naive val acc')\n",
    "ax.set(xlabel='epoch', ylabel='Accuracy')\n",
    "plt.show()\n",
    "\n",
    "sns.lineplot(data=results, x='epoch', y='trunc_test_acc', label='trunc test acc')\n",
    "ax = sns.lineplot(data=results, x='epoch', y='naive_test_acc', label='naive test acc')\n",
    "ax.set(xlabel='epoch', ylabel='Test Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
