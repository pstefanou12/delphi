{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../..')\n",
    "from cox.utils import Parameters\n",
    "from cox.store import Store\n",
    "from cox.readers import CollectionReader\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import itertools\n",
    "import numpy as np\n",
    "import torch as ch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Gumbel, Uniform\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import datetime\n",
    "from delphi.oracle import oracle\n",
    "from delphi import train\n",
    "from delphi.utils import constants as consts\n",
    "\n",
    "# set default tensor type \n",
    "ch.set_default_tensor_type(ch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRUNCATED_STORE_PATH = '/home/pstefanou/MultinomialLogisticRegression/Truncated/'\n",
    "STANDARD_STORE_PATH = '/home/pstefanou/MultinomialLogisticRegression/Standard/'\n",
    "\n",
    "TRUNCATED_EVAL_STORE_PATH = '/home/pstefanou/MultinomialLogisticRegression/TruncatedTest/'\n",
    "STANDARD_EVAL_STORE_PATH = '/home/pstefanou/MultinomialLogisticRegression/StandardTest/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# membership oracles\n",
    "class DNN_Lower(oracle): \n",
    "    \"\"\"\n",
    "    Lower bound truncation on the DNN logits.\n",
    "    \"\"\"\n",
    "    def __init__(self, lower): \n",
    "        self.lower = lower\n",
    "        \n",
    "    def __call__(self, x): \n",
    "        return (x > self.lower).float()\n",
    "\n",
    "\n",
    "class Identity(oracle): \n",
    "    def __call__(self, x): \n",
    "        return ch.ones(x.size())\n",
    "\n",
    "def plot():\n",
    "    # TRUNCATED CE LOSS DATA\n",
    "    trunc_reader = CollectionReader(TRUNCATED_STORE_PATH)\n",
    "    trunc_logs = trunc_reader.df(consts.LOGS_TABLE)\n",
    "    trunc_reader.close() # close reader\n",
    "\n",
    "    # STANDARD CE LOSS DATA\n",
    "    standard_reader = CollectionReader(STANDARD_STORE_PATH)\n",
    "    standard_logs = standard_reader.df(consts.LOGS_TABLE)\n",
    "    standard_reader.close() # close reader\n",
    "\n",
    "    # TEST SET RESULTS \n",
    "    trunc_test_reader = CollectionReader(TRUNCATED_EVAL_STORE_PATH)\n",
    "    trunc_test_results = trunc_test_reader.df(consts.EVAL_LOGS_TABLE)\n",
    "    trunc_test_reader.close() # close reader\n",
    "\n",
    "    # TEST SET RESULTS \n",
    "    standard_test_reader = CollectionReader(STANDARD_EVAL_STORE_PATH)\n",
    "    standard_test_results = standard_test_reader.df(consts.EVAL_LOGS_TABLE)\n",
    "    standard_test_reader.close() # close reader\n",
    "\n",
    "    sns.lineplot(data=trunc_logs, x='epoch', y='train_loss', label='Train Loss')\n",
    "    sns.lineplot(data=standard_logs, x='epoch', y='train_loss', label='Naive Train Loss')\n",
    "    sns.lineplot(data=trunc_logs, x='epoch', y='val_loss', color='red', label='Trunc Val Loss')\n",
    "    ax = sns.lineplot(data=standard_logs, x='epoch', y='val_loss', color='red', label='Naive Val Loss')\n",
    "    ax.set(xlabel='epoch', ylabel='CE Loss')\n",
    "    plt.show()\n",
    "\n",
    "    sns.lineplot(data=trunc_logs, x='epoch', y='train_prec1', label='Trunc Train Acc')\n",
    "    sns.lineplot(data=standard_logs, x='epoch', y='train_prec1', label='Naive Train Acc')\n",
    "    sns.lineplot(data=trunc_logs, x='epoch', y='val_prec1', label='Trunc Val Acc')\n",
    "    ax = sns.lineplot(data=standard_logs, x='epoch', y='val_prec1', label='Naive Val Acc')\n",
    "    ax.set(xlabel='epoch', ylabel='Accuracy')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Standard Test Accuracy: {}\".format(standard_test_results['test_prec1']))\n",
    "    print(\"Truncated Test Accuracy: {}\".format(trunc_test_results['test_prec1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CE Latent Variable Model Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GumbelCE(ch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, pred, targ):\n",
    "        ctx.save_for_backward(pred, targ)\n",
    "        ce_loss = ch.nn.CrossEntropyLoss()\n",
    "        return ce_loss(pred, targ)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        pred, targ = ctx.saved_tensors\n",
    "        # initialize gumbel distribution\n",
    "        gumbel = Gumbel(0, 1)\n",
    "        # make num_samples copies of pred logits\n",
    "        stacked = pred[None, ...].repeat(args.num_samples, 1, 1)        \n",
    "        # add gumbel noise to logits\n",
    "        rand_noise = gumbel.sample(stacked.size())\n",
    "        noised = stacked + rand_noise \n",
    "        noised_labs = noised.argmax(-1)\n",
    "        # remove the logits from the trials, where the kth logit is not the largest value\n",
    "        good_mask = noised_labs.eq(targ)[..., None]\n",
    "        inner_exp = 1 - ch.exp(-rand_noise)\n",
    "        avg = (inner_exp * good_mask).sum(0) / (good_mask.sum(0) + 1e-5) / pred.size(0)\n",
    "        return -avg , None\n",
    "    \n",
    "class TruncatedGumbelCE(ch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, pred, targ):\n",
    "        ctx.save_for_backward(pred, targ)\n",
    "        ce_loss = ch.nn.CrossEntropyLoss()\n",
    "        return ce_loss(pred, targ)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        pred, targ = ctx.saved_tensors\n",
    "        # initialize gumbel distribution\n",
    "        gumbel = Gumbel(0, 1)\n",
    "        # make num_samples copies of pred logits\n",
    "        stacked = pred[None, ...].repeat(args.num_samples, 1, 1)   \n",
    "        # add gumbel noise to logits\n",
    "        rand_noise = gumbel.sample(stacked.size())\n",
    "        noised = stacked + rand_noise \n",
    "        # truncate - if one of the noisy logits does not fall within the truncation set, remove it\n",
    "        filtered = ch.all(args.phi(noised).bool(), dim=2).float().unsqueeze(2)\n",
    "        noised_labs = noised.argmax(-1)\n",
    "        # mask takes care of invalid logits and truncation set\n",
    "        mask = noised_labs.eq(targ)[..., None] * filtered\n",
    "        inner_exp = 1 - ch.exp(-rand_noise)\n",
    "                \n",
    "        avg = ((inner_exp * mask).sum(0) / (mask.sum(0) + 1e-5) - (inner_exp * filtered).sum(0) / (filtered.sum(0) + 1e-5)) \n",
    "        return -avg / pred.size(0), None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"epochs\": 25,\n",
       "  \"num_workers\": 0,\n",
       "  \"batch_size\": 100,\n",
       "  \"bias\": true,\n",
       "  \"num_samples\": 100000,\n",
       "  \"clamp\": true,\n",
       "  \"radius\": 5.0,\n",
       "  \"lr\": 0.1,\n",
       "  \"shuffle\": true,\n",
       "  \"samples\": 10000,\n",
       "  \"in_features\": 10,\n",
       "  \"k\": 10,\n",
       "  \"lower\": -1,\n",
       "  \"upper\": 1,\n",
       "  \"trials\": 10,\n",
       "  \"log_iters\": 1,\n",
       "  \"should_save_ckpt\": true,\n",
       "  \"save_ckpt_iters\": -1,\n",
       "  \"validation_split\": 0.8,\n",
       "  \"momentum\": 0.0,\n",
       "  \"weight_decay\": 0.0,\n",
       "  \"custom_lr_multiplier\": \"cosine\",\n",
       "  \"device\": \"cuda\"\n",
       "}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# procedure hyperparameters\n",
    "args = Parameters({ \n",
    "    'epochs': 25,\n",
    "    'num_workers': 0, \n",
    "    'batch_size': 100,\n",
    "    'bias': True,\n",
    "    'num_samples': 100000,\n",
    "    'clamp': True, \n",
    "    'radius': 5.0, \n",
    "    'lr': 1e-1,\n",
    "    'shuffle': False, \n",
    "    'samples': 10000,  # number of samples to generate for ground truth\n",
    "    'in_features': 10, # number of in-features to multi-log-reg\n",
    "    'k': 10, # number of classes\n",
    "    'lower': -1, # lower bound for generating ground truth weights\n",
    "    'upper': 1,  # upper bound for generating ground truth weights\n",
    "    'trials': 10,\n",
    "    'log_iters': 1,    \n",
    "    'should_save_ckpt': True,\n",
    "    'save_ckpt_iters': -1,\n",
    "    'validation_split': .8,\n",
    "    'momentum': 0.0,\n",
    "    'weight_decay': 0.0,\n",
    "    'custom_lr_multiplier': consts.COSINE, \n",
    "    'shuffle': True,\n",
    "})\n",
    "\n",
    "if ch.cuda.is_available(): \n",
    "    args.__setattr__('device', 'cuda')\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truncated Multinomial Logistic Regression Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phi = DNN_Lower(ch.full(ch.Size([args.K,]), -5))\n",
    "phi = DNN_Lower(Tensor([-2, -3, -2, -3, -4, -5, -6, -7, -6, -5]))\n",
    "# phi = Identity()\n",
    "\n",
    "# distributions\n",
    "gumbel = Gumbel(0, 1)\n",
    "U = Uniform(args.lower, args.upper) # distribution to generate ground-truth parameters\n",
    "U_ = Uniform(-5, 5) # distribution to generate samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4ed5ea0892646a199f61e172210c109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=17.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# perform number of trials experiments\n",
    "for i in range(args.trials):\n",
    "    # generate synthetic data until survival probability of more than 40%\n",
    "    alpha = None\n",
    "    while alpha is None or alpha < .2:\n",
    "        # generate ground-truth from uniform distribution\n",
    "        ground_truth = nn.Linear(in_features=args.IN_FEATURES, out_features=args.K, bias=args.bias)\n",
    "        ground_truth.weight = nn.Parameter(U.sample(ch.Size([args.K, args.IN_FEATURES])))\n",
    "        if ground_truth.bias is not None: \n",
    "            ground_truth.bias = nn.Parameter(U.sample(ch.Size([args.K,])))\n",
    "        # independent variable \n",
    "        X = U_.sample(ch.Size([args.samples, args.IN_FEATURES]))\n",
    "        # determine base model logits \n",
    "        z = ground_truth(X)\n",
    "        # apply softmax to unnormalized likelihoods\n",
    "        y = ch.argmax(ch.nn.Softmax(dim=1)(z), dim=1)\n",
    "\n",
    "        # TRUNCATE\n",
    "        trunc = phi(z)\n",
    "        indices = ch.all(trunc.bool(), dim=1).float().nonzero(as_tuple=False).flatten()\n",
    "        y_trunc = y[indices]\n",
    "        x_trunc = X[indices]\n",
    "        alpha = x_trunc.size(0) / X.size(0)\n",
    "        print(\"alpha: {}\".format(alpha))\n",
    "\n",
    "        # all synthetic data \n",
    "        ds = TensorDataset(x_trunc, y_trunc)\n",
    "        # split ds into training and validation data sets - 80% training, 20% validation\n",
    "        train_length = int(len(ds)*.8)\n",
    "        val_length = len(ds) - train_length\n",
    "        train_ds, val_ds = ch.utils.data.random_split(ds, [train_length, val_length])\n",
    "        # train and validation loaders\n",
    "        train_loader = DataLoader(train_ds, num_workers=args.num_workers, batch_size=args.batch_size)\n",
    "        val_loader = DataLoader(val_ds, num_workers=args.num_workers, batch_size=args.batch_size)\n",
    "        loaders = (train_loader, val_loader)\n",
    "\n",
    "        # test dataset\n",
    "        x_test = X[~indices]\n",
    "        y_test = y[~indices]\n",
    "        test_ds = TensorDataset(x_test, y_test)\n",
    "        test_loader = DataLoader(test_ds, num_workers=args.num_workers, batch_size=args.batch_size)\n",
    "    \n",
    "    # new classifier models at the beginning of each trial\n",
    "    trunc_multi_log_reg = nn.Linear(in_features=args.IN_FEATURES, out_features=args.K, bias=args.bias)\n",
    "    \n",
    "    # truncated store\n",
    "    out_store = Store(TRUNCATED_STORE_PATH)\n",
    "    trunc_ce = TruncatedGumbelCE.apply\n",
    "    args.__setattr__('custom_criterion', trunc_ce)  # truncated ce loss\n",
    "    args.__setattr__('phi', phi)\n",
    "    train.train_model(args, trunc_multi_log_reg, loaders, store=out_store, device=args.device)\n",
    "    \n",
    "    # new classifier models at the beginning of each trial\n",
    "    standard_multi_log_reg = nn.Linear(in_features=args.IN_FEATURES, out_features=args.K, bias=args.bias)\n",
    "    \n",
    "    # naive ce loss\n",
    "    out_store = Store(STANDARD_STORE_PATH)\n",
    "    args.__setattr__('custom_criterion', None) # default ce loss\n",
    "    train.train_model(args, standard_multi_log_reg, loaders, store=out_store, device=args.device)\n",
    "    \n",
    "    # truncated multinomial logistic regression eval\n",
    "    out_store = Store(TRUNCATED_EVAL_STORE_PATH)\n",
    "    train.eval_model(args, trunc_multi_log_reg, test_loader, out_store)\n",
    "\n",
    "    \n",
    "    # standard multinomial logistic regression eval\n",
    "    out_store = Store(STANDARD_EVAL_STORE_PATH)\n",
    "    train.eval_model(args, standard_multi_log_reg, test_loader, out_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and Accuracy Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
