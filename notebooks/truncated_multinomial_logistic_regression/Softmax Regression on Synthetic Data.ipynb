{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../..')\n",
    "sys.path.append('/opt/anaconda3/lib/python3.7/site-packages')\n",
    "from cox.utils import Parameters\n",
    "from cox.store import Store\n",
    "from cox.readers import CollectionReader\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import itertools\n",
    "import numpy as np\n",
    "import torch as ch\n",
    "from torch import Tensor\n",
    "from torch import sigmoid as sig\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD, lr_scheduler\n",
    "from torch.distributions import Gumbel, Uniform\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.distributions.transforms import SigmoidTransform\n",
    "from torch.distributions.transformed_distribution import TransformedDistribution\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from tqdm.autonotebook import tqdm as tqdm\n",
    "from abc import ABC\n",
    "import os\n",
    "import dill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRUNCATED_STORE_PATH = '/Users/patroklos/MultinomialLogisticRegressionTruncated/'\n",
    "STANDARD_STORE_PATH = '/Users/patroklos/MultinomialLogisticRegressionStandard/'\n",
    "\n",
    "TRUNCATED_EVAL_STORE_PATH = '/Users/patroklos/MultinomialLogisticRegressionTruncatedTest/'\n",
    "STANDARD_EVAL_STORE_PATH = '/Users/patroklos/MultinomialLogisticRegressionStandardTest/'\n",
    "\n",
    "GUMBEL_CE_STORE_PATH = '/Users/patroklos/MultinomialLogisticRegressionGumbelCE'\n",
    "\n",
    "LOGS_SCHEMA = {\n",
    "    'epoch':int,\n",
    "    'val_prec1':float,\n",
    "    'val_loss':float,\n",
    "    'train_prec1':float,\n",
    "    'train_loss':float,\n",
    "    'time':float\n",
    "}\n",
    "\n",
    "EVAL_LOGS_SCHEMA = {\n",
    "    'test_prec1':float,\n",
    "    'test_loss':float,\n",
    "    'time':float\n",
    "}\n",
    "\n",
    "# scheduler constants\n",
    "CYCLIC='cyclic'\n",
    "COSINE='cosine'\n",
    "LINEAR='linear'\n",
    "\n",
    "LOGS_TABLE = 'logs'\n",
    "EVAL_LOGS_TABLE = 'eval'\n",
    "\n",
    "CKPT_NAME = 'checkpoint.pt'\n",
    "BEST_APPEND = '.best'\n",
    "CKPT_NAME_LATEST = CKPT_NAME + '.latest'\n",
    "CKPT_NAME_BEST = CKPT_NAME + BEST_APPEND"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation Procedure Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer_and_schedule(args, model, params):\n",
    "    param_list = model.parameters() if params is None else params\n",
    "\n",
    "    optimizer = SGD(param_list, args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "\n",
    "    # Make schedule\n",
    "    schedule = None\n",
    "    if args.custom_lr_multiplier == CYCLIC:\n",
    "        eps = args.epochs\n",
    "        lr_func = lambda t: np.interp([t], [0, eps*4//15, eps], [0, 1, 0])[0]\n",
    "        schedule = lr_scheduler.LambdaLR(optimizer, lr_func)\n",
    "    elif args.custom_lr_multiplier == COSINE:\n",
    "        eps = args.epochs\n",
    "        schedule = lr_scheduler.CosineAnnealingLR(optimizer, eps)\n",
    "    elif args.custom_lr_multiplier:\n",
    "        cs = args.custom_lr_multiplier\n",
    "        periods = eval(cs) if type(cs) is str else cs\n",
    "        if args.lr_interpolation == LINEAR:\n",
    "            lr_func = lambda t: np.interp([t], *zip(*periods))[0]\n",
    "        else:\n",
    "            def lr_func(ep):\n",
    "                for (milestone, lr) in reversed(periods):\n",
    "                    if ep >= milestone: return lr\n",
    "                return 1.0\n",
    "        schedule = lr_scheduler.LambdaLR(optimizer, lr_func)\n",
    "    elif args.step_lr:\n",
    "        schedule = lr_scheduler.StepLR(optimizer, step_size=args.step_lr, gamma=args.step_lr_gamma)\n",
    "        \n",
    "    return optimizer, schedule\n",
    "\n",
    "\n",
    "def eval_model(args, model, loader, store):\n",
    "    \"\"\"\n",
    "    Evaluate a model for standard (and optionally adversarial) accuracy.\n",
    "    Args:\n",
    "        args (object) : A list of arguments---should be a python object\n",
    "            implementing ``getattr()`` and ``setattr()``.\n",
    "        model (AttackerModel) : model to evaluate\n",
    "        loader (iterable) : a dataloader serving `(input, label)` batches from\n",
    "            the validation set\n",
    "        store (cox.Store) : store for saving results in (via tensorboardX)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    if store is not None:\n",
    "        store.add_table(EVAL_LOGS_TABLE, EVAL_LOGS_SCHEMA)\n",
    "    writer = store.tensorboard if store else None\n",
    "\n",
    "    # put model on device\n",
    "    model.to(args.device)\n",
    "\n",
    "    assert not hasattr(model, \"module\"), \"model is already in DataParallel.\"\n",
    "    if next(model.parameters()).is_cuda and False:\n",
    "        model = ch.nn.DataParallel(model)\n",
    "\n",
    "    test_prec1, test_loss, score = model_loop(args, 'val', loader,\n",
    "                                        model, None, 0, writer, args.device)\n",
    "\n",
    "    log_info = {\n",
    "        'test_prec1': test_prec1,\n",
    "        'test_loss': test_loss,\n",
    "        'time': time.time() - start_time\n",
    "    }\n",
    "\n",
    "    # Log info into the logs table\n",
    "    if store:\n",
    "        store[EVAL_LOGS_TABLE].append_row(log_info)\n",
    "        store.close()\n",
    "\n",
    "    return log_info\n",
    "\n",
    "\n",
    "def train_model(args, model, loaders, *, device=\"cpu\", dp_device_ids=None,\n",
    "                store=None, update_params=None, disable_no_grad=False):\n",
    "    # clear jupyter/ipython output before each training run\n",
    "    if store is not None:\n",
    "        store.add_table(LOGS_TABLE, LOGS_SCHEMA)\n",
    "    writer = store.tensorboard if store else None\n",
    "\n",
    "    # data loaders\n",
    "    train_loader, val_loader = loaders\n",
    "    optimizer, schedule = make_optimizer_and_schedule(args, model, update_params)\n",
    "\n",
    "    # put the model into parallel mode\n",
    "    assert not has_attr(model, \"module\"), \"model is already in DataParallel.\"\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    best_prec1, start_epoch = (0, 0)\n",
    "\n",
    "    # keep track of the start time\n",
    "    start_time = time.time()\n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "        train_prec1, train_loss, score = model_loop(args, 'train', train_loader, model, optimizer, epoch+1, writer, device=device)\n",
    "\n",
    "        # check score tolerance\n",
    "        if args.score and ch.all(ch.where(ch.abs(score) < args.tol, ch.ones(1), ch.zeros(1)).bool()):\n",
    "            break\n",
    "\n",
    "        last_epoch = (epoch == (args.epochs - 1))\n",
    "\n",
    "        # if neural network passed through framework, use log performance\n",
    "        if args.should_save_ckpt:\n",
    "            # evaluate on validation set\n",
    "            sd_info = {\n",
    "                'model':model.state_dict(),\n",
    "                'optimizer':optimizer.state_dict(),\n",
    "                'schedule':(schedule and schedule.state_dict()),\n",
    "                'epoch': epoch+1,\n",
    "            }\n",
    "\n",
    "            def save_checkpoint(filename):\n",
    "                ckpt_save_path = os.path.join(args.out_dir if not store else \\\n",
    "                                              store.path, filename)\n",
    "                ch.save(sd_info, ckpt_save_path, pickle_module=dill)\n",
    "\n",
    "            save_its = args.save_ckpt_iters\n",
    "            should_save_ckpt = (epoch % save_its == 0) and (save_its > 0)\n",
    "            should_log = (epoch % args.log_iters == 0)\n",
    "\n",
    "            if should_log or last_epoch or should_save_ckpt:\n",
    "                # log + get best\n",
    "                ctx = ch.enable_grad() if disable_no_grad else ch.no_grad()\n",
    "                with ctx:\n",
    "                    val_prec1, val_loss, score = model_loop(args, 'val', val_loader, model,\n",
    "                            None, epoch + 1, writer, device=device)\n",
    "\n",
    "                # remember best prec@1 and save checkpoint\n",
    "                is_best = val_prec1 > best_prec1\n",
    "                best_prec1 = max(val_prec1, best_prec1)\n",
    "                sd_info['prec1'] = val_prec1\n",
    "\n",
    "                # TODO: add custom logging hook\n",
    "\n",
    "                # log every checkpoint\n",
    "                log_info = {\n",
    "                    'epoch': epoch + 1,\n",
    "                    'val_prec1': val_prec1,\n",
    "                    'val_loss': val_loss,\n",
    "                    'train_prec1': train_prec1,\n",
    "                    'train_loss': train_loss,\n",
    "                    'time': time.time() - start_time\n",
    "                }\n",
    "\n",
    "                # Log info into the logs table\n",
    "                if store: store[LOGS_TABLE].append_row(log_info)\n",
    "                # If we are at a saving epoch (or the last epoch), save a checkpoint\n",
    "                if should_save_ckpt or last_epoch: save_checkpoint(ckpt_at_epoch(epoch))\n",
    "\n",
    "                # Update the latest and best checkpoints (overrides old one)\n",
    "                save_checkpoint(CKPT_NAME_LATEST)\n",
    "                if is_best: save_checkpoint(CKPT_NAME_BEST)\n",
    "        \n",
    "        # update lr\n",
    "        if schedule: schedule.step()\n",
    "\n",
    "        tqdm._instances.clear()\n",
    "\n",
    "    # TODO: add end training hook\n",
    "\n",
    "    # model results\n",
    "    if isinstance(score, Tensor):\n",
    "        print(\"avg score: \\n {}\".format([round(x, 4) for x in score.tolist()]))\n",
    "    if train_loss != 0:\n",
    "        print(\"avg loss: {}\".format(train_loss))\n",
    "    if train_prec1 != 0:\n",
    "        print(\"avg top 1: {}\".format(train_prec1))\n",
    "\n",
    "    # close store at end of training\n",
    "    if store is not None:\n",
    "        store.close()\n",
    "    return model\n",
    "            \n",
    "            \n",
    "def model_loop(args, loop_type, loader, model, optimizer, epoch, writer, device):\n",
    "    # check loop type \n",
    "    if not loop_type in ['train', 'val']: \n",
    "        err_msg = \"loop type must be in {0} must be 'train' or 'val\".format(loop_type)\n",
    "        raise ValueError(err_msg)\n",
    "    is_train = (loop_type == 'train')\n",
    "    \n",
    "    loop_msg = 'Train' if is_train else 'Val'\n",
    "\n",
    "    # algorithm metrics\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    score = AverageMeter()\n",
    "    \n",
    "    # check for custom criterion\n",
    "    has_custom_criterion = has_attr(args, 'custom_criterion')\n",
    "    criterion = args.custom_criterion if has_custom_criterion else ch.nn.CrossEntropyLoss()\n",
    "\n",
    "    iterator = tqdm(enumerate(loader), total=len(loader))\n",
    "    for i, batch in iterator:\n",
    "        inp, target, output = None, None, None\n",
    "        loss = 0.0\n",
    "\n",
    "        inp, target = batch\n",
    "        inp, target = inp.to(device), target.to(device)\n",
    "        output = model(inp)\n",
    "        # attacker model returns both output anf final input\n",
    "        if isinstance(output, tuple):\n",
    "            output, final_inp = output\n",
    "        # lambda parameter used for regression with unknown noise variance\n",
    "        try:\n",
    "            loss = criterion(output, target, model.lambda_)\n",
    "        except Exception as e:\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "        # regularizer option \n",
    "        reg_term = 0.0\n",
    "        if has_attr(args, \"regularizer\") and isinstance(model, ch.nn.Module):\n",
    "            reg_term = args.regularizer(model, inp, target)\n",
    "        loss = loss + reg_term\n",
    "        \n",
    "        # perform backprop and take optimizer step\n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if len(loss.size()) > 0: loss = loss.mean()\n",
    "\n",
    "        model_logits = output[0] if isinstance(output, tuple) else output\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        top1_acc = float('nan')\n",
    "        top5_acc = float('nan')\n",
    "        try:\n",
    "            losses.update(loss.item(), inp.size(0))\n",
    "            # calculate score\n",
    "            if args.bias:\n",
    "                score.update(ch.cat([model.weight.grad.T, model.bias.grad.unsqueeze(0)]).flatten(), inp.size(0))\n",
    "            else:\n",
    "                score.update(ch.cat([model.weight.grad.T]).flatten(), inp.size(0))\n",
    "\n",
    "            if model_logits is not None:\n",
    "                # accuracy\n",
    "                maxk = min(5, model_logits.shape[-1])\n",
    "                if has_attr(args, \"custom_accuracy\"):\n",
    "                    prec1, prec5 = args.custom_accuracy(model_logits, target)\n",
    "                else:\n",
    "                    prec1, prec5 = accuracy(model_logits, target, topk=(1, maxk))\n",
    "                    prec1, prec5 = prec1[0], prec5[0]\n",
    "\n",
    "                top1.update(prec1, inp.size(0))\n",
    "                top5.update(prec5, inp.size(0))\n",
    "                top1_acc = top1.avg\n",
    "                top5_acc = top5.avg\n",
    "\n",
    "                # ITERATOR\n",
    "                desc = ('Epoch:{0} | Score: {score} \\n | Loss {loss.avg:.4f} | '\n",
    "                        '{1}1 {top1_acc:.3f} | {1}5 {top5_acc:.3f} | '\n",
    "                        'Reg term: {reg} ||'.format(epoch, loop_msg, score=[round(x, 4) for x in score.avg.tolist()],\n",
    "                                                    loss=losses, top1_acc=top1_acc, top5_acc=top5_acc, reg=reg_term))\n",
    "        except Exception as e:\n",
    "            warnings.warn('Failed to calculate the accuracy.')\n",
    "            # ITERATOR\n",
    "            desc = ('Epoch:{0} |  Score: {score} \\n | Loss {loss.avg:.4f} ||'.format(\n",
    "                epoch, loop_msg, score=[round(x, 4) for x in score.avg.tolist()], loss=losses))\n",
    "        \n",
    "        iterator.set_description(desc)\n",
    "    \n",
    "        # USER-DEFINED HOOK\n",
    "        if has_attr(args, 'iteration_hook'):\n",
    "            args.iteration_hook(model, i, loop_type, inp, target)\n",
    "\n",
    "    if writer is not None:\n",
    "        descs = ['loss', 'top1', 'top5']\n",
    "        vals = [losses, top1, top5]\n",
    "        for d, v in zip(descs, vals):\n",
    "            writer.add_scalar('_'.join([loop_type, d]), v.avg,\n",
    "                              epoch)\n",
    "\n",
    "    # LOSS AND ACCURACY\n",
    "    return top1.avg, losses.avg, score.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# membership oracles\n",
    "class oracle(ABC):\n",
    "    \"\"\"\n",
    "    Oracle for data sets.\n",
    "    \"\"\"\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Membership oracle.\n",
    "        Args: \n",
    "            x: samples to check membership\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "class DNN_Lower(oracle): \n",
    "    \"\"\"\n",
    "    Lower bound truncation on the DNN logits.\n",
    "    \"\"\"\n",
    "    def __init__(self, lower): \n",
    "        self.lower = lower\n",
    "        \n",
    "    def __call__(self, x): \n",
    "        return (x > self.lower).float()\n",
    "    \n",
    "class DNN_Logit_Ball(oracle): \n",
    "    \"\"\"\n",
    "    Truncation ball placed on DNN logits.\n",
    "    INTUITION: logits that are neither very large nor very small insinuate\n",
    "    that the classification is not \n",
    "    \"\"\"\n",
    "    def __init__(self, lower, upper): \n",
    "        self.lower = lower \n",
    "        self.upper = upper\n",
    "        \n",
    "    def __call__(self, x): \n",
    "        return ((x < self.lower) | (x > self.upper)).float()\n",
    "        \n",
    "\n",
    "class Identity(oracle): \n",
    "    def __call__(self, x): \n",
    "        return ch.ones(x.size())\n",
    "    \n",
    "def gen_data(): \n",
    "    \"\"\"\n",
    "    Generate dataset for truncated multinomial logistic \n",
    "    regression model. Returns ground_truth and train, validation, and test loaders.\n",
    "    \"\"\"\n",
    "    # distributions\n",
    "    gumbel = Gumbel(0, 1)\n",
    "    U = Uniform(args.lower, args.upper) # distribution to generate ground-truth parameters\n",
    "    U_ = Uniform(-5, 5) # distribution to generate samples\n",
    "    \n",
    "    # no grad required for dataset\n",
    "    with ch.no_grad():\n",
    "        # generate synthetic data until survival probability of more than 40%\n",
    "        alpha = None\n",
    "        while alpha is None or alpha < args.ALPHA_THRESH:\n",
    "            # generate ground-truth from uniform distribution\n",
    "            ground_truth = nn.Linear(in_features=args.IN_FEATURES, out_features=args.K, bias=args.bias)\n",
    "            ground_truth.weight = nn.Parameter(U.sample(ch.Size([args.K, args.IN_FEATURES])))\n",
    "            if ground_truth.bias is not None: \n",
    "                ground_truth.bias = nn.Parameter(U.sample(ch.Size([args.K,])))\n",
    "            # independent variable \n",
    "            X = U_.sample(ch.Size([args.samples, args.IN_FEATURES]))\n",
    "            # determine base model logits \n",
    "            z = ground_truth(X)\n",
    "            # add noise to the logits\n",
    "            noised = z + gumbel.sample(z.size())\n",
    "            # apply softmax to unnormalized likelihoods\n",
    "            y = ch.argmax(noised, dim=1)\n",
    "\n",
    "            # TRUNCATE\n",
    "            trunc = args.phi(z)\n",
    "            indices = ch.all(trunc.bool(), dim=1).float().nonzero(as_tuple=False).flatten()\n",
    "            x_trunc, y_trunc = X[indices], y[indices]\n",
    "            alpha = x_trunc.size(0) / X.size(0)\n",
    "\n",
    "            # all synthetic data \n",
    "            ds = TensorDataset(x_trunc, y_trunc)\n",
    "            # split ds into training and validation data sets - 80% training, 20% validation\n",
    "            train_length = int(len(ds)*.8)\n",
    "            val_length = len(ds) - train_length\n",
    "            train_ds, val_ds = ch.utils.data.random_split(ds, [train_length, val_length])\n",
    "            # train and validation loaders\n",
    "            train_loader = DataLoader(train_ds, num_workers=args.num_workers, batch_size=args.batch_size)\n",
    "            val_loader = DataLoader(val_ds, num_workers=args.num_workers, batch_size=args.batch_size)\n",
    "\n",
    "            # test dataset\n",
    "            x_test = X[~indices]\n",
    "            y_test = y[~indices]\n",
    "            test_ds = TensorDataset(x_test, y_test)\n",
    "            test_loader = DataLoader(test_ds, num_workers=args.num_workers, batch_size=args.batch_size)\n",
    "            \n",
    "    return ground_truth, (train_loader, val_loader), test_loader\n",
    "\n",
    "def plot():\n",
    "    # TRUNCATED CE LOSS DATA\n",
    "    trunc_reader = CollectionReader(TRUNCATED_STORE_PATH)\n",
    "    trunc_logs = trunc_reader.df(LOGS_TABLE)\n",
    "    trunc_reader.close() # close reader\n",
    "\n",
    "    # STANDARD CE LOSS DATA\n",
    "    standard_reader = CollectionReader(STANDARD_STORE_PATH)\n",
    "    standard_logs = standard_reader.df(LOGS_TABLE)\n",
    "    standard_reader.close() # close reader\n",
    "\n",
    "    # TEST SET RESULTS \n",
    "    trunc_test_reader = CollectionReader(TRUNCATED_EVAL_STORE_PATH)\n",
    "    trunc_test_results = trunc_test_reader.df(EVAL_LOGS_TABLE)\n",
    "    trunc_test_reader.close() # close reader\n",
    "\n",
    "\n",
    "\n",
    "    sns.lineplot(data=trunc_logs, x='epoch', y='train_loss', label='Train Loss')\n",
    "    sns.lineplot(data=standard_logs, x='epoch', y='train_loss', label='Naive Train Loss')\n",
    "    sns.lineplot(data=trunc_logs, x='epoch', y='val_loss', color='red', label='Trunc Val Loss')\n",
    "    ax = sns.lineplot(data=standard_logs, x='epoch', y='val_loss', color='red', label='Naive Val Loss')\n",
    "    ax.set(xlabel='epoch', ylabel='CE Loss')\n",
    "    plt.show()\n",
    "\n",
    "    sns.lineplot(data=trunc_logs, x='epoch', y='train_prec1', label='Trunc Train Acc')\n",
    "    sns.lineplot(data=standard_logs, x='epoch', y='train_prec1', label='Naive Train Acc')\n",
    "    sns.lineplot(data=trunc_logs, x='epoch', y='val_prec1', label='Trunc Val Acc')\n",
    "    ax = sns.lineplot(data=standard_logs, x='epoch', y='val_prec1', label='Naive Val Acc')\n",
    "    ax.set(xlabel='epoch', ylabel='Accuracy')\n",
    "    plt.show()\n",
    "    \n",
    "    # TEST SET RESULTS \n",
    "    try: \n",
    "        standard_test_reader = CollectionReader(STANDARD_EVAL_STORE_PATH)\n",
    "        standard_test_results = standard_test_reader.df(EVAL_LOGS_TABLE)\n",
    "        standard_test_reader.close() # close reader\n",
    "\n",
    "        print(\"Standard Test Accuracy: {}\".format(standard_test_results['test_prec1']))\n",
    "        print(\"Truncated Test Accuracy: {}\".format(trunc_test_results['test_prec1']))\n",
    "    except: \n",
    "        print(\"No Test Results to Report\")\n",
    "        \n",
    "        \n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def has_attr(obj, k):\n",
    "    \"\"\"Checks both that obj.k exists and is not equal to None\"\"\"\n",
    "    try:\n",
    "        return (getattr(obj, k) is not None)\n",
    "    except KeyError as e:\n",
    "        return False\n",
    "    except AttributeError as e:\n",
    "        return False\n",
    "    \n",
    "def accuracy(output, target, topk=(1,), exact=False):\n",
    "    \"\"\"\n",
    "        Computes the top-k accuracy for the specified values of k\n",
    "\n",
    "        Args:\n",
    "            output (ch.Tensor) : model output (N, classes) or (N, attributes) \n",
    "                for sigmoid/multitask binary classification\n",
    "            target (ch.Tensor) : correct labels (N,) [multiclass] or (N,\n",
    "                attributes) [multitask binary]\n",
    "            topk (tuple) : for each item \"k\" in this tuple, this method\n",
    "                will return the top-k accuracy\n",
    "            exact (bool) : whether to return aggregate statistics (if\n",
    "                False) or per-example correctness (if True)\n",
    "\n",
    "        Returns:\n",
    "            A list of top-k accuracies.\n",
    "    \"\"\"\n",
    "    with ch.no_grad():\n",
    "        # Binary Classification\n",
    "        if len(target.shape) > 1:\n",
    "            assert output.shape == target.shape, \\\n",
    "                \"Detected binary classification but output shape != target shape\"\n",
    "            return [ch.round(ch.sigmoid(output)).eq(ch.round(target)).float().mean()], [-1.0] \n",
    "\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        res_exact = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float()\n",
    "            ck_sum = correct_k.sum(0, keepdim=True)\n",
    "            res.append(ck_sum.mul_(100.0 / batch_size))\n",
    "            res_exact.append(correct_k)\n",
    "\n",
    "        if not exact:\n",
    "            return res\n",
    "        else:\n",
    "            return res_exact\n",
    "        \n",
    "def ckpt_at_epoch(num):\n",
    "    return '%s_%s' % (num, CKPT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CE Latent Variable Model Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TruncatedBCE(ch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, pred, targ):\n",
    "        ctx.save_for_backward(pred, targ)\n",
    "        loss = ch.nn.BCEWithLogitsLoss()\n",
    "        return loss(pred, targ)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        pred, targ = ctx.saved_tensors\n",
    "\n",
    "        # logistic distribution\n",
    "        base_distribution = Uniform(0, 1)\n",
    "        transforms_ = [SigmoidTransform().inv]\n",
    "        logistic = TransformedDistribution(base_distribution, transforms_)\n",
    "\n",
    "        stacked = pred[None, ...].repeat(args.num_samples, 1, 1)\n",
    "        # add noise\n",
    "        noised = stacked + logistic.sample(stacked.size())\n",
    "        # filter\n",
    "        filtered = ch.stack([args.phi(batch) for batch in noised]).float()\n",
    "        out = (noised * filtered).sum(dim=0) / (filtered.sum(dim=0) + 1e-5)\n",
    "        grad = ch.where(ch.abs(out) > 1e-5, sig(out), targ) - targ\n",
    "        return grad / pred.size(0), -grad / pred.size(0)\n",
    "\n",
    "class GumbelCE(ch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, pred, targ):\n",
    "        ctx.save_for_backward(pred, targ)\n",
    "        ce_loss = ch.nn.CrossEntropyLoss()\n",
    "        return ce_loss(pred, targ)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        pred, targ = ctx.saved_tensors\n",
    "        # gumbel distribution\n",
    "        gumbel = Gumbel(0, 1)\n",
    "        # make num_samples copies of pred logits\n",
    "        stacked = pred[None, ...].repeat(args.num_samples, 1, 1)        \n",
    "        # add gumbel noise to logits\n",
    "        rand_noise = gumbel.sample(stacked.size())\n",
    "        noised = stacked + rand_noise \n",
    "        noised_labs = noised.argmax(-1)\n",
    "        # remove the logits from the trials, where the kth logit is not the largest value\n",
    "        good_mask = noised_labs.eq(targ)[..., None]\n",
    "        inner_exp = 1 - ch.exp(-rand_noise)\n",
    "        avg = (inner_exp * good_mask).sum(0) / (good_mask.sum(0) + 1e-5) / pred.size(0)\n",
    "        return -avg , None\n",
    "    \n",
    "class TruncatedGumbelCE(ch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, pred, targ):\n",
    "        ctx.save_for_backward(pred, targ)\n",
    "        ce_loss = ch.nn.CrossEntropyLoss()\n",
    "        return ce_loss(pred, targ)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        pred, targ = ctx.saved_tensors\n",
    "        # initialize gumbel distribution\n",
    "        gumbel = Gumbel(0, 1)\n",
    "        # make num_samples copies of pred logits\n",
    "        stacked = pred[None, ...].repeat(args.num_samples, 1, 1)   \n",
    "        # add gumbel noise to logits\n",
    "        rand_noise = gumbel.sample(stacked.size())\n",
    "        noised = stacked + rand_noise \n",
    "        # truncate - if one of the noisy logits does not fall within the truncation set, remove it\n",
    "        filtered = ch.all(args.phi(noised).bool(), dim=2).float().unsqueeze(2)\n",
    "        noised_labs = noised.argmax(-1)\n",
    "        # mask takes care of invalid logits and truncation set\n",
    "        mask = noised_labs.eq(targ)[..., None] * filtered\n",
    "        inner_exp = 1 - ch.exp(-rand_noise)\n",
    "\n",
    "        avg = (((inner_exp * mask).sum(0) / (mask.sum(0) + 1e-5)) - ((inner_exp * filtered).sum(0) / (filtered.sum(0) + 1e-5))) \n",
    "        return -avg / pred.size(0), None, None\n",
    "\n",
    "# gradients\n",
    "trunc_bce = TruncatedBCE.apply\n",
    "gumbel_ce = GumbelCE.apply\n",
    "trunc_ce = TruncatedGumbelCE.apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"epochs\": 25,\n",
       "  \"num_workers\": 0,\n",
       "  \"batch_size\": 100,\n",
       "  \"bias\": true,\n",
       "  \"num_samples\": 1000,\n",
       "  \"clamp\": true,\n",
       "  \"radius\": 5.0,\n",
       "  \"lr\": 0.1,\n",
       "  \"shuffle\": true,\n",
       "  \"samples\": 10000,\n",
       "  \"in_features\": 2,\n",
       "  \"k\": 2,\n",
       "  \"lower\": -1,\n",
       "  \"upper\": 1,\n",
       "  \"trials\": 1,\n",
       "  \"log_iters\": 1,\n",
       "  \"should_save_ckpt\": true,\n",
       "  \"save_ckpt_iters\": -1,\n",
       "  \"validation_split\": 0.8,\n",
       "  \"momentum\": 0.0,\n",
       "  \"weight_decay\": 0.0,\n",
       "  \"custom_lr_multiplier\": \"cosine\",\n",
       "  \"device\": \"cpu\",\n",
       "  \"alpha_thresh\": 0.2\n",
       "}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# procedure hyperparameters\n",
    "args = Parameters({ \n",
    "    'epochs': 25,\n",
    "    'num_workers': 0, \n",
    "    'batch_size': 100,\n",
    "    'bias': True,\n",
    "    'num_samples': 1000,\n",
    "    'clamp': True, \n",
    "    'radius': 5.0, \n",
    "    'lr': 1e-1,\n",
    "    'shuffle': False, \n",
    "    'samples': 10000,  # number of samples to generate for ground truth\n",
    "    'in_features': 2, # number of in-features to multi-log-reg\n",
    "    'k': 2, # number of classes\n",
    "    'lower': -1, # lower bound for generating ground truth weights\n",
    "    'upper': 1,  # upper bound for generating ground truth weights\n",
    "    'trials': 1,\n",
    "    'log_iters': 1,    \n",
    "    'should_save_ckpt': True,\n",
    "    'save_ckpt_iters': -1,\n",
    "    'validation_split': .8,\n",
    "    'momentum': 0.0,\n",
    "    'weight_decay': 0.0,\n",
    "    'custom_lr_multiplier': COSINE, \n",
    "    'shuffle': True,\n",
    "    'device': 'cpu',\n",
    "    'alpha_thresh': .2,\n",
    "})\n",
    "\n",
    "if ch.cuda.is_available(): \n",
    "    args.__setattr__('device', 'cuda')\n",
    "else: \n",
    "    args.__setattr__('device', 'cpu')\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truncated Multinomial Logistic Regression Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phi = DNN_Lower(ch.full(ch.Size([args.K,]), -2, dtype=ch.float32))\n",
    "# phi = DNN_Lower(Tensor([-2, -3, -2, -3, -4, -5, -6, -7, -6, -5]))\n",
    "phi = Identity()\n",
    "# phi = DNN_Logit_Ball(ch.full(ch.Size([args.K,]), -2, dtype=ch.float32), ch.full(ch.Size([args.K,]), 2, dtype=ch.float32))\n",
    "args.__setattr__('phi', phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging in: /Users/patroklos/MultinomialLogisticRegressionTruncated/8c14dea8-a2fe-40a9-b32c-948b30a9fb34\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "177bb981315b46b796d5bc98b5ce0ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcf82417d3f544fab4e53230ed148f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6119eb2500b44dc98e00a499fdb797ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "103e2034ed704620be4b85c60f07f405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5388379a5d24632bd78a13c1cb59862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beab5b0bef12426fb8691fb2bfebe4bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af89993b5097460d962513aee4f9baa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22a51368c224ac1a086e1f0690b157d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397268d7908445efb3aba763a9c6491a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "749b3983c3694f37ac24a1abaf409cfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5cd1cbff0d34c299f82e21f437eb018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c7a91b4408f4a9bb81d5fc69432ef96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0898a90ed40440aabe5d7c07fdd5fd49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9019b9c93b1048939162bc6bc39fae7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a650c3cc6cc14ac2a90209124d7a8003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a695aa3ffc934e04b6cce4f335f78c85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e98c221d932d4502908e359d959d8721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e136aa7abea49de86cb113c7c553f6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6e71fa8da7410aa0c04e102ba38b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# perform number of trials experiments\n",
    "for i in range(args.trials):\n",
    "    # generate data for exp\n",
    "    ground_truth, loaders, test_loader = gen_data()\n",
    "\n",
    "    # new classifier models at the beginning of each trial\n",
    "    trunc_multi_log_reg = nn.Linear(in_features=args.IN_FEATURES, out_features=args.K, bias=args.bias)\n",
    "\n",
    "    # truncated store\n",
    "    out_store = Store(TRUNCATED_STORE_PATH)\n",
    "    args.__setattr__('custom_criterion', trunc_ce)  # truncated ce loss\n",
    "    train_model(args, trunc_multi_log_reg, loaders, store=out_store, device=args.device)\n",
    "\n",
    "    # new classifier models at the beginning of each trial\n",
    "    standard_multi_log_reg = nn.Linear(in_features=args.IN_FEATURES, out_features=args.K, bias=args.bias)\n",
    "\n",
    "    # naive ce loss\n",
    "    out_store = Store(STANDARD_STORE_PATH)\n",
    "    args.__setattr__('custom_criterion', None) # default ce loss\n",
    "    train_model(args, standard_multi_log_reg, loaders, store=out_store, device=args.device)\n",
    "\n",
    "    # truncated multinomial logistic regression eval\n",
    "    out_store = Store(TRUNCATED_EVAL_STORE_PATH)\n",
    "    eval_model(args, trunc_multi_log_reg, test_loader, out_store)\n",
    "    \n",
    "    # Gumbel CE store path \n",
    "    out_store = Store(GUMBEL_CE_STORE_PATH)\n",
    "    args.__setattr__('custom_criterion', gumbel_ce)\n",
    "    gumbel_ce_multi_log_reg = nn.Linear(in_features=args.IN_FEATURES, out_features=args.K, bias=args.bias)\n",
    "    train_model(args, gumbel_ce_multi_log_reg, loaders, store=out_store, device=args.device)\n",
    "\n",
    "    # standard multinomial logistic regression eval - if there is a test set\n",
    "    if len(test_loader.dataset):\n",
    "        out_store = Store(STANDARD_EVAL_STORE_PATH)\n",
    "        eval_model(args, standard_multi_log_reg, test_loader, out_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity and L2 Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"cosine trunc weight: {}\".format(ch.nn.functional.cosine_similarity(trunc_multi_log_reg.weight, ground_truth.weight)))\n",
    "print(\"cosine trunc bias: {}\".format(ch.nn.functional.cosine_similarity(trunc_multi_log_reg.bias.unsqueeze(0), ground_truth.bias.unsqueeze(0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"cosine standard weight: {}\".format(ch.nn.functional.cosine_similarity(standard_multi_log_reg.weight, ground_truth.weight)))\n",
    "print(\"cosine standard bias: {}\".format(ch.nn.functional.cosine_similarity(standard_multi_log_reg.bias.unsqueeze(0), ground_truth.bias.unsqueeze(0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"cosine gumbel ce weight: {}\".format(ch.nn.functional.cosine_similarity(gumbel_ce_multi_log_reg.weight, ground_truth.weight)))\n",
    "print(\"cosine gumbel ce bias: {}\".format(ch.nn.functional.cosine_similarity(gumbel_ce_multi_log_reg.bias.unsqueeze(0), ground_truth.bias.unsqueeze(0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 2])\n",
      "Correlation between real and estimated gradient:  tensor(0.9973)\n",
      "Correlation between real and truncated gradient:  tensor(0.9980)\n",
      "Correlation between trunc bce and real ce gradient:  tensor(-9.4713e-09)\n"
     ]
    }
   ],
   "source": [
    "from torch.distributions.transforms import SigmoidTransform\n",
    "from torch.distributions.transformed_distribution import TransformedDistribution\n",
    "\n",
    "gumbel = Gumbel(0, 1)\n",
    "\n",
    "# logistic distribution\n",
    "base_distribution = Uniform(0, 1)\n",
    "transforms_ = [SigmoidTransform().inv]\n",
    "logistic = TransformedDistribution(base_distribution, transforms_)\n",
    "\n",
    "phi = Identity()\n",
    "\n",
    "\n",
    "m = ch.nn.Linear(10, 2)\n",
    "x = ch.rand(1000, 10)\n",
    "w = ch.randn(10, 2)\n",
    "y = (x @ w + gumbel.sample([1000, 2])).argmax(1)\n",
    "\n",
    "m_ = ch.nn.Linear(10, 1)\n",
    "x_ = ch.rand(1000, 10)\n",
    "w_ = ch.randn(10, 1)\n",
    "y_ = (x_ @ w_ + logistic.sample([1000, 1])).argmax(1).float()\n",
    "\n",
    "out = m(x)\n",
    "loss_ = GumbelCE.apply(out, y)\n",
    "g, = ch.autograd.grad(loss_, [out])\n",
    "\n",
    "gt_loss = nn.CrossEntropyLoss()(out, y)\n",
    "gt_g, = ch.autograd.grad(gt_loss, [out])\n",
    "\n",
    "trunc_loss = trunc_ce(out, y)\n",
    "gt_trunc, = ch.autograd.grad(trunc_loss, [out])\n",
    "\n",
    "out_ = m_(x_)\n",
    "trunc_bce_loss = trunc_bce(out_, y_.unsqueeze(1))\n",
    "g_bce, = ch.autograd.grad(trunc_bce_loss, [out_])\n",
    "\n",
    "print('Correlation between real and estimated gradient: ',\n",
    "        (gt_g * g).sum() / (gt_g.norm() * g.norm()))\n",
    "\n",
    "print('Correlation between real and truncated gradient: ',\n",
    "        (gt_g * gt_trunc).sum() / (gt_g.norm() * gt_trunc.norm()))\n",
    "\n",
    "print('Correlation between trunc bce and real ce gradient: ', \n",
    "     (g_bce * gt_g).sum() / (g_bce.norm() * gt_g.norm()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0004],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0004],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0004],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0004],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0004],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0004],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0004],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0004],\n",
       "        [0.0005],\n",
       "        [0.0004],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0004],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0004],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0004],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0004],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0004],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0004],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0004],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0004],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0004],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0004],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0004],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0004],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0004],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0004],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0004],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0004],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0004],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005],\n",
       "        [0.0005],\n",
       "        [0.0004],\n",
       "        [0.0005],\n",
       "        [0.0005]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_bce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0004,  0.0004],\n",
       "        [-0.0005,  0.0005],\n",
       "        [-0.0004,  0.0004],\n",
       "        ...,\n",
       "        [-0.0004,  0.0004],\n",
       "        [-0.0004,  0.0004],\n",
       "        [-0.0005,  0.0005]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
