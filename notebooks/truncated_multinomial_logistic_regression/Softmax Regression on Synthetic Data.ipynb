{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### MAY NEED TO REMOVE --> THESE IMPORTS ARE JUST FOR MY DEPENDENCIES ON MY LOCAL DEVICE\n",
    "import sys \n",
    "sys.path.append('../..')\n",
    "######\n",
    "\n",
    "from cox.utils import Parameters\n",
    "from cox.store import Store\n",
    "from cox.readers import CollectionReader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch as ch\n",
    "from torch import Tensor\n",
    "from torch import sigmoid as sig\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD, lr_scheduler\n",
    "from torch.distributions import Gumbel, Uniform\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.distributions.transforms import SigmoidTransform\n",
    "from torch.distributions.transformed_distribution import TransformedDistribution\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from tqdm.autonotebook import tqdm as tqdm\n",
    "from abc import ABC\n",
    "import IPython\n",
    "import os\n",
    "import dill\n",
    "# set environment variable so that stores can create output files\n",
    "os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'\n",
    "\n",
    "ch.set_default_tensor_type(ch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path\n",
    "SOFTMAX_REGRESSION_STORE_PATH = '/home/gridsan/stefanou/SoftmaxRegressionScale2/'\n",
    "\n",
    "# tables\n",
    "STANDARD_STORE_TABLE = 'standard_table'\n",
    "TRUNCATED_STORE_TABLE = 'truncated_table'\n",
    "GUMBEL_CE_TABLE = 'gumbel_table'\n",
    "\n",
    "STANDARD_EVAL_TABLE = 'standard_eval_table'\n",
    "TRUNCATED_EVAL_TABLE = 'truncated_eval_table'\n",
    "GUMBEL_EVAL_TABLE = 'gumbel_eval_table'\n",
    "\n",
    "LOGS_SCHEMA = {\n",
    "    'epoch': int,\n",
    "    'val_prec1': float,\n",
    "    'val_loss': float,\n",
    "    'train_prec1': float,\n",
    "    'train_loss': float,\n",
    "    'time': float\n",
    "}\n",
    "\n",
    "EVAL_LOGS_SCHEMA = {\n",
    "    'test_prec1': float,\n",
    "    'test_loss': float,\n",
    "    'time': float\n",
    "}\n",
    "\n",
    "GROUND_TRUTH_SCHEMA = { \n",
    "    'cos_sim': float, \n",
    "    'l2': float,\n",
    "    'epoch': int,\n",
    "}\n",
    "\n",
    "# scheduler constants\n",
    "CYCLIC='cyclic'\n",
    "COSINE='cosine'\n",
    "LINEAR='linear'\n",
    "\n",
    "LOGS_TABLE = 'logs'\n",
    "EVAL_LOGS_TABLE = 'eval'\n",
    "GROUND_TRUTH_TABLE ='ground_truth'\n",
    "\n",
    "CKPT_NAME = 'checkpoint.pt'\n",
    "BEST_APPEND = '.best'\n",
    "CKPT_NAME_LATEST = CKPT_NAME + '.latest'\n",
    "CKPT_NAME_BEST = CKPT_NAME + BEST_APPEND"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation Procedure Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer_and_schedule(args, model, params):\n",
    "    param_list = model.parameters() if params is None else params\n",
    "\n",
    "    optimizer = SGD(param_list, args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "\n",
    "    # Make schedule\n",
    "    schedule = None\n",
    "    if args.custom_lr_multiplier == CYCLIC:\n",
    "        eps = args.epochs\n",
    "        lr_func = lambda t: np.interp([t], [0, eps*4//15, eps], [0, 1, 0])[0]\n",
    "        schedule = lr_scheduler.LambdaLR(optimizer, lr_func)\n",
    "    elif args.custom_lr_multiplier == COSINE:\n",
    "        eps = args.epochs\n",
    "        schedule = lr_scheduler.CosineAnnealingLR(optimizer, eps)\n",
    "    elif args.custom_lr_multiplier:\n",
    "        cs = args.custom_lr_multiplier\n",
    "        periods = eval(cs) if type(cs) is str else cs\n",
    "        if args.lr_interpolation == LINEAR:\n",
    "            lr_func = lambda t: np.interp([t], *zip(*periods))[0]\n",
    "        else:\n",
    "            def lr_func(ep):\n",
    "                for (milestone, lr) in reversed(periods):\n",
    "                    if ep >= milestone: return lr\n",
    "                return 1.0\n",
    "        schedule = lr_scheduler.LambdaLR(optimizer, lr_func)\n",
    "    elif args.step_lr:\n",
    "        schedule = lr_scheduler.StepLR(optimizer, step_size=args.step_lr, gamma=args.step_lr_gamma)\n",
    "        \n",
    "    return optimizer, schedule\n",
    "\n",
    "\n",
    "def eval_model(args, model, loader, store, table):\n",
    "    \"\"\"\n",
    "    Evaluate a model for standard (and optionally adversarial) accuracy.\n",
    "    Args:\n",
    "        args (object) : A list of arguments---should be a python object\n",
    "            implementing ``getattr()`` and ``setattr()``.\n",
    "        model (AttackerModel) : model to evaluate\n",
    "        loader (iterable) : a dataloader serving `(input, label)` batches from\n",
    "            the validation set\n",
    "        store (cox.Store) : store for saving results in (via tensorboardX)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    table = EVAL_LOGS_TABLE if table is None else table\n",
    "\n",
    "    if store is not None:\n",
    "        store.add_table(table, EVAL_LOGS_SCHEMA)\n",
    "    writer = store.tensorboard if store else None\n",
    "\n",
    "    # put model on device\n",
    "    model.to(args.device)\n",
    "\n",
    "    assert not hasattr(model, \"module\"), \"model is already in DataParallel.\"\n",
    "    if next(model.parameters()).is_cuda and False:\n",
    "        model = ch.nn.DataParallel(model)\n",
    "\n",
    "    test_prec1, test_loss, score = model_loop(args, 'val', loader,\n",
    "                                        model, None, 0, writer, args.device)\n",
    "\n",
    "    log_info = {\n",
    "        'test_prec1': test_prec1,\n",
    "        'test_loss': test_loss,\n",
    "        'time': time.time() - start_time\n",
    "    }\n",
    "\n",
    "    # Log info into the logs table\n",
    "    if store: store[table].append_row(log_info)\n",
    "\n",
    "    return log_info\n",
    "\n",
    "\n",
    "def train_model(args, model, loaders, *, ground_truth=None, device=\"cpu\", dp_device_ids=None,\n",
    "                store=None, table=None, update_params=None, disable_no_grad=False):\n",
    "\n",
    "    table = LOGS_TABLE if table is None else table\n",
    "    \n",
    "    if store is not None:\n",
    "        store.add_table(table, LOGS_SCHEMA)\n",
    "        # ground-truth comparison table\n",
    "        if ground_truth is not None: \n",
    "            store.add_table(GROUND_TRUTH_TABLE, GROUND_TRUTH_SCHEMA)\n",
    "    writer = store.tensorboard if store else None\n",
    "\n",
    "    # data loaders\n",
    "    train_loader, val_loader = loaders\n",
    "    optimizer, schedule = make_optimizer_and_schedule(args, model, update_params)\n",
    "\n",
    "    # put the model into parallel mode\n",
    "    assert not has_attr(model, \"module\"), \"model is already in DataParallel.\"\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    best_prec1, start_epoch = (0, 0)\n",
    "\n",
    "    # keep track of the start time\n",
    "    start_time = time.time()\n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "        train_prec1, train_loss, score = model_loop(args, 'train', train_loader, model, optimizer, epoch+1, writer, device=device)\n",
    "\n",
    "        # check score tolerance\n",
    "        if args.score and ch.all(ch.where(ch.abs(score) < args.tol, ch.ones(1), ch.zeros(1)).bool()):\n",
    "            break\n",
    "\n",
    "        last_epoch = (epoch == (args.epochs - 1))\n",
    "\n",
    "        # if neural network passed through framework, use log performance\n",
    "        if args.should_save_ckpt:\n",
    "            save_its = args.save_ckpt_iters\n",
    "            should_save_ckpt = (epoch % save_its == 0) and (save_its > 0)\n",
    "            should_log = (epoch % args.log_iters == 0)\n",
    "\n",
    "            if should_log or last_epoch or should_save_ckpt:\n",
    "                # log + get best\n",
    "                ctx = ch.enable_grad() if disable_no_grad else ch.no_grad()\n",
    "                with ctx:\n",
    "                    val_prec1, val_loss, score = model_loop(args, 'val', val_loader, model,\n",
    "                            None, epoch + 1, writer, device=device)\n",
    "\n",
    "                # remember best prec@1 and save checkpoint\n",
    "                is_best = val_prec1 > best_prec1\n",
    "                best_prec1 = max(val_prec1, best_prec1)\n",
    "\n",
    "                # log every checkpoint\n",
    "                log_info = {\n",
    "                    'epoch': epoch + 1,\n",
    "                    'val_prec1': val_prec1,\n",
    "                    'val_loss': val_loss,\n",
    "                    'train_prec1': train_prec1,\n",
    "                    'train_loss': train_loss,\n",
    "                    'time': time.time() - start_time\n",
    "                }\n",
    "\n",
    "                # Log info into the logs table\n",
    "                if store: store[table].append_row(log_info)\n",
    "        \n",
    "        # update lr\n",
    "        if schedule: schedule.step()\n",
    "\n",
    "        tqdm._instances.clear()\n",
    "        \n",
    "        if ground_truth is not None: \n",
    "            # ground_truth and current model parameters\n",
    "            gt_params = ch.cat([ground_truth.weight.flatten(), ground_truth.bias]).unsqueeze(1)\n",
    "            trunc_params = ch.cat([model.weight.flatten(), model.bias]).unsqueeze(1)\n",
    "            # cosine similarity and l2 distance\n",
    "            cos_sim = float(ch.nn.functional.cosine_similarity(gt_params, trunc_params, dim=0))\n",
    "            l2_dist = float(ch.nn.MSELoss()(gt_params, trunc_params))\n",
    "            \n",
    "            ground_truth_info = { \n",
    "                'epoch': epoch + 1, \n",
    "                'cos_sim': cos_sim, \n",
    "                'l2': l2_dist,\n",
    "            }\n",
    "            \n",
    "            store[GROUND_TRUTH_TABLE].append_row(ground_truth_info)\n",
    "    return model\n",
    "            \n",
    "            \n",
    "def model_loop(args, loop_type, loader, model, optimizer, epoch, writer, device):\n",
    "    # check loop type \n",
    "    if not loop_type in ['train', 'val']: \n",
    "        err_msg = \"loop type must be in {0} must be 'train' or 'val\".format(loop_type)\n",
    "        raise ValueError(err_msg)\n",
    "    is_train = (loop_type == 'train')\n",
    "    \n",
    "    loop_msg = 'Train' if is_train else 'Val'\n",
    "\n",
    "    # algorithm metrics\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    score = AverageMeter()\n",
    "    \n",
    "    # check for custom criterion\n",
    "    has_custom_criterion = has_attr(args, 'custom_criterion')\n",
    "    criterion = args.custom_criterion if has_custom_criterion else ch.nn.CrossEntropyLoss()\n",
    "\n",
    "    iterator = tqdm(enumerate(loader), total=len(loader), leave=False)\n",
    "    for i, batch in iterator:\n",
    "        inp, target, output = None, None, None\n",
    "        loss = 0.0\n",
    "\n",
    "        inp, target = batch\n",
    "        inp, target = inp.to(device), target.to(device)\n",
    "        output = model(inp)\n",
    "        # attacker model returns both output anf final input\n",
    "        if isinstance(output, tuple):\n",
    "            output, final_inp = output\n",
    "        # lambda parameter used for regression with unknown noise variance\n",
    "        try:\n",
    "            loss = criterion(output, target, model.lambda_)\n",
    "        except Exception as e:\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "        # regularizer option \n",
    "        reg_term = 0.0\n",
    "        if has_attr(args, \"regularizer\") and isinstance(model, ch.nn.Module):\n",
    "            reg_term = args.regularizer(model, inp, target)\n",
    "        loss = loss + reg_term\n",
    "        \n",
    "        # perform backprop and take optimizer step\n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if len(loss.size()) > 0: loss = loss.mean()\n",
    "\n",
    "        model_logits = output[0] if isinstance(output, tuple) else output\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        top1_acc = float('nan')\n",
    "        top5_acc = float('nan')\n",
    "        try:\n",
    "            losses.update(loss.item(), inp.size(0))\n",
    "            # calculate score\n",
    "            if args.bias:\n",
    "                score.update(ch.cat([model.weight.grad.T, model.bias.grad.unsqueeze(0)]).flatten(), inp.size(0))\n",
    "            else:\n",
    "                score.update(ch.cat([model.weight.grad.T]).flatten(), inp.size(0))\n",
    "\n",
    "            if model_logits is not None:\n",
    "                # accuracy\n",
    "                maxk = min(5, model_logits.shape[-1])\n",
    "                if has_attr(args, \"custom_accuracy\"):\n",
    "                    prec1, prec5 = args.custom_accuracy(model_logits, target)\n",
    "                else:\n",
    "                    prec1, prec5 = accuracy(model_logits, target, topk=(1, maxk))\n",
    "                    prec1, prec5 = prec1[0], prec5[0]\n",
    "\n",
    "                top1.update(prec1, inp.size(0))\n",
    "                top5.update(prec5, inp.size(0))\n",
    "                top1_acc = top1.avg\n",
    "                top5_acc = top5.avg\n",
    "\n",
    "                # ITERATOR\n",
    "                desc = ('Epoch:{0} | Score: {score} \\n | Loss {loss.avg:.4f} | '\n",
    "                        '{1}1 {top1_acc:.3f} | {1}5 {top5_acc:.3f} | '\n",
    "                        'Reg term: {reg} ||'.format(epoch, loop_msg, score=[round(x, 4) for x in score.avg.tolist()],\n",
    "                                                    loss=losses, top1_acc=top1_acc, top5_acc=top5_acc, reg=reg_term))\n",
    "        except Exception as e:\n",
    "            warnings.warn('Failed to calculate the accuracy.')\n",
    "            # ITERATOR\n",
    "            desc = ('Epoch:{0} |  Score: {score} \\n | Loss {loss.avg:.4f} ||'.format(\n",
    "                epoch, loop_msg, score=[round(x, 4) for x in score.avg.tolist()], loss=losses))\n",
    "        \n",
    "        iterator.set_description(desc)\n",
    "    \n",
    "        # USER-DEFINED HOOK\n",
    "        if has_attr(args, 'iteration_hook'):\n",
    "            args.iteration_hook(model, i, loop_type, inp, target)\n",
    "\n",
    "    if writer is not None:\n",
    "        descs = ['loss', 'top1', 'top5']\n",
    "        vals = [losses, top1, top5]\n",
    "        for d, v in zip(descs, vals):\n",
    "            writer.add_scalar('_'.join([loop_type, d]), v.avg,\n",
    "                              epoch)\n",
    "\n",
    "    # ACCURACY, LOSS, AND SCORE\n",
    "    return top1.avg, losses.avg, score.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# membership oracles\n",
    "class oracle(ABC):\n",
    "    \"\"\"\n",
    "    Oracle for data sets.\n",
    "    \"\"\"\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Membership oracle.\n",
    "        Args: \n",
    "            x: samples to check membership\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "class DNN_Lower(oracle): \n",
    "    \"\"\"\n",
    "    Lower bound truncation on the DNN logits.\n",
    "    \"\"\"\n",
    "    def __init__(self, lower): \n",
    "        self.lower = lower\n",
    "        \n",
    "    def __call__(self, x): \n",
    "        return (x > self.lower).float()\n",
    "    \n",
    "class DNN_Logit_Ball(oracle): \n",
    "    \"\"\"\n",
    "    Truncation ball placed on DNN logits.\n",
    "    INTUITION: logits that are neither very large nor very small insinuate\n",
    "    that the classification is not \n",
    "    \"\"\"\n",
    "    def __init__(self, lower, upper): \n",
    "        self.lower = lower \n",
    "        self.upper = upper\n",
    "        \n",
    "    def __call__(self, x): \n",
    "        return ((x < self.lower) | (x > self.upper)).float()\n",
    "        \n",
    "\n",
    "class Identity(oracle): \n",
    "    \"\"\"\n",
    "    Identity membership oracle for DNNs. All logits are accepted within the truncation set.\n",
    "    \"\"\"\n",
    "    def __call__(self, x): \n",
    "        return ch.ones(x.size())\n",
    "    \n",
    "def gen_data(): \n",
    "    \"\"\"\n",
    "    Generate dataset for truncated multinomial logistic \n",
    "    regression model. Returns ground_truth and train, validation, and test loaders.\n",
    "    \"\"\"\n",
    "    # distributions\n",
    "    gumbel = Gumbel(0, 2)\n",
    "    U = Uniform(args.lower, args.upper) # distribution to generate ground-truth parameters\n",
    "    U_ = Uniform(-5, 5) # distribution to generate samples\n",
    "    \n",
    "    # no grad required for dataset\n",
    "    with ch.no_grad():\n",
    "        # generate synthetic data until survival probability of more than 40%\n",
    "        alpha = None\n",
    "        while alpha is None or alpha < args.ALPHA_THRESH:\n",
    "            # generate ground-truth from uniform distribution\n",
    "            ground_truth = nn.Linear(in_features=args.IN_FEATURES, out_features=args.K, bias=args.bias)\n",
    "            ground_truth.weight = nn.Parameter(U.sample(ch.Size([args.K, args.IN_FEATURES])))\n",
    "            if ground_truth.bias is not None: \n",
    "                ground_truth.bias = nn.Parameter(U.sample(ch.Size([args.K,])))\n",
    "            # independent variable \n",
    "            X = U_.sample(ch.Size([args.samples, args.IN_FEATURES]))\n",
    "            # determine base model logits \n",
    "            z = ground_truth(X)\n",
    "            # add noise to the logits\n",
    "            noised = z + gumbel.sample(z.size())\n",
    "            # apply softmax to unnormalized likelihoods\n",
    "            y = ch.argmax(noised, dim=1)\n",
    "\n",
    "            # TRUNCATE\n",
    "            trunc = args.phi(z)\n",
    "            indices = ch.all(trunc.bool(), dim=1).float().nonzero(as_tuple=False).flatten()\n",
    "            x_trunc, y_trunc = X[indices].cpu(), y[indices].cpu()\n",
    "            alpha = x_trunc.size(0) / X.size(0)\n",
    "\n",
    "            # all synthetic data \n",
    "            ds = TensorDataset(x_trunc, y_trunc)\n",
    "            # split ds into training and validation data sets - 80% training, 20% validation\n",
    "            train_length = int(len(ds)*.8)\n",
    "            val_length = len(ds) - train_length\n",
    "            train_ds, val_ds = ch.utils.data.random_split(ds, [train_length, val_length])\n",
    "            # train and validation loaders\n",
    "            train_loader = DataLoader(train_ds, num_workers=args.num_workers, batch_size=args.batch_size)\n",
    "            val_loader = DataLoader(val_ds, num_workers=args.num_workers, batch_size=args.batch_size)\n",
    "\n",
    "            # test dataset\n",
    "            x_test = X[~indices].cpu()\n",
    "            y_test = y[~indices].cpu()\n",
    "            test_ds = TensorDataset(x_test, y_test)\n",
    "            test_loader = DataLoader(test_ds, num_workers=args.num_workers, batch_size=args.batch_size)\n",
    "            \n",
    "    return ground_truth, (train_loader, val_loader), test_loader\n",
    "\n",
    "def plot():\n",
    "    reader = CollectionReader(SOFTMAX_REGRESSION_STORE_PATH)\n",
    "\n",
    "    # TRUNCATED CE LOSS DATA\n",
    "    trunc_logs = reader.df(TRUNCATED_STORE_TABLE)\n",
    "    trunc_comp = reader.df(GROUND_TRUTH_TABLE)\n",
    "\n",
    "    # STANDARD CE LOSS DATA\n",
    "    standard_logs = reader.df(STANDARD_STORE_TABLE)\n",
    "    standard_comp = reader.df(GROUND_TRUTH_TABLE)\n",
    "\n",
    "    sns.lineplot(data=trunc_logs, x='epoch', y='train_loss', label='Train Loss')\n",
    "    sns.lineplot(data=standard_logs, x='epoch', y='train_loss', label='Naive Train Loss')\n",
    "    sns.lineplot(data=trunc_logs, x='epoch', y='val_loss', color='red', label='Trunc Val Loss')\n",
    "    ax = sns.lineplot(data=standard_logs, x='epoch', y='val_loss', color='red', label='Naive Val Loss')\n",
    "    ax.set(xlabel='Epoch', ylabel='CE Loss')\n",
    "    plt.show()\n",
    "\n",
    "    sns.lineplot(data=trunc_logs, x='epoch', y='train_prec1', label='Trunc Train Acc')\n",
    "    sns.lineplot(data=standard_logs, x='epoch', y='train_prec1', label='Naive Train Acc')\n",
    "    sns.lineplot(data=trunc_logs, x='epoch', y='val_prec1', label='Trunc Val Acc')\n",
    "    ax = sns.lineplot(data=standard_logs, x='epoch', y='val_prec1', label='Naive Val Acc')\n",
    "    ax.set(xlabel='Epoch', ylabel='Accuracy')\n",
    "    plt.show()\n",
    "    \n",
    "    sns.lineplot(data=standard_comp, x='epoch', y='cos_sim', label='Naive Cosine Similarity')\n",
    "    ax = sns.lineplot(data=trunc_comp, x='epoch', y='cos_sim', label='Truncated Cosine Similarity')\n",
    "    ax.set(xlabel='Epoch', ylabel='Cosine Similarity')\n",
    "    plt.show()\n",
    "    \n",
    "    sns.lineplot(data=standard_comp, x='epoch', y='l2', label='Naive L2')\n",
    "    ax = sns.lineplot(data=trunc_comp, x='epoch', y='l2', label='Truncated L2')\n",
    "    ax.set(xlabel='Epoch', ylabel='L2 Distance')\n",
    "    plt.show()\n",
    "    \n",
    "    try: \n",
    "        # STANDARD TEST SET RESULTS \n",
    "        standard_test_results = reader.df(STANDARD_EVAL_TABLE)\n",
    "        \n",
    "        # TRUNCATED TEST SET RESULTS \n",
    "        trunc_test_results = reader.df(TRUNCATED_EVAL_TABLE)\n",
    "\n",
    "        print(\"Standard Test Accuracy: {}\".format(standard_test_results['test_prec1']))\n",
    "        print(\"Truncated Test Accuracy: {}\".format(trunc_test_results['test_prec1']))\n",
    "    except: \n",
    "        print(\"No Test Results to Report\")\n",
    "        \n",
    "    reader.close()\n",
    "        \n",
    "        \n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def has_attr(obj, k):\n",
    "    \"\"\"Checks both that obj.k exists and is not equal to None\"\"\"\n",
    "    try:\n",
    "        return (getattr(obj, k) is not None)\n",
    "    except KeyError as e:\n",
    "        return False\n",
    "    except AttributeError as e:\n",
    "        return False\n",
    "    \n",
    "def accuracy(output, target, topk=(1,), exact=False):\n",
    "    \"\"\"\n",
    "        Computes the top-k accuracy for the specified values of k\n",
    "\n",
    "        Args:\n",
    "            output (ch.Tensor) : model output (N, classes) or (N, attributes) \n",
    "                for sigmoid/multitask binary classification\n",
    "            target (ch.Tensor) : correct labels (N,) [multiclass] or (N,\n",
    "                attributes) [multitask binary]\n",
    "            topk (tuple) : for each item \"k\" in this tuple, this method\n",
    "                will return the top-k accuracy\n",
    "            exact (bool) : whether to return aggregate statistics (if\n",
    "                False) or per-example correctness (if True)\n",
    "\n",
    "        Returns:\n",
    "            A list of top-k accuracies.\n",
    "    \"\"\"\n",
    "    with ch.no_grad():\n",
    "        # Binary Classification\n",
    "        if len(target.shape) > 1:\n",
    "            assert output.shape == target.shape, \\\n",
    "                \"Detected binary classification but output shape != target shape\"\n",
    "            return [ch.round(ch.sigmoid(output)).eq(ch.round(target)).float().mean()], [-1.0] \n",
    "\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        res_exact = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float()\n",
    "            ck_sum = correct_k.sum(0, keepdim=True)\n",
    "            res.append(ck_sum.mul_(100.0 / batch_size))\n",
    "            res_exact.append(correct_k)\n",
    "\n",
    "        if not exact:\n",
    "            return res\n",
    "        else:\n",
    "            return res_exact\n",
    "        \n",
    "def ckpt_at_epoch(num):\n",
    "    return '%s_%s' % (num, CKPT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TruncatedBCE(ch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, pred, targ):\n",
    "        ctx.save_for_backward(pred, targ)\n",
    "        loss = ch.nn.BCEWithLogitsLoss()\n",
    "        return loss(pred, targ)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        pred, targ = ctx.saved_tensors\n",
    "\n",
    "        # logistic distribution\n",
    "        base_distribution = Uniform(0, 1)\n",
    "        transforms_ = [SigmoidTransform().inv]\n",
    "        logistic = TransformedDistribution(base_distribution, transforms_)\n",
    "\n",
    "        stacked = pred[None, ...].repeat(args.num_samples, 1, 1)\n",
    "        # add noise\n",
    "        noised = stacked + logistic.sample(stacked.size())\n",
    "        # filter\n",
    "        filtered = ch.stack([args.phi(batch) for batch in noised]).float()\n",
    "        out = (noised * filtered).sum(dim=0) / (filtered.sum(dim=0) + 1e-5)\n",
    "        grad = ch.where(ch.abs(out) > 1e-5, sig(out), targ) - targ\n",
    "        return grad / pred.size(0), -grad / pred.size(0)\n",
    "\n",
    "class GumbelCE(ch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, pred, targ):\n",
    "        ctx.save_for_backward(pred, targ)\n",
    "        ce_loss = ch.nn.CrossEntropyLoss()\n",
    "        return ce_loss(pred, targ)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        pred, targ = ctx.saved_tensors\n",
    "        # gumbel distribution\n",
    "        gumbel = Gumbel(0, 1)\n",
    "        # make num_samples copies of pred logits\n",
    "        stacked = pred[None, ...].repeat(args.num_samples, 1, 1)        \n",
    "        # add gumbel noise to logits\n",
    "        rand_noise = gumbel.sample(stacked.size())\n",
    "        noised = stacked + rand_noise \n",
    "        noised_labs = noised.argmax(-1)\n",
    "        # remove the logits from the trials, where the kth logit is not the largest value\n",
    "        good_mask = noised_labs.eq(targ)[..., None]\n",
    "        inner_exp = 1 - ch.exp(-rand_noise)\n",
    "        avg = (inner_exp * good_mask).sum(0) / (good_mask.sum(0) + 1e-5) / pred.size(0)\n",
    "        return -avg , None\n",
    "    \n",
    "class TruncatedGumbelCE(ch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, pred, targ):\n",
    "        ctx.save_for_backward(pred, targ)\n",
    "        ce_loss = ch.nn.CrossEntropyLoss()\n",
    "        return ce_loss(pred, targ)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        pred, targ = ctx.saved_tensors\n",
    "        # initialize gumbel distribution\n",
    "        gumbel = Gumbel(0, 1)\n",
    "        # make num_samples copies of pred logits\n",
    "        stacked = pred[None, ...].repeat(args.num_samples, 1, 1)   \n",
    "        # add gumbel noise to logits\n",
    "        rand_noise = gumbel.sample(stacked.size())\n",
    "        noised = stacked + rand_noise \n",
    "        # truncate - if one of the noisy logits does not fall within the truncation set, remove it\n",
    "        filtered = ch.all(args.phi(noised).bool(), dim=2).float().unsqueeze(2)\n",
    "        noised_labs = noised.argmax(-1)\n",
    "        # mask takes care of invalid logits and truncation set\n",
    "        mask = noised_labs.eq(targ)[..., None] * filtered\n",
    "        inner_exp = 1 - ch.exp(-rand_noise)\n",
    "        \n",
    "        avg = (((inner_exp * mask).sum(0) / (mask.sum(0) + 1e-5)) - ((inner_exp * filtered).sum(0) / (filtered.sum(0) + 1e-5))) \n",
    "        return -avg / pred.size(0), None, None\n",
    "\n",
    "# gradients\n",
    "trunc_bce = TruncatedBCE.apply\n",
    "gumbel_ce = GumbelCE.apply\n",
    "trunc_ce = TruncatedGumbelCE.apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Experiment HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"epochs\": 25,\n",
       "  \"num_workers\": 8,\n",
       "  \"batch_size\": 100,\n",
       "  \"bias\": true,\n",
       "  \"num_samples\": 1000,\n",
       "  \"clamp\": true,\n",
       "  \"radius\": 5.0,\n",
       "  \"lr\": 0.1,\n",
       "  \"shuffle\": true,\n",
       "  \"samples\": 10000,\n",
       "  \"in_features\": 2,\n",
       "  \"k\": 2,\n",
       "  \"lower\": -1,\n",
       "  \"upper\": 1,\n",
       "  \"trials\": 10,\n",
       "  \"log_iters\": 1,\n",
       "  \"should_save_ckpt\": true,\n",
       "  \"save_ckpt_iters\": -1,\n",
       "  \"validation_split\": 0.8,\n",
       "  \"momentum\": 0.0,\n",
       "  \"weight_decay\": 0.0,\n",
       "  \"device\": \"cuda\",\n",
       "  \"alpha_thresh\": 0.2,\n",
       "  \"step_lr\": true,\n",
       "  \"step_lr_gamma\": 0.9\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# procedure hyperparameters\n",
    "args = Parameters({ \n",
    "    'epochs': 25,\n",
    "    'num_workers': 8, \n",
    "    'batch_size': 100,\n",
    "    'bias': True,\n",
    "    'num_samples': 1000,\n",
    "    'clamp': True, \n",
    "    'radius': 5.0, \n",
    "    'lr': 1e-1,\n",
    "    'shuffle': False, \n",
    "    'samples': 10000,  # number of samples to generate for ground truth\n",
    "    'in_features': 2, # number of in-features to multi-log-reg\n",
    "    'k': 2, # number of classes\n",
    "    'lower': -1, # lower bound for generating ground truth weights\n",
    "    'upper': 1,  # upper bound for generating ground truth weights\n",
    "    'trials': 10,\n",
    "    'log_iters': 1,    \n",
    "    'should_save_ckpt': True,\n",
    "    'save_ckpt_iters': -1,\n",
    "    'validation_split': .8,\n",
    "    'momentum': 0.0,\n",
    "    'weight_decay': 0.0,\n",
    "#     'custom_lr_multiplier': CYCLIC, \n",
    "    'shuffle': True,\n",
    "    'device': 'cpu',\n",
    "    'alpha_thresh': .2,\n",
    "    'step_lr': True, \n",
    "    'step_lr_gamma': .9,\n",
    "})\n",
    "\n",
    "if ch.cuda.is_available(): \n",
    "    args.__setattr__('device', 'cuda')\n",
    "else: \n",
    "    args.__setattr__('device', 'cpu')\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truncated Multinomial Logistic Regression Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phi = DNN_Lower(ch.full(ch.Size([args.K,]), -2, dtype=ch.float32))\n",
    "# phi = DNN_Lower(Tensor([-2, -3, -2, -3, -4, -5, -6, -7, -6, -5]))\n",
    "# phi = Identity()\n",
    "phi = DNN_Logit_Ball(ch.full(ch.Size([args.K,]), -2, dtype=ch.float32), ch.full(ch.Size([args.K,]), 2, dtype=ch.float32))\n",
    "args.__setattr__('phi', phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# perform number of trials experiments\n",
    "for i in range(args.trials):\n",
    "    # generate data for exp\n",
    "    ground_truth, loaders, test_loader = gen_data()\n",
    "\n",
    "    # new classifier models at the beginning of each trial\n",
    "    trunc_multi_log_reg = nn.Linear(in_features=args.IN_FEATURES, out_features=args.K, bias=args.bias)\n",
    "\n",
    "    # truncated store\n",
    "    out_store = Store(SOFTMAX_REGRESSION_STORE_PATH)\n",
    "    args.__setattr__('custom_criterion', trunc_ce)  # truncated ce loss\n",
    "    train_model(args, trunc_multi_log_reg, loaders, ground_truth=ground_truth, store=out_store, \n",
    "                table=TRUNCATED_STORE_TABLE, device=args.device)\n",
    "\n",
    "    # new classifier models at the beginning of each trial\n",
    "    standard_multi_log_reg = nn.Linear(in_features=args.IN_FEATURES, out_features=args.K, bias=args.bias)\n",
    "\n",
    "    # naive ce loss\n",
    "    args.__setattr__('custom_criterion', ch.nn.CrossEntropyLoss()) # default ce loss\n",
    "    train_model(args, standard_multi_log_reg, loaders, ground_truth=ground_truth, store=out_store, \n",
    "                table=STANDARD_STORE_TABLE, device=args.device)\n",
    "    \n",
    "    # Gumbel CE store path \n",
    "    args.__setattr__('custom_criterion', gumbel_ce)\n",
    "    gumbel_ce_multi_log_reg = nn.Linear(in_features=args.IN_FEATURES, out_features=args.K, bias=args.bias)\n",
    "    train_model(args, gumbel_ce_multi_log_reg, loaders, ground_truth=ground_truth, store=out_store, table=GUMBEL_CE_TABLE,device=args.device)\n",
    "\n",
    "    # standard multinomial logistic regression eval - if there is a test set\n",
    "    if len(test_loader.dataset) > 0:\n",
    "        # truncated multinomial logistic regression eval\n",
    "        eval_model(args, trunc_multi_log_reg, test_loader, out_store, table=TRUNCATED_EVAL_TABLE)\n",
    "        # clear output\n",
    "        \n",
    "        # standard multinomial logistic regression eval\n",
    "        eval_model(args, standard_multi_log_reg, test_loader, out_store, table=STANDARD_EVAL_TABLE)\n",
    "\n",
    "    # close store after each experiment\n",
    "    out_store.close()\n",
    "    IPython.display.clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot results\n",
    "plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between real and estimated gradient:  tensor(0.5030)\n"
     ]
    }
   ],
   "source": [
    "gumbel = Gumbel(0, 1)\n",
    "\n",
    "loss = ch.nn.CrossEntropyLoss()\n",
    "\n",
    "m = ch.nn.Linear(10, 2)\n",
    "x = ch.rand(1000, 10)\n",
    "w = ch.randn(10, 2)\n",
    "\n",
    "z = x @ w + gumbel.sample([1000, 2])\n",
    "\n",
    "y = z.argmax(1)\n",
    "\n",
    "indices = ch.all(phi(z).bool(), dim=1).float().nonzero(as_tuple=False).flatten()\n",
    "\n",
    "x_trunc, y_trunc = x[indices], y[indices]\n",
    "\n",
    "\n",
    "out = m(x_trunc)\n",
    "loss_ = TruncatedGumbelCE.apply(out, y_trunc)\n",
    "g, = ch.autograd.grad(loss_, [out])\n",
    "\n",
    "gt_loss = loss(out, y_trunc)\n",
    "gt_g, = ch.autograd.grad(gt_loss, [out])\n",
    "\n",
    "print('Correlation between real and estimated gradient: ',\n",
    "        (gt_g * g).sum() / (gt_g.norm() * g.norm()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0033,  0.0033],\n",
       "        [-0.0036,  0.0036],\n",
       "        [-0.0034,  0.0034],\n",
       "        [-0.0032,  0.0032],\n",
       "        [ 0.0028, -0.0028],\n",
       "        [-0.0036,  0.0036],\n",
       "        [-0.0033,  0.0033],\n",
       "        [-0.0029,  0.0029],\n",
       "        [-0.0035,  0.0035],\n",
       "        [-0.0039,  0.0039],\n",
       "        [-0.0034,  0.0034],\n",
       "        [-0.0035,  0.0035],\n",
       "        [-0.0034,  0.0034],\n",
       "        [-0.0034,  0.0034],\n",
       "        [-0.0027,  0.0027],\n",
       "        [-0.0036,  0.0036],\n",
       "        [-0.0037,  0.0037],\n",
       "        [-0.0034,  0.0034],\n",
       "        [-0.0030,  0.0030],\n",
       "        [-0.0039,  0.0039],\n",
       "        [-0.0037,  0.0037],\n",
       "        [-0.0028,  0.0028],\n",
       "        [-0.0037,  0.0037],\n",
       "        [-0.0033,  0.0033],\n",
       "        [-0.0033,  0.0033],\n",
       "        [-0.0036,  0.0036],\n",
       "        [-0.0038,  0.0038],\n",
       "        [-0.0033,  0.0033],\n",
       "        [-0.0030,  0.0030],\n",
       "        [-0.0034,  0.0034],\n",
       "        [-0.0040,  0.0040],\n",
       "        [-0.0035,  0.0035],\n",
       "        [-0.0034,  0.0034],\n",
       "        [-0.0035,  0.0035],\n",
       "        [-0.0038,  0.0038],\n",
       "        [-0.0031,  0.0031],\n",
       "        [-0.0029,  0.0029],\n",
       "        [-0.0037,  0.0037],\n",
       "        [-0.0039,  0.0039],\n",
       "        [-0.0035,  0.0035],\n",
       "        [-0.0035,  0.0035],\n",
       "        [-0.0038,  0.0038],\n",
       "        [-0.0034,  0.0034],\n",
       "        [-0.0034,  0.0034],\n",
       "        [-0.0041,  0.0041],\n",
       "        [-0.0038,  0.0038],\n",
       "        [-0.0034,  0.0034],\n",
       "        [-0.0033,  0.0033],\n",
       "        [-0.0032,  0.0032],\n",
       "        [-0.0035,  0.0035],\n",
       "        [-0.0032,  0.0032],\n",
       "        [-0.0030,  0.0030],\n",
       "        [-0.0029,  0.0029],\n",
       "        [ 0.0025, -0.0025],\n",
       "        [-0.0034,  0.0034],\n",
       "        [-0.0035,  0.0035],\n",
       "        [-0.0032,  0.0032],\n",
       "        [ 0.0027, -0.0027],\n",
       "        [-0.0036,  0.0036],\n",
       "        [-0.0037,  0.0037],\n",
       "        [-0.0034,  0.0034],\n",
       "        [-0.0034,  0.0034],\n",
       "        [-0.0040,  0.0040],\n",
       "        [-0.0037,  0.0037],\n",
       "        [-0.0031,  0.0031],\n",
       "        [-0.0033,  0.0033],\n",
       "        [-0.0036,  0.0036],\n",
       "        [-0.0032,  0.0032],\n",
       "        [-0.0038,  0.0038],\n",
       "        [-0.0033,  0.0033],\n",
       "        [-0.0034,  0.0034],\n",
       "        [-0.0036,  0.0036],\n",
       "        [-0.0038,  0.0038],\n",
       "        [-0.0034,  0.0034],\n",
       "        [ 0.0028, -0.0028],\n",
       "        [-0.0038,  0.0038],\n",
       "        [-0.0035,  0.0035],\n",
       "        [-0.0036,  0.0036],\n",
       "        [-0.0034,  0.0034],\n",
       "        [-0.0042,  0.0042],\n",
       "        [-0.0038,  0.0038],\n",
       "        [-0.0035,  0.0035],\n",
       "        [-0.0035,  0.0035],\n",
       "        [-0.0039,  0.0039],\n",
       "        [-0.0034,  0.0034],\n",
       "        [-0.0033,  0.0033],\n",
       "        [-0.0033,  0.0033],\n",
       "        [-0.0033,  0.0033],\n",
       "        [-0.0034,  0.0034],\n",
       "        [-0.0032,  0.0032],\n",
       "        [-0.0032,  0.0032],\n",
       "        [-0.0033,  0.0033],\n",
       "        [-0.0027,  0.0027],\n",
       "        [ 0.0029, -0.0029],\n",
       "        [-0.0036,  0.0036],\n",
       "        [-0.0032,  0.0032],\n",
       "        [-0.0037,  0.0037],\n",
       "        [-0.0029,  0.0029],\n",
       "        [-0.0035,  0.0035],\n",
       "        [-0.0036,  0.0036],\n",
       "        [-0.0032,  0.0032],\n",
       "        [-0.0032,  0.0032],\n",
       "        [-0.0035,  0.0035],\n",
       "        [-0.0034,  0.0034],\n",
       "        [-0.0035,  0.0035],\n",
       "        [-0.0037,  0.0037],\n",
       "        [ 0.0027, -0.0027],\n",
       "        [-0.0040,  0.0040],\n",
       "        [-0.0029,  0.0029],\n",
       "        [-0.0034,  0.0034],\n",
       "        [-0.0030,  0.0030],\n",
       "        [-0.0034,  0.0034],\n",
       "        [-0.0036,  0.0036],\n",
       "        [-0.0029,  0.0029],\n",
       "        [-0.0031,  0.0031],\n",
       "        [-0.0034,  0.0034],\n",
       "        [-0.0038,  0.0038],\n",
       "        [-0.0034,  0.0034],\n",
       "        [-0.0035,  0.0035],\n",
       "        [-0.0037,  0.0037],\n",
       "        [-0.0035,  0.0035],\n",
       "        [-0.0039,  0.0039],\n",
       "        [-0.0035,  0.0035],\n",
       "        [-0.0028,  0.0028],\n",
       "        [-0.0037,  0.0037],\n",
       "        [-0.0036,  0.0036],\n",
       "        [-0.0035,  0.0035],\n",
       "        [-0.0032,  0.0032],\n",
       "        [-0.0040,  0.0040],\n",
       "        [-0.0037,  0.0037],\n",
       "        [-0.0035,  0.0035],\n",
       "        [-0.0032,  0.0032],\n",
       "        [-0.0042,  0.0042],\n",
       "        [-0.0029,  0.0029],\n",
       "        [-0.0032,  0.0032],\n",
       "        [-0.0035,  0.0035],\n",
       "        [-0.0028,  0.0028],\n",
       "        [-0.0035,  0.0035],\n",
       "        [-0.0034,  0.0034],\n",
       "        [-0.0029,  0.0029],\n",
       "        [-0.0036,  0.0036],\n",
       "        [-0.0034,  0.0034],\n",
       "        [-0.0041,  0.0041],\n",
       "        [-0.0031,  0.0031],\n",
       "        [-0.0030,  0.0030],\n",
       "        [-0.0028,  0.0028],\n",
       "        [-0.0033,  0.0033],\n",
       "        [-0.0035,  0.0035],\n",
       "        [-0.0035,  0.0035],\n",
       "        [-0.0035,  0.0035],\n",
       "        [-0.0039,  0.0039],\n",
       "        [-0.0038,  0.0038],\n",
       "        [-0.0029,  0.0029],\n",
       "        [-0.0035,  0.0035],\n",
       "        [-0.0037,  0.0037],\n",
       "        [ 0.0024, -0.0024],\n",
       "        [-0.0034,  0.0034],\n",
       "        [-0.0033,  0.0033],\n",
       "        [-0.0032,  0.0032],\n",
       "        [-0.0035,  0.0035],\n",
       "        [-0.0041,  0.0041],\n",
       "        [-0.0040,  0.0040],\n",
       "        [-0.0035,  0.0035],\n",
       "        [-0.0030,  0.0030],\n",
       "        [-0.0040,  0.0040],\n",
       "        [-0.0036,  0.0036],\n",
       "        [-0.0032,  0.0032]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-9.0107e-05,  2.0942e-04],\n",
       "        [-8.0266e-05,  1.4869e-04],\n",
       "        [-1.6857e-04,  1.6530e-04],\n",
       "        [-1.7369e-04,  2.1271e-04],\n",
       "        [ 1.6890e-04, -1.5768e-04],\n",
       "        [-2.1151e-04,  1.0587e-04],\n",
       "        [-1.3928e-04,  1.3514e-04],\n",
       "        [-2.2089e-03,  1.8572e-04],\n",
       "        [-6.1423e-05,  3.7357e-04],\n",
       "        [-1.5620e-04,  3.5249e-04],\n",
       "        [-1.7155e-04,  1.6050e-04],\n",
       "        [-1.1539e-04,  2.1615e-04],\n",
       "        [-6.9747e-05,  1.6683e-04],\n",
       "        [-6.9633e-05,  1.3634e-04],\n",
       "        [-2.3469e-04,  1.9879e-04],\n",
       "        [-6.4718e-05,  1.4184e-04],\n",
       "        [-3.5937e-03,  1.9341e-04],\n",
       "        [-6.5498e-05,  3.4894e-04],\n",
       "        [-8.1015e-05,  1.1920e-04],\n",
       "        [-3.8032e-03,  2.7512e-04],\n",
       "        [-1.4172e-04,  2.4176e-04],\n",
       "        [-1.7244e-04,  2.0455e-04],\n",
       "        [-9.1084e-05,  1.3118e-04],\n",
       "        [-3.2361e-04,  4.5780e-05],\n",
       "        [-2.0791e-04,  1.8226e-04],\n",
       "        [-1.4336e-04,  2.5588e-04],\n",
       "        [-1.6582e-03,  4.7993e-04],\n",
       "        [-1.6106e-04,  1.1574e-04],\n",
       "        [-2.7022e-04,  2.0873e-04],\n",
       "        [-7.7036e-05,  2.7856e-04],\n",
       "        [-2.5076e-03,  3.1809e-04],\n",
       "        [-1.1457e-04,  2.5547e-04],\n",
       "        [-1.3593e-04,  2.1754e-04],\n",
       "        [-1.6623e-04,  2.3789e-04],\n",
       "        [-1.8162e-04,  1.9016e-04],\n",
       "        [-2.3039e-04,  1.1968e-04],\n",
       "        [-2.5347e-04,  2.3378e-04],\n",
       "        [-1.8170e-04,  3.1053e-04],\n",
       "        [-1.6334e-04,  9.3585e-05],\n",
       "        [-1.8556e-04,  1.9564e-04],\n",
       "        [-1.4606e-04,  2.0682e-04],\n",
       "        [-1.9480e-04,  3.1573e-04],\n",
       "        [-1.2860e-04,  1.6931e-04],\n",
       "        [-2.0213e-03,  2.5611e-04],\n",
       "        [-1.4368e-03,  4.0285e-04],\n",
       "        [-2.3160e-04,  1.5345e-04],\n",
       "        [-1.6801e-04,  2.0855e-04],\n",
       "        [-1.9759e-04,  1.7932e-04],\n",
       "        [-2.0666e-04,  1.9795e-04],\n",
       "        [-9.0793e-05,  1.9984e-04],\n",
       "        [-1.1533e-04,  1.3189e-04],\n",
       "        [-1.5333e-04,  2.6210e-04],\n",
       "        [-2.4354e-04,  2.3190e-04],\n",
       "        [ 1.4077e-04, -2.6185e-04],\n",
       "        [-1.0270e-04,  2.2619e-04],\n",
       "        [-2.8484e-04,  2.3201e-04],\n",
       "        [-1.4778e-04,  8.1008e-05],\n",
       "        [ 2.3197e-04, -2.1794e-04],\n",
       "        [-1.4893e-04,  1.8742e-04],\n",
       "        [-1.3063e-04,  2.2615e-04],\n",
       "        [-1.4755e-04,  1.3538e-04],\n",
       "        [-1.8991e-04,  2.2232e-04],\n",
       "        [-1.3258e-04,  1.7916e-04],\n",
       "        [-2.3836e-04,  1.8740e-04],\n",
       "        [-1.2909e-04,  2.4034e-04],\n",
       "        [-2.7841e-03,  8.2454e-05],\n",
       "        [-1.4802e-04,  1.8694e-04],\n",
       "        [-1.4537e-04,  2.2275e-04],\n",
       "        [-2.5967e-04,  1.5516e-04],\n",
       "        [-1.3388e-04,  2.4906e-04],\n",
       "        [-1.6993e-04,  1.7304e-04],\n",
       "        [-1.5089e-04,  1.7603e-04],\n",
       "        [-1.0595e-04,  1.1757e-04],\n",
       "        [-1.3886e-04,  1.7797e-04],\n",
       "        [ 2.3006e-04, -2.2612e-04],\n",
       "        [-1.2432e-04,  1.0989e-04],\n",
       "        [-1.1041e-04,  2.1423e-04],\n",
       "        [-1.6235e-04,  2.1930e-04],\n",
       "        [-7.7040e-05,  2.0778e-04],\n",
       "        [-1.6206e-03,  1.6926e-04],\n",
       "        [-1.5864e-04,  3.1026e-04],\n",
       "        [-2.2083e-04,  8.8601e-05],\n",
       "        [-2.0963e-04,  2.1829e-04],\n",
       "        [-1.7239e-04,  2.2011e-04],\n",
       "        [-8.4374e-05,  2.4798e-04],\n",
       "        [-3.0423e-04,  3.0359e-04],\n",
       "        [-2.0999e-04,  1.7224e-04],\n",
       "        [-1.0599e-04,  2.2207e-04],\n",
       "        [-1.3546e-04,  8.3958e-05],\n",
       "        [-2.1185e-04,  1.8464e-04],\n",
       "        [-1.6044e-04,  2.4055e-04],\n",
       "        [-1.4450e-04,  1.1783e-04],\n",
       "        [-1.7178e-04,  1.3958e-04],\n",
       "        [ 2.8587e-04, -3.6678e-05],\n",
       "        [-1.0788e-04,  1.6553e-04],\n",
       "        [-1.9419e-04,  2.0412e-04],\n",
       "        [-1.8305e-04,  1.7731e-04],\n",
       "        [-6.5019e-05,  2.4965e-04],\n",
       "        [-2.1383e-04,  3.5521e-04],\n",
       "        [-2.4247e-03,  2.1919e-04],\n",
       "        [-2.1670e-04,  1.8913e-04],\n",
       "        [-1.4582e-04,  3.9361e-04],\n",
       "        [-1.8230e-04,  1.1257e-04],\n",
       "        [-1.6877e-04,  1.4645e-04],\n",
       "        [-7.4205e-05,  3.6524e-04],\n",
       "        [-1.6909e-04,  1.4105e-04],\n",
       "        [ 1.5234e-04, -1.9095e-04],\n",
       "        [-4.1186e-03,  1.6779e-04],\n",
       "        [-1.0600e-04,  1.8523e-04],\n",
       "        [-1.0874e-04,  1.5983e-04],\n",
       "        [-4.6823e-05,  1.9529e-04],\n",
       "        [-1.7714e-04,  1.5022e-04],\n",
       "        [-1.3558e-04,  2.4804e-04],\n",
       "        [-1.3019e-04,  1.5490e-04],\n",
       "        [-3.4693e-05,  2.4040e-04],\n",
       "        [-1.5257e-04,  3.4088e-04],\n",
       "        [-9.6825e-05,  2.8284e-04],\n",
       "        [-1.4518e-04,  1.9916e-04],\n",
       "        [-6.3642e-05,  3.6749e-04],\n",
       "        [-1.9684e-04,  1.7340e-04],\n",
       "        [-1.4681e-04,  3.8800e-04],\n",
       "        [-1.0909e-04,  3.8424e-04],\n",
       "        [-1.3536e-04,  2.1731e-04],\n",
       "        [-1.9979e-04,  1.8537e-04],\n",
       "        [-1.9660e-04,  2.8578e-04],\n",
       "        [-1.2504e-04,  3.2843e-04],\n",
       "        [-1.1458e-04,  1.2779e-04],\n",
       "        [-1.3044e-04,  1.3379e-04],\n",
       "        [-1.1203e-04,  1.8262e-04],\n",
       "        [-8.3087e-05,  1.9671e-04],\n",
       "        [-1.8964e-04,  2.6094e-04],\n",
       "        [-1.1329e-04,  2.5629e-04],\n",
       "        [-1.1587e-04,  2.9426e-04],\n",
       "        [-2.4965e-04,  2.0657e-04],\n",
       "        [-1.6754e-04,  2.2870e-04],\n",
       "        [-1.6969e-03,  1.6247e-04],\n",
       "        [-3.0267e-04,  1.1092e-04],\n",
       "        [-1.0125e-04,  2.5190e-04],\n",
       "        [-9.2754e-05,  2.1320e-04],\n",
       "        [-3.0723e-04,  2.0121e-04],\n",
       "        [-1.3179e-04,  3.4011e-04],\n",
       "        [-7.4174e-05,  1.8153e-04],\n",
       "        [-9.9012e-05,  3.1559e-04],\n",
       "        [-1.6099e-04,  1.5226e-04],\n",
       "        [-7.7874e-05,  2.2082e-04],\n",
       "        [-2.4235e-04,  2.1617e-04],\n",
       "        [-2.3215e-04,  7.4080e-05],\n",
       "        [-9.1356e-05,  3.0529e-04],\n",
       "        [-2.6279e-04,  3.1776e-04],\n",
       "        [-1.9605e-03,  1.7682e-04],\n",
       "        [-2.2476e-04,  3.1136e-04],\n",
       "        [-2.3461e-03,  1.6203e-04],\n",
       "        [-1.7543e-04,  1.7600e-04],\n",
       "        [-3.3110e-03,  2.3773e-04],\n",
       "        [-1.4787e-04,  2.0717e-04],\n",
       "        [ 9.1101e-05, -3.5169e-04],\n",
       "        [-9.7195e-05,  3.3455e-04],\n",
       "        [-2.2075e-04,  1.1125e-04],\n",
       "        [-1.9643e-04,  2.1440e-04],\n",
       "        [-1.9451e-04,  1.7895e-04],\n",
       "        [-1.8170e-04,  2.5998e-04],\n",
       "        [-2.2012e-04,  3.7986e-04],\n",
       "        [-2.1684e-04,  2.5164e-04],\n",
       "        [-1.2582e-04,  1.0524e-04],\n",
       "        [-1.1566e-04,  8.3126e-05],\n",
       "        [-2.0691e-04,  1.2656e-04],\n",
       "        [-1.9694e-04,  2.4594e-04]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
